This is
/home/melpa/melpa/working/python-info-20130916.1420/python.info,
produced by makeinfo version 4.13 from
/home/melpa/melpa/working/python-info/python.texi.

Generated by Sphinx 1.1.3.
INFO-DIR-SECTION Programming
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY

     Python 2.7.5, September 16, 2013

     Georg Brandl

     Copyright (C) 1990-2013, Python Software Foundation


File: python.info,  Node: thread --- Multiple threads of control,  Next: dummy_threading --- Drop-in replacement for the threading module,  Prev: threading --- Higher-level threading interface,  Up: Optional Operating System Services

5.16.3 `thread' -- Multiple threads of control
----------------------------------------------

     Note: The *note thread: 178. module has been renamed to `_thread'
     in Python 3.  The *note 2to3: bbc. tool will automatically adapt
     imports when converting your sources to Python 3; however, you
     should consider using the high-level *note threading: 179. module
     instead.

This module provides low-level primitives for working with multiple
threads (also called _light-weight processes_ or _tasks_) -- multiple
threads of control sharing their global data space.  For
synchronization, simple locks (also called _mutexes_ or _binary
semaphores_) are provided.  The *note threading: 179. module provides
an easier to use and higher-level threading API built on top of this
module.

  The module is optional.  It is supported on Windows, Linux, SGI IRIX,
Solaris 2.x, as well as on systems that have a POSIX thread (a.k.a.
"pthread") implementation.  For systems lacking the *note thread: 178.
module, the *note dummy_thread: b8. module is available. It duplicates
this module's interface and can be used as a drop-in replacement.

  It defines the following constant and functions:

 -- Exception: thread.error
     Raised on thread-specific errors.

 -- Data: thread.LockType
     This is the type of lock objects.

 -- Function: thread.start_new_thread (function, args[, kwargs])
     Start a new thread and return its identifier.  The thread executes
     the function _function_ with the argument list _args_ (which must
     be a tuple).  The optional _kwargs_ argument specifies a
     dictionary of keyword arguments. When the function returns, the
     thread silently exits.  When the function terminates with an
     unhandled exception, a stack trace is printed and then the thread
     exits (but other threads continue to run).

 -- Function: thread.interrupt_main ()
     Raise a *note KeyboardInterrupt: 24e. exception in the main
     thread.  A subthread can use this function to interrupt the main
     thread.

     New in version 2.3.

 -- Function: thread.exit ()
     Raise the *note SystemExit: 32b. exception.  When not caught, this
     will cause the thread to exit silently.

 -- Function: thread.allocate_lock ()
     Return a new lock object.  Methods of locks are described below.
     The lock is initially unlocked.

 -- Function: thread.get_ident ()
     Return the 'thread identifier' of the current thread.  This is a
     nonzero integer.  Its value has no direct meaning; it is intended
     as a magic cookie to be used e.g. to index a dictionary of
     thread-specific data.  Thread identifiers may be recycled when a
     thread exits and another thread is created.

 -- Function: thread.stack_size ([size])
     Return the thread stack size used when creating new threads.  The
     optional _size_ argument specifies the stack size to be used for
     subsequently created threads, and must be 0 (use platform or
     configured default) or a positive integer value of at least 32,768
     (32kB). If changing the thread stack size is unsupported, the
     *note error: 15da. exception is raised.  If the specified stack
     size is invalid, a *note ValueError: 233. is raised and the stack
     size is unmodified.  32kB is currently the minimum supported stack
     size value to guarantee sufficient stack space for the interpreter
     itself.  Note that some platforms may have particular restrictions
     on values for the stack size, such as requiring a minimum stack
     size > 32kB or requiring allocation in multiples of the system
     memory page size - platform documentation should be referred to
     for more information (4kB pages are common; using multiples of
     4096 for the stack size is the suggested approach in the absence
     of more specific information).  Availability: Windows, systems
     with POSIX threads.

     New in version 2.5.

  Lock objects have the following methods:

 -- Method: lock.acquire ([waitflag])
     Without the optional argument, this method acquires the lock
     unconditionally, if necessary waiting until it is released by
     another thread (only one thread at a time can acquire a lock --
     that's their reason for existence).  If the integer _waitflag_
     argument is present, the action depends on its value: if it is
     zero, the lock is only acquired if it can be acquired immediately
     without waiting, while if it is nonzero, the lock is acquired
     unconditionally as before.  The return value is `True' if the lock
     is acquired successfully, `False' if not.

 -- Method: lock.release ()
     Releases the lock.  The lock must have been acquired earlier, but
     not necessarily by the same thread.

 -- Method: lock.locked ()
     Return the status of the lock: `True' if it has been acquired by
     some thread, `False' if not.

  In addition to these methods, lock objects can also be used via the
*note with: 1bd. statement, e.g.:

    import thread

    a_lock = thread.allocate_lock()

    with a_lock:
        print "a_lock is locked while this executes"

*Caveats:*

 
   * Threads interact strangely with interrupts: the *note
     KeyboardInterrupt: 24e.  exception will be received by an
     arbitrary thread.  (When the *note signal: 155.  module is
     available, interrupts always go to the main thread.)

   * Calling *note sys.exit(): 2a4. or raising the *note SystemExit:
     32b. exception is equivalent to calling *note thread.exit(): 15de.

   * Not all built-in functions that may block waiting for I/O allow
     other threads to run.  (The most popular ones (*note time.sleep():
     11cf, *note file.read(): 8f7, *note select.select(): 1579.) work
     as expected.)

   * It is not possible to interrupt the `acquire()' method on a lock
     -- the *note KeyboardInterrupt: 24e. exception will happen after
     the lock has been acquired.

   * When the main thread exits, it is system defined whether the other
     threads survive.  On SGI IRIX using the native thread
     implementation, they survive.  On most other systems, they are
     killed without executing *note try: 38e ...  *note finally: 38f.
     clauses or executing object destructors.

   * When the main thread exits, it does not do any of its usual
     cleanup (except that *note try: 38e ... *note finally: 38f.
     clauses are honored), and the standard I/O files are not flushed.


File: python.info,  Node: dummy_threading --- Drop-in replacement for the threading module,  Next: dummy_thread --- Drop-in replacement for the thread module,  Prev: thread --- Multiple threads of control,  Up: Optional Operating System Services

5.16.4 `dummy_threading' -- Drop-in replacement for the `threading' module
--------------------------------------------------------------------------

*Source code:* Lib/dummy_threading.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  This module provides a duplicate interface to the *note threading:
179. module.  It is meant to be imported when the *note thread: 178.
module is not provided on a platform.

  Suggested usage is:

    try:
        import threading as _threading
    except ImportError:
        import dummy_threading as _threading

Be careful to not use this module where deadlock might occur from a
thread being created that blocks waiting for another thread to be
created.  This  often occurs with blocking I/O.

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/dummy_threading.py


File: python.info,  Node: dummy_thread --- Drop-in replacement for the thread module,  Next: multiprocessing --- Process-based "threading" interface,  Prev: dummy_threading --- Drop-in replacement for the threading module,  Up: Optional Operating System Services

5.16.5 `dummy_thread' -- Drop-in replacement for the `thread' module
--------------------------------------------------------------------

     Note: The *note dummy_thread: b8. module has been renamed to
     `_dummy_thread' in Python 3.  The *note 2to3: bbc. tool will
     automatically adapt imports when converting your sources to Python
     3; however, you should consider using the high-lever *note
     dummy_threading: b9. module instead.

*Source code:* Lib/dummy_thread.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  This module provides a duplicate interface to the *note thread: 178.
module.  It is meant to be imported when the *note thread: 178. module
is not provided on a platform.

  Suggested usage is:

    try:
        import thread as _thread
    except ImportError:
        import dummy_thread as _thread

Be careful to not use this module where deadlock might occur from a
thread being created that blocks waiting for another thread to be
created.  This  often occurs with blocking I/O.

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/dummy_thread.py


File: python.info,  Node: multiprocessing --- Process-based "threading" interface,  Next: mmap --- Memory-mapped file support,  Prev: dummy_thread --- Drop-in replacement for the thread module,  Up: Optional Operating System Services

5.16.6 `multiprocessing' -- Process-based "threading" interface
---------------------------------------------------------------

New in version 2.6.

* Menu:

* Introduction: Introduction<7>.
* Reference::
* Programming guidelines::
* Examples: Examples<7>.

Introduction

* The Process class::
* Exchanging objects between processes::
* Synchronization between processes::
* Sharing state between processes::
* Using a pool of workers::

Reference

* Process and exceptions::
* Pipes and Queues::
* Miscellaneous: Miscellaneous<2>.
* Connection Objects: Connection Objects<2>.
* Synchronization primitives::
* Shared ctypes Objects::
* Managers::
* Proxy Objects::
* Process Pools::
* Listeners and Clients::
* Authentication keys::
* Logging: Logging<2>.
* The multiprocessing.dummy module: The multiprocessing dummy module.

Shared ctypes Objects

* The multiprocessing.sharedctypes module: The multiprocessing sharedctypes module.

Managers

* Namespace objects::
* Customized managers::
* Using a remote manager::

Proxy Objects

* Cleanup: Cleanup<2>.

Listeners and Clients

* Address Formats::

Programming guidelines

* All platforms::
* Windows::


File: python.info,  Node: Introduction<7>,  Next: Reference,  Up: multiprocessing --- Process-based "threading" interface

5.16.6.1 Introduction
.....................

*note multiprocessing: 119. is a package that supports spawning
processes using an API similar to the *note threading: 179. module.
The *note multiprocessing: 119. package offers both local and remote
concurrency, effectively side-stepping the *note Global Interpreter
Lock: 1523. by using subprocesses instead of threads.  Due to this, the
*note multiprocessing: 119. module allows the programmer to fully
leverage multiple processors on a given machine.  It runs on both Unix
and Windows.

     Warning: Some of this package's functionality requires a
     functioning shared semaphore implementation on the host operating
     system. Without one, the `multiprocessing.synchronize' module will
     be disabled, and attempts to import it will result in an *note
     ImportError: 369. See issue 3770(1) for additional information.

     Note: Functionality within this package requires that the
     `__main__' module be importable by the children. This is covered
     in *note Programming guidelines: 15eb.  however it is worth
     pointing out here. This means that some examples, such as the
     `multiprocessing.Pool' examples will not work in the interactive
     interpreter. For example:

         >>> from multiprocessing import Pool
         >>> p = Pool(5)
         >>> def f(x):
         ...     return x*x
         ...
         >>> p.map(f, [1,2,3])
         Process PoolWorker-1:
         Process PoolWorker-2:
         Process PoolWorker-3:
         Traceback (most recent call last):
         Traceback (most recent call last):
         Traceback (most recent call last):
         AttributeError: 'module' object has no attribute 'f'
         AttributeError: 'module' object has no attribute 'f'
         AttributeError: 'module' object has no attribute 'f'

     (If you try this it will actually output three full tracebacks
     interleaved in a semi-random fashion, and then you may have to
     stop the master process somehow.)

* Menu:

* The Process class::
* Exchanging objects between processes::
* Synchronization between processes::
* Sharing state between processes::
* Using a pool of workers::

  ---------- Footnotes ----------

  (1) http://bugs.python.org/issue3770


File: python.info,  Node: The Process class,  Next: Exchanging objects between processes,  Up: Introduction<7>

5.16.6.2 The `Process' class
............................

In *note multiprocessing: 119, processes are spawned by creating a
*note Process: 15ed.  object and then calling its *note start(): 15ee.
method.  *note Process: 15ed.  follows the API of *note
threading.Thread: 1599.  A trivial example of a multiprocess program is

    from multiprocessing import Process

    def f(name):
        print 'hello', name

    if __name__ == '__main__':
        p = Process(target=f, args=('bob',))
        p.start()
        p.join()

To show the individual process IDs involved, here is an expanded
example:

    from multiprocessing import Process
    import os

    def info(title):
        print title
        print 'module name:', __name__
        if hasattr(os, 'getppid'):  # only available on Unix
            print 'parent process:', os.getppid()
        print 'process id:', os.getpid()

    def f(name):
        info('function f')
        print 'hello', name

    if __name__ == '__main__':
        info('main line')
        p = Process(target=f, args=('bob',))
        p.start()
        p.join()

For an explanation of why (on Windows) the `if __name__ == '__main__''
part is necessary, see *note Programming guidelines: 15eb.


File: python.info,  Node: Exchanging objects between processes,  Next: Synchronization between processes,  Prev: The Process class,  Up: Introduction<7>

5.16.6.3 Exchanging objects between processes
.............................................

*note multiprocessing: 119. supports two types of communication channel
between processes:

  *Queues*

     The *note Queue: 15f0. class is a near clone of *note Queue.Queue:
     5f7.  For example:

         from multiprocessing import Process, Queue

         def f(q):
             q.put([42, None, 'hello'])

         if __name__ == '__main__':
             q = Queue()
             p = Process(target=f, args=(q,))
             p.start()
             print q.get()    # prints "[42, None, 'hello']"
             p.join()

     Queues are thread and process safe.

  *Pipes*

     The *note Pipe(): 15f1. function returns a pair of connection
     objects connected by a pipe which by default is duplex (two-way).
     For example:

         from multiprocessing import Process, Pipe

         def f(conn):
             conn.send([42, None, 'hello'])
             conn.close()

         if __name__ == '__main__':
             parent_conn, child_conn = Pipe()
             p = Process(target=f, args=(child_conn,))
             p.start()
             print parent_conn.recv()   # prints "[42, None, 'hello']"
             p.join()

     The two connection objects returned by *note Pipe(): 15f1.
     represent the two ends of the pipe.  Each connection object has
     *note send(): 15f2. and *note recv(): 15f3. methods (among
     others).  Note that data in a pipe may become corrupted if two
     processes (or threads) try to read from or write to the _same_ end
     of the pipe at the same time.  Of course there is no risk of
     corruption from processes using different ends of the pipe at the
     same time.


File: python.info,  Node: Synchronization between processes,  Next: Sharing state between processes,  Prev: Exchanging objects between processes,  Up: Introduction<7>

5.16.6.4 Synchronization between processes
..........................................

*note multiprocessing: 119. contains equivalents of all the
synchronization primitives from *note threading: 179.  For instance one
can use a lock to ensure that only one process prints to standard
output at a time:

    from multiprocessing import Process, Lock

    def f(l, i):
        l.acquire()
        print 'hello world', i
        l.release()

    if __name__ == '__main__':
        lock = Lock()

        for num in range(10):
            Process(target=f, args=(lock, num)).start()

Without using the lock output from the different processes is liable to
get all mixed up.


File: python.info,  Node: Sharing state between processes,  Next: Using a pool of workers,  Prev: Synchronization between processes,  Up: Introduction<7>

5.16.6.5 Sharing state between processes
........................................

As mentioned above, when doing concurrent programming it is usually
best to avoid using shared state as far as possible.  This is
particularly true when using multiple processes.

  However, if you really do need to use some shared data then *note
multiprocessing: 119. provides a couple of ways of doing so.

  *Shared memory*

     Data can be stored in a shared memory map using *note Value: 15f6.
     or *note Array: 15f7.  For example, the following code

         from multiprocessing import Process, Value, Array

         def f(n, a):
             n.value = 3.1415927
             for i in range(len(a)):
                 a[i] = -a[i]

         if __name__ == '__main__':
             num = Value('d', 0.0)
             arr = Array('i', range(10))

             p = Process(target=f, args=(num, arr))
             p.start()
             p.join()

             print num.value
             print arr[:]

     will print

         3.1415927
         [0, -1, -2, -3, -4, -5, -6, -7, -8, -9]

     The `'d'' and `'i'' arguments used when creating `num' and `arr'
     are typecodes of the kind used by the *note array: e. module:
     `'d'' indicates a double precision float and `'i'' indicates a
     signed integer.  These shared objects will be process and
     thread-safe.

     For more flexibility in using shared memory one can use the *note
     multiprocessing.sharedctypes: 11e. module which supports the
     creation of arbitrary ctypes objects allocated from shared memory.

  *Server process*

     A manager object returned by `Manager()' controls a server process
     which holds Python objects and allows other processes to
     manipulate them using proxies.

     A manager returned by `Manager()' will support types *note list:
     3b5, *note dict: 2fe, `Namespace', *note Lock: 15f8, *note RLock:
     15f9, *note Semaphore: 15fa, *note BoundedSemaphore: 15fb, *note
     Condition: 15fc, *note Event: 15fd, *note Queue: 15f0, *note
     Value: 15f6. and *note Array: 15f7.  For example,

         from multiprocessing import Process, Manager

         def f(d, l):
             d[1] = '1'
             d['2'] = 2
             d[0.25] = None
             l.reverse()

         if __name__ == '__main__':
             manager = Manager()

             d = manager.dict()
             l = manager.list(range(10))

             p = Process(target=f, args=(d, l))
             p.start()
             p.join()

             print d
             print l

     will print

         {0.25: None, 1: '1', '2': 2}
         [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]

     Server process managers are more flexible than using shared memory
     objects because they can be made to support arbitrary object
     types.  Also, a single manager can be shared by processes on
     different computers over a network.  They are, however, slower
     than using shared memory.


File: python.info,  Node: Using a pool of workers,  Prev: Sharing state between processes,  Up: Introduction<7>

5.16.6.6 Using a pool of workers
................................

The `Pool' class represents a pool of worker processes.  It has methods
which allows tasks to be offloaded to the worker processes in a few
different ways.

  For example:

    from multiprocessing import Pool

    def f(x):
        return x*x

    if __name__ == '__main__':
        pool = Pool(processes=4)              # start 4 worker processes
        result = pool.apply_async(f, [10])    # evaluate "f(10)" asynchronously
        print result.get(timeout=1)           # prints "100" unless your computer is *very* slow
        print pool.map(f, range(10))          # prints "[0, 1, 4,..., 81]"



File: python.info,  Node: Reference,  Next: Programming guidelines,  Prev: Introduction<7>,  Up: multiprocessing --- Process-based "threading" interface

5.16.6.7 Reference
..................

The *note multiprocessing: 119. package mostly replicates the API of the
*note threading: 179. module.

* Menu:

* Process and exceptions::
* Pipes and Queues::
* Miscellaneous: Miscellaneous<2>.
* Connection Objects: Connection Objects<2>.
* Synchronization primitives::
* Shared ctypes Objects::
* Managers::
* Proxy Objects::
* Process Pools::
* Listeners and Clients::
* Authentication keys::
* Logging: Logging<2>.
* The multiprocessing.dummy module: The multiprocessing dummy module.


File: python.info,  Node: Process and exceptions,  Next: Pipes and Queues,  Up: Reference

5.16.6.8 `Process' and exceptions
.................................

 -- Class: multiprocessing.Process (group=None, target=None, name=None,
          args=(), kwargs={})
     Process objects represent activity that is run in a separate
     process. The *note Process: 15ed. class has equivalents of all the
     methods of *note threading.Thread: 1599.

     The constructor should always be called with keyword arguments.
     _group_ should always be `None'; it exists solely for
     compatibility with *note threading.Thread: 1599.  _target_ is the
     callable object to be invoked by the *note run(): 1601. method.
     It defaults to `None', meaning nothing is called. _name_ is the
     process name.  By default, a unique name is constructed of the
     form 'Process-N[1]:N[2]:...:N[k]' where N[1],N[2],...,N[k] is a
     sequence of integers whose length is determined by the
     _generation_ of the process.  _args_ is the argument tuple for the
     target invocation.  _kwargs_ is a dictionary of keyword arguments
     for the target invocation.  By default, no arguments are passed to
     _target_.

     If a subclass overrides the constructor, it must make sure it
     invokes the base class constructor (`Process.__init__()') before
     doing anything else to the process.

      -- Method: run ()
          Method representing the process's activity.

          You may override this method in a subclass.  The standard
          *note run(): 1601.  method invokes the callable object passed
          to the object's constructor as the target argument, if any,
          with sequential and keyword arguments taken from the _args_
          and _kwargs_ arguments, respectively.

      -- Method: start ()
          Start the process's activity.

          This must be called at most once per process object.  It
          arranges for the object's *note run(): 1601. method to be
          invoked in a separate process.

      -- Method: join ([timeout])
          Block the calling thread until the process whose *note
          join(): 1602. method is called terminates or until the
          optional timeout occurs.

          If _timeout_ is `None' then there is no timeout.

          A process can be joined many times.

          A process cannot join itself because this would cause a
          deadlock.  It is an error to attempt to join a process before
          it has been started.

      -- Attribute: name
          The process's name.

          The name is a string used for identification purposes only.
          It has no semantics.  Multiple processes may be given the
          same name.  The initial name is set by the constructor.

      -- Method: is_alive ()
          Return whether the process is alive.

          Roughly, a process object is alive from the moment the *note
          start(): 15ee.  method returns until the child process
          terminates.

      -- Attribute: daemon
          The process's daemon flag, a Boolean value.  This must be set
          before *note start(): 15ee. is called.

          The initial value is inherited from the creating process.

          When a process exits, it attempts to terminate all of its
          daemonic child processes.

          Note that a daemonic process is not allowed to create child
          processes.  Otherwise a daemonic process would leave its
          children orphaned if it gets terminated when its parent
          process exits. Additionally, these are *not* Unix daemons or
          services, they are normal processes that will be terminated
          (and not joined) if non-daemonic processes have exited.

     In addition to the  `Threading.Thread' API, *note Process: 15ed.
     objects also support the following attributes and methods:

      -- Attribute: pid
          Return the process ID.  Before the process is spawned, this
          will be `None'.

      -- Attribute: exitcode
          The child's exit code.  This will be `None' if the process
          has not yet terminated.  A negative value _-N_ indicates that
          the child was terminated by signal _N_.

      -- Attribute: authkey
          The process's authentication key (a byte string).

          When *note multiprocessing: 119. is initialized the main
          process is assigned a random string using `os.random()'.

          When a *note Process: 15ed. object is created, it will
          inherit the authentication key of its parent process,
          although this may be changed by setting *note authkey: 1608.
          to another byte string.

          See *note Authentication keys: 1609.

      -- Method: terminate ()
          Terminate the process.  On Unix this is done using the
          `SIGTERM' signal; on Windows `TerminateProcess()' is used.
          Note that exit handlers and finally clauses, etc., will not
          be executed.

          Note that descendant processes of the process will _not_ be
          terminated - they will simply become orphaned.

               Warning: If this method is used when the associated
               process is using a pipe or queue then the pipe or queue
               is liable to become corrupted and may become unusable by
               other process.  Similarly, if the process has acquired a
               lock or semaphore etc. then terminating it is liable to
               cause other processes to deadlock.

     Note that the *note start(): 15ee, *note join(): 1602, *note
     is_alive(): 1604. and `exit_code' methods should only be called by
     the process that created the process object.

     Example usage of some of the methods of *note Process: 15ed.:

         >>> import multiprocessing, time, signal
         >>> p = multiprocessing.Process(target=time.sleep, args=(1000,))
         >>> print p, p.is_alive()
         <Process(Process-1, initial)> False
         >>> p.start()
         >>> print p, p.is_alive()
         <Process(Process-1, started)> True
         >>> p.terminate()
         >>> time.sleep(0.1)
         >>> print p, p.is_alive()
         <Process(Process-1, stopped[SIGTERM])> False
         >>> p.exitcode == -signal.SIGTERM
         True



 -- Exception: multiprocessing.BufferTooShort
     Exception raised by *note Connection.recv_bytes_into(): 160c. when
     the supplied buffer object is too small for the message read.

     If `e' is an instance of *note BufferTooShort: 160b. then
     `e.args[0]' will give the message as a byte string.


File: python.info,  Node: Pipes and Queues,  Next: Miscellaneous<2>,  Prev: Process and exceptions,  Up: Reference

5.16.6.9 Pipes and Queues
.........................

When using multiple processes, one generally uses message passing for
communication between processes and avoids having to use any
synchronization primitives like locks.

  For passing messages one can use *note Pipe(): 15f1. (for a
connection between two processes) or a queue (which allows multiple
producers and consumers).

  The *note Queue: 15f0, *note multiprocessing.queues.SimpleQueue:
160e. and *note JoinableQueue: 160f. types are multi-producer,
multi-consumer FIFO queues modelled on the *note Queue.Queue: 5f7.
class in the standard library.  They differ in that *note Queue: 15f0.
lacks the *note task_done(): bc9. and *note join(): bca. methods
introduced into Python 2.5's *note Queue.Queue: 5f7. class.

  If you use *note JoinableQueue: 160f. then you *must* call *note
JoinableQueue.task_done(): 1610. for each task removed from the queue
or else the semaphore used to count the number of unfinished tasks may
eventually overflow, raising an exception.

  Note that one can also create a shared queue by using a manager
object - see *note Managers: 1611.

     Note: *note multiprocessing: 119. uses the usual *note
     Queue.Empty: 804. and *note Queue.Full: bbf. exceptions to signal
     a timeout.  They are not available in the *note multiprocessing:
     119. namespace so you need to import them from *note Queue: 140.

     Warning: If a process is killed using *note Process.terminate():
     160a. or *note os.kill(): 2ce.  while it is trying to use a *note
     Queue: 15f0, then the data in the queue is likely to become
     corrupted.  This may cause any other process to get an exception
     when it tries to use the queue later on.

     Warning: As mentioned above, if a child process has put items on a
     queue (and it has not used `JoinableQueue.cancel_join_thread()'),
     then that process will not terminate until all buffered items have
     been flushed to the pipe.

     This means that if you try joining that process you may get a
     deadlock unless you are sure that all items which have been put on
     the queue have been consumed.  Similarly, if the child process is
     non-daemonic then the parent process may hang on exit when it
     tries to join all its non-daemonic children.

     Note that a queue created using a manager does not have this
     issue.  See *note Programming guidelines: 15eb.

  For an example of the usage of queues for interprocess communication
see *note Examples: 1612.

 -- Function: multiprocessing.Pipe ([duplex])
     Returns a pair `(conn1, conn2)' of *note Connection: 1613. objects
     representing the ends of a pipe.

     If _duplex_ is `True' (the default) then the pipe is
     bidirectional.  If _duplex_ is `False' then the pipe is
     unidirectional: `conn1' can only be used for receiving messages
     and `conn2' can only be used for sending messages.

 -- Class: multiprocessing.Queue ([maxsize])
     Returns a process shared queue implemented using a pipe and a few
     locks/semaphores.  When a process first puts an item on the queue
     a feeder thread is started which transfers objects from a buffer
     into the pipe.

     The usual *note Queue.Empty: 804. and *note Queue.Full: bbf.
     exceptions from the standard library's *note Queue: 140. module
     are raised to signal timeouts.

     *note Queue: 15f0. implements all the methods of *note
     Queue.Queue: 5f7. except for *note task_done(): bc9. and *note
     join(): bca.

      -- Method: qsize ()
          Return the approximate size of the queue.  Because of
          multithreading/multiprocessing semantics, this number is not
          reliable.

          Note that this may raise *note NotImplementedError: 93b. on
          Unix platforms like Mac OS X where `sem_getvalue()' is not
          implemented.

      -- Method: empty ()
          Return `True' if the queue is empty, `False' otherwise.
          Because of multithreading/multiprocessing semantics, this is
          not reliable.

      -- Method: full ()
          Return `True' if the queue is full, `False' otherwise.
          Because of multithreading/multiprocessing semantics, this is
          not reliable.

      -- Method: put (obj[, block[, timeout]])
          Put obj into the queue.  If the optional argument _block_ is
          `True' (the default) and _timeout_ is `None' (the default),
          block if necessary until a free slot is available.  If
          _timeout_ is a positive number, it blocks at most _timeout_
          seconds and raises the *note Queue.Full: bbf. exception if no
          free slot was available within that time.  Otherwise (_block_
          is `False'), put an item on the queue if a free slot is
          immediately available, else raise the *note Queue.Full: bbf.
          exception (_timeout_ is ignored in that case).

      -- Method: put_nowait (obj)
          Equivalent to `put(obj, False)'.

      -- Method: get ([block[, timeout]])
          Remove and return an item from the queue.  If optional args
          _block_ is `True' (the default) and _timeout_ is `None' (the
          default), block if necessary until an item is available.  If
          _timeout_ is a positive number, it blocks at most _timeout_
          seconds and raises the *note Queue.Empty: 804.  exception if
          no item was available within that time.  Otherwise (block is
          `False'), return an item if one is immediately available,
          else raise the *note Queue.Empty: 804. exception (_timeout_
          is ignored in that case).

      -- Method: get_nowait ()
          Equivalent to `get(False)'.

     *note Queue: 15f0. has a few additional methods not found in *note
     Queue.Queue: 5f7.  These methods are usually unnecessary for most
     code:

      -- Method: close ()
          Indicate that no more data will be put on this queue by the
          current process.  The background thread will quit once it has
          flushed all buffered data to the pipe.  This is called
          automatically when the queue is garbage collected.

      -- Method: join_thread ()
          Join the background thread.  This can only be used after
          *note close(): 161b. has been called.  It blocks until the
          background thread exits, ensuring that all data in the buffer
          has been flushed to the pipe.

          By default if a process is not the creator of the queue then
          on exit it will attempt to join the queue's background
          thread.  The process can call *note cancel_join_thread():
          161d. to make *note join_thread(): 161c. do nothing.

      -- Method: cancel_join_thread ()
          Prevent *note join_thread(): 161c. from blocking.  In
          particular, this prevents the background thread from being
          joined automatically when the process exits - see *note
          join_thread(): 161c.

 -- Class: multiprocessing.queues.SimpleQueue
     It is a simplified *note Queue: 15f0. type, very close to a locked
     *note Pipe: 15f1.

      -- Method: empty ()
          Return `True' if the queue is empty, `False' otherwise.

      -- Method: get ()
          Remove and return an item from the queue.

      -- Method: put (item)
          Put _item_ into the queue.

 -- Class: multiprocessing.JoinableQueue ([maxsize])
     *note JoinableQueue: 160f, a *note Queue: 15f0. subclass, is a
     queue which additionally has *note task_done(): 1610. and *note
     join(): 1621. methods.

      -- Method: task_done ()
          Indicate that a formerly enqueued task is complete. Used by
          queue consumer threads.  For each *note get(): 1619. used to
          fetch a task, a subsequent call to *note task_done(): 1610.
          tells the queue that the processing on the task is complete.

          If a `join()' is currently blocking, it will resume when all
          items have been processed (meaning that a *note task_done():
          1610. call was received for every item that had been *note
          put(): 1617. into the queue).

          Raises a *note ValueError: 233. if called more times than
          there were items placed in the queue.

      -- Method: join ()
          Block until all items in the queue have been gotten and
          processed.

          The count of unfinished tasks goes up whenever an item is
          added to the queue.  The count goes down whenever a consumer
          thread calls *note task_done(): 1610. to indicate that the
          item was retrieved and all work on it is complete.  When the
          count of unfinished tasks drops to zero, `join()' unblocks.


File: python.info,  Node: Miscellaneous<2>,  Next: Connection Objects<2>,  Prev: Pipes and Queues,  Up: Reference

5.16.6.10 Miscellaneous
.......................

 -- Function: multiprocessing.active_children ()
     Return list of all live children of the current process.

     Calling this has the side affect of "joining" any processes which
     have already finished.

 -- Function: multiprocessing.cpu_count ()
     Return the number of CPUs in the system.  May raise *note
     NotImplementedError: 93b.

 -- Function: multiprocessing.current_process ()
     Return the *note Process: 15ed. object corresponding to the
     current process.

     An analogue of *note threading.current_thread(): 159c.

 -- Function: multiprocessing.freeze_support ()
     Add support for when a program which uses *note multiprocessing:
     119. has been frozen to produce a Windows executable.  (Has been
     tested with *py2exe*, *PyInstaller* and *cx_Freeze*.)

     One needs to call this function straight after the `if __name__ ==
     '__main__'' line of the main module.  For example:

         from multiprocessing import Process, freeze_support

         def f():
             print 'hello world!'

         if __name__ == '__main__':
             freeze_support()
             Process(target=f).start()

     If the `freeze_support()' line is omitted then trying to run the
     frozen executable will raise *note RuntimeError: 394.

     If the module is being run normally by the Python interpreter then
     *note freeze_support(): 1626. has no effect.

 -- Function: multiprocessing.set_executable ()
     Sets the path of the Python interpreter to use when starting a
     child process.  (By default *note sys.executable: 1628. is used).
     Embedders will probably need to do some thing like

         set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))

     before they can create child processes.  (Windows only)

     Note: *note multiprocessing: 119. contains no analogues of *note
     threading.active_count(): 1597, *note threading.enumerate(): 159a,
     *note threading.settrace(): 15a9, *note threading.setprofile():
     15aa, *note threading.Timer: ba8, or *note threading.local: 15a0.


File: python.info,  Node: Connection Objects<2>,  Next: Synchronization primitives,  Prev: Miscellaneous<2>,  Up: Reference

5.16.6.11 Connection Objects
............................

Connection objects allow the sending and receiving of picklable objects
or strings.  They can be thought of as message oriented connected
sockets.

  Connection objects are usually created using *note Pipe(): 15f1. -
see also *note Listeners and Clients: 162a.

 -- Class: multiprocessing.Connection
      -- Method: send (obj)
          Send an object to the other end of the connection which
          should be read using *note recv(): 15f3.

          The object must be picklable.  Very large pickles
          (approximately 32 MB+, though it depends on the OS) may raise
          a *note ValueError: 233. exception.

      -- Method: recv ()
          Return an object sent from the other end of the connection
          using *note send(): 15f2.  Blocks until there its something
          to receive.  Raises *note EOFError: 874. if there is nothing
          left to receive and the other end was closed.

      -- Method: fileno ()
          Return the file descriptor or handle used by the connection.

      -- Method: close ()
          Close the connection.

          This is called automatically when the connection is garbage
          collected.

      -- Method: poll ([timeout])
          Return whether there is any data available to be read.

          If _timeout_ is not specified then it will return
          immediately.  If _timeout_ is a number then this specifies
          the maximum time in seconds to block.  If _timeout_ is `None'
          then an infinite timeout is used.

      -- Method: send_bytes (buffer[, offset[, size]])
          Send byte data from an object supporting the buffer interface
          as a complete message.

          If _offset_ is given then data is read from that position in
          _buffer_.  If _size_ is given then that many bytes will be
          read from buffer.  Very large buffers (approximately 32 MB+,
          though it depends on the OS) may raise a *note ValueError:
          233. exception

      -- Method: recv_bytes ([maxlength])
          Return a complete message of byte data sent from the other
          end of the connection as a string.  Blocks until there is
          something to receive.  Raises *note EOFError: 874. if there
          is nothing left to receive and the other end has closed.

          If _maxlength_ is specified and the message is longer than
          _maxlength_ then *note IOError: 1f7. is raised and the
          connection will no longer be readable.

      -- Method: recv_bytes_into (buffer[, offset])
          Read into _buffer_ a complete message of byte data sent from
          the other end of the connection and return the number of
          bytes in the message.  Blocks until there is something to
          receive.  Raises *note EOFError: 874. if there is nothing
          left to receive and the other end was closed.

          _buffer_ must be an object satisfying the writable buffer
          interface.  If _offset_ is given then the message will be
          written into the buffer from that position.  Offset must be a
          non-negative integer less than the length of _buffer_ (in
          bytes).

          If the buffer is too short then a *note BufferTooShort: 160b.
          exception is raised and the complete message is available as
          `e.args[0]' where `e' is the exception instance.

  For example:

    >>> from multiprocessing import Pipe
    >>> a, b = Pipe()
    >>> a.send([1, 'hello', None])
    >>> b.recv()
    [1, 'hello', None]
    >>> b.send_bytes('thank you')
    >>> a.recv_bytes()
    'thank you'
    >>> import array
    >>> arr1 = array.array('i', range(5))
    >>> arr2 = array.array('i', [0] * 10)
    >>> a.send_bytes(arr1)
    >>> count = b.recv_bytes_into(arr2)
    >>> assert count == len(arr1) * arr1.itemsize
    >>> arr2
    array('i', [0, 1, 2, 3, 4, 0, 0, 0, 0, 0])


     Warning: The *note Connection.recv(): 15f3. method automatically
     unpickles the data it receives, which can be a security risk
     unless you can trust the process which sent the message.

     Therefore, unless the connection object was produced using *note
     Pipe(): 15f1. you should only use the *note recv(): 15f3. and
     *note send(): 15f2.  methods after performing some sort of
     authentication.  See *note Authentication keys: 1609.

     Warning: If a process is killed while it is trying to read or
     write to a pipe then the data in the pipe is likely to become
     corrupted, because it may become impossible to be sure where the
     message boundaries lie.


File: python.info,  Node: Synchronization primitives,  Next: Shared ctypes Objects,  Prev: Connection Objects<2>,  Up: Reference

5.16.6.12 Synchronization primitives
....................................

Generally synchronization primitives are not as necessary in a
multiprocess program as they are in a multithreaded program.  See the
documentation for *note threading: 179. module.

  Note that one can also create synchronization primitives by using a
manager object - see *note Managers: 1611.

 -- Class: multiprocessing.BoundedSemaphore ([value])
     A bounded semaphore object: a clone of *note
     threading.BoundedSemaphore: 15a6.

     (On Mac OS X, this is indistinguishable from *note Semaphore:
     15fa. because `sem_getvalue()' is not implemented on that
     platform).

 -- Class: multiprocessing.Condition ([lock])
     A condition variable: a clone of *note threading.Condition: 15c1.

     If _lock_ is specified then it should be a *note Lock: 15f8. or
     *note RLock: 15f9.  object from *note multiprocessing: 119.

 -- Class: multiprocessing.Event
     A clone of *note threading.Event: 266.  This method returns the
     state of the internal semaphore on exit, so it will always return
     `True' except if a timeout is given and the operation times out.

     Changed in version 2.7: Previously, the method always returned
     `None'.

 -- Class: multiprocessing.Lock
     A non-recursive lock object: a clone of *note threading.Lock: 15a1.

 -- Class: multiprocessing.RLock
     A recursive lock object: a clone of *note threading.RLock: 15a3.

 -- Class: multiprocessing.Semaphore ([value])
     A semaphore object: a clone of *note threading.Semaphore: 15c9.

     Note: The `acquire()' method of *note BoundedSemaphore: 15fb,
     *note Lock: 15f8, *note RLock: 15f9. and *note Semaphore: 15fa.
     has a timeout parameter not supported by the equivalents in *note
     threading: 179.  The signature is `acquire(block=True,
     timeout=None)' with keyword parameters being acceptable.  If
     _block_ is `True' and _timeout_ is not `None' then it specifies a
     timeout in seconds.  If _block_ is `False' then _timeout_ is
     ignored.

     On Mac OS X, `sem_timedwait' is unsupported, so calling
     `acquire()' with a timeout will emulate that function's behavior
     using a sleeping loop.

     Note: If the SIGINT signal generated by Ctrl-C arrives while the
     main thread is blocked by a call to `BoundedSemaphore.acquire()',
     `Lock.acquire()', `RLock.acquire()', `Semaphore.acquire()',
     `Condition.acquire()' or `Condition.wait()' then the call will be
     immediately interrupted and *note KeyboardInterrupt: 24e. will be
     raised.

     This differs from the behaviour of *note threading: 179. where
     SIGINT will be ignored while the equivalent blocking calls are in
     progress.


File: python.info,  Node: Shared ctypes Objects,  Next: Managers,  Prev: Synchronization primitives,  Up: Reference

5.16.6.13 Shared `ctypes' Objects
.................................

It is possible to create shared objects using shared memory which can be
inherited by child processes.

 -- Function: multiprocessing.Value (typecode_or_type, *args[, lock])
     Return a *note ctypes: 78. object allocated from shared memory.
     By default the return value is actually a synchronized wrapper for
     the object.

     _typecode_or_type_ determines the type of the returned object: it
     is either a ctypes type or a one character typecode of the kind
     used by the *note array: e.  module.  _*args_ is passed on to the
     constructor for the type.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a *note
     Lock: 15f8. or *note RLock: 15f9. object then that will be used to
     synchronize access to the value.  If _lock_ is `False' then access
     to the returned object will not be automatically protected by a
     lock, so it will not necessarily be "process-safe".

     Note that _lock_ is a keyword-only argument.

 -- Function: multiprocessing.Array (typecode_or_type,
          size_or_initializer, *, lock=True)
     Return a ctypes array allocated from shared memory.  By default
     the return value is actually a synchronized wrapper for the array.

     _typecode_or_type_ determines the type of the elements of the
     returned array: it is either a ctypes type or a one character
     typecode of the kind used by the *note array: e. module.  If
     _size_or_initializer_ is an integer, then it determines the length
     of the array, and the array will be initially zeroed.  Otherwise,
     _size_or_initializer_ is a sequence which is used to initialize
     the array and whose length determines the length of the array.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a *note
     Lock: 15f8. or *note RLock: 15f9. object then that will be used to
     synchronize access to the value.  If _lock_ is `False' then access
     to the returned object will not be automatically protected by a
     lock, so it will not necessarily be "process-safe".

     Note that _lock_ is a keyword only argument.

     Note that an array of *note ctypes.c_char: 14dc. has _value_ and
     _raw_ attributes which allow one to use it to store and retrieve
     strings.

* Menu:

* The multiprocessing.sharedctypes module: The multiprocessing sharedctypes module.


File: python.info,  Node: The multiprocessing sharedctypes module,  Up: Shared ctypes Objects

5.16.6.14 The `multiprocessing.sharedctypes' module
...................................................

The *note multiprocessing.sharedctypes: 11e. module provides functions
for allocating *note ctypes: 78. objects from shared memory which can
be inherited by child processes.

     Note: Although it is possible to store a pointer in shared memory
     remember that this will refer to a location in the address space
     of a specific process.  However, the pointer is quite likely to be
     invalid in the context of a second process and trying to
     dereference the pointer from the second process may cause a crash.

 -- Function: multiprocessing.sharedctypes.RawArray (typecode_or_type,
          size_or_initializer)
     Return a ctypes array allocated from shared memory.

     _typecode_or_type_ determines the type of the elements of the
     returned array: it is either a ctypes type or a one character
     typecode of the kind used by the *note array: e. module.  If
     _size_or_initializer_ is an integer then it determines the length
     of the array, and the array will be initially zeroed.  Otherwise
     _size_or_initializer_ is a sequence which is used to initialize the
     array and whose length determines the length of the array.

     Note that setting and getting an element is potentially non-atomic
     - use *note Array(): 1634. instead to make sure that access is
     automatically synchronized using a lock.

 -- Function: multiprocessing.sharedctypes.RawValue (typecode_or_type,
          *args)
     Return a ctypes object allocated from shared memory.

     _typecode_or_type_ determines the type of the returned object: it
     is either a ctypes type or a one character typecode of the kind
     used by the *note array: e.  module.  _*args_ is passed on to the
     constructor for the type.

     Note that setting and getting the value is potentially non-atomic
     - use *note Value(): 1636. instead to make sure that access is
     automatically synchronized using a lock.

     Note that an array of *note ctypes.c_char: 14dc. has `value' and
     `raw' attributes which allow one to use it to store and retrieve
     strings - see documentation for *note ctypes: 78.

 -- Function: multiprocessing.sharedctypes.Array (typecode_or_type,
          size_or_initializer, *args[, lock])
     The same as *note RawArray(): 1633. except that depending on the
     value of _lock_ a process-safe synchronization wrapper may be
     returned instead of a raw ctypes array.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a `Lock'
     or `RLock' object then that will be used to synchronize access to
     the value.  If _lock_ is `False' then access to the returned
     object will not be automatically protected by a lock, so it will
     not necessarily be "process-safe".

     Note that _lock_ is a keyword-only argument.

 -- Function: multiprocessing.sharedctypes.Value (typecode_or_type,
          *args[, lock])
     The same as *note RawValue(): 1635. except that depending on the
     value of _lock_ a process-safe synchronization wrapper may be
     returned instead of a raw ctypes object.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a `Lock'
     or `RLock' object then that will be used to synchronize access to
     the value.  If _lock_ is `False' then access to the returned
     object will not be automatically protected by a lock, so it will
     not necessarily be "process-safe".

     Note that _lock_ is a keyword-only argument.

 -- Function: multiprocessing.sharedctypes.copy (obj)
     Return a ctypes object allocated from shared memory which is a
     copy of the ctypes object _obj_.

 -- Function: multiprocessing.sharedctypes.synchronized (obj[, lock])
     Return a process-safe wrapper object for a ctypes object which
     uses _lock_ to synchronize access.  If _lock_ is `None' (the
     default) then a *note multiprocessing.RLock: 15f9. object is
     created automatically.

     A synchronized wrapper will have two methods in addition to those
     of the object it wraps: `get_obj()' returns the wrapped object and
     `get_lock()' returns the lock object used for synchronization.

     Note that accessing the ctypes object through the wrapper can be a
     lot slower than accessing the raw ctypes object.

  The table below compares the syntax for creating shared ctypes
objects from shared memory with the normal ctypes syntax.  (In the
table `MyStruct' is some subclass of *note ctypes.Structure: 14fd.)

ctypes                   sharedctypes using type        sharedctypes using typecode
---------------------------------------------------------------------------------------- 
c_double(2.4)            RawValue(c_double, 2.4)        RawValue('d', 2.4)
MyStruct(4, 6)           RawValue(MyStruct, 4, 6)       
(c_short * 7)()          RawArray(c_short, 7)           RawArray('h', 7)
(c_int * 3)(9, 2, 8)     RawArray(c_int, (9, 2, 8))     RawArray('i', (9, 2, 8))

  Below is an example where a number of ctypes objects are modified by
a child process:

    from multiprocessing import Process, Lock
    from multiprocessing.sharedctypes import Value, Array
    from ctypes import Structure, c_double

    class Point(Structure):
        _fields_ = [('x', c_double), ('y', c_double)]

    def modify(n, x, s, A):
        n.value **= 2
        x.value **= 2
        s.value = s.value.upper()
        for a in A:
            a.x **= 2
            a.y **= 2

    if __name__ == '__main__':
        lock = Lock()

        n = Value('i', 7)
        x = Value(c_double, 1.0/3.0, lock=False)
        s = Array('c', 'hello world', lock=lock)
        A = Array(Point, [(1.875,-6.25), (-5.75,2.0), (2.375,9.5)], lock=lock)

        p = Process(target=modify, args=(n, x, s, A))
        p.start()
        p.join()

        print n.value
        print x.value
        print s.value
        print [(a.x, a.y) for a in A]

The results printed are

    49
    0.1111111111111111
    HELLO WORLD
    [(3.515625, 39.0625), (33.0625, 4.0), (5.640625, 90.25)]



File: python.info,  Node: Managers,  Next: Proxy Objects,  Prev: Shared ctypes Objects,  Up: Reference

5.16.6.15 Managers
..................

Managers provide a way to create data which can be shared between
different processes. A manager object controls a server process which
manages _shared objects_.  Other processes can access the shared
objects by using proxies.

 -- Function: multiprocessing.Manager ()
     Returns a started *note SyncManager: 163b. object which can be
     used for sharing objects between processes.  The returned manager
     object corresponds to a spawned child process and has methods
     which will create shared objects and return corresponding proxies.
  
  Manager processes will be shutdown as soon as they are garbage
collected or their parent process exits.  The manager classes are
defined in the *note multiprocessing.managers: 11c. module:

 -- Class: multiprocessing.managers.BaseManager ([address[, authkey]])
     Create a BaseManager object.

     Once created one should call *note start(): 163d. or
     `get_server().serve_forever()' to ensure that the manager object
     refers to a started manager process.

     _address_ is the address on which the manager process listens for
     new connections.  If _address_ is `None' then an arbitrary one is
     chosen.

     _authkey_ is the authentication key which will be used to check
     the validity of incoming connections to the server process.  If
     _authkey_ is `None' then `current_process().authkey'.  Otherwise
     _authkey_ is used and it must be a string.

      -- Method: start ([initializer[, initargs]])
          Start a subprocess to start the manager.  If _initializer_ is
          not `None' then the subprocess will call
          `initializer(*initargs)' when it starts.

      -- Method: get_server ()
          Returns a `Server' object which represents the actual server
          under the control of the Manager. The `Server' object
          supports the `serve_forever()' method:

              >>> from multiprocessing.managers import BaseManager
              >>> manager = BaseManager(address=('', 50000), authkey='abc')
              >>> server = manager.get_server()
              >>> server.serve_forever()

          `Server' additionally has an *note address: 163f. attribute.

      -- Method: connect ()
          Connect a local manager object to a remote manager process:

              >>> from multiprocessing.managers import BaseManager
              >>> m = BaseManager(address=('127.0.0.1', 5000), authkey='abc')
              >>> m.connect()



      -- Method: shutdown ()
          Stop the process used by the manager.  This is only available
          if *note start(): 163d. has been used to start the server
          process.

          This can be called multiple times.

      -- Method: register (typeid[, callable[, proxytype[, exposed[,
               method_to_typeid[, create_method]]]]])
          A classmethod which can be used for registering a type or
          callable with the manager class.

          _typeid_ is a "type identifier" which is used to identify a
          particular type of shared object.  This must be a string.

          _callable_ is a callable used for creating objects for this
          type identifier.  If a manager instance will be created using
          the `from_address()' classmethod or if the _create_method_
          argument is `False' then this can be left as `None'.

          _proxytype_ is a subclass of *note BaseProxy: 1643. which is
          used to create proxies for shared objects with this _typeid_.
          If `None' then a proxy class is created automatically.

          _exposed_ is used to specify a sequence of method names which
          proxies for this typeid should be allowed to access using
          `BaseProxy._callMethod()'.  (If _exposed_ is `None' then
          `proxytype._exposed_' is used instead if it exists.)  In the
          case where no exposed list is specified, all "public methods"
          of the shared object will be accessible.  (Here a "public
          method" means any attribute which has a *note __call__():
          6ea. method and whose name does not begin with `'_''.)

          _method_to_typeid_ is a mapping used to specify the return
          type of those exposed methods which should return a proxy.
          It maps method names to typeid strings.  (If
          _method_to_typeid_ is `None' then
          `proxytype._method_to_typeid_' is used instead if it exists.)
          If a method's name is not a key of this mapping or if the
          mapping is `None' then the object returned by the method will
          be copied by value.

          _create_method_ determines whether a method should be created
          with name _typeid_ which can be used to tell the server
          process to create a new shared object and return a proxy for
          it.  By default it is `True'.

     *note BaseManager: 163c. instances also have one read-only
     property:

      -- Attribute: address
          The address used by the manager.

 -- Class: multiprocessing.managers.SyncManager
     A subclass of *note BaseManager: 163c. which can be used for the
     synchronization of processes.  Objects of this type are returned by
     `multiprocessing.Manager()'.

     It also supports creation of shared lists and dictionaries.

      -- Method: BoundedSemaphore ([value])
          Create a shared *note threading.BoundedSemaphore: 15a6.
          object and return a proxy for it.

      -- Method: Condition ([lock])
          Create a shared *note threading.Condition: 15c1. object and
          return a proxy for it.

          If _lock_ is supplied then it should be a proxy for a *note
          threading.Lock: 15a1. or *note threading.RLock: 15a3. object.

      -- Method: Event ()
          Create a shared *note threading.Event: 266. object and return
          a proxy for it.

      -- Method: Lock ()
          Create a shared *note threading.Lock: 15a1. object and return
          a proxy for it.

      -- Method: Namespace ()
          Create a shared *note Namespace: 1648. object and return a
          proxy for it.

      -- Method: Queue ([maxsize])
          Create a shared *note Queue.Queue: 5f7. object and return a
          proxy for it.

      -- Method: RLock ()
          Create a shared *note threading.RLock: 15a3. object and
          return a proxy for it.

      -- Method: Semaphore ([value])
          Create a shared *note threading.Semaphore: 15c9. object and
          return a proxy for it.

      -- Method: Array (typecode, sequence)
          Create an array and return a proxy for it.

      -- Method: Value (typecode, value)
          Create an object with a writable `value' attribute and return
          a proxy for it.

      -- Method: dict ()
      -- Method: dict (mapping)
      -- Method: dict (sequence)
          Create a shared `dict' object and return a proxy for it.

      -- Method: list ()
      -- Method: list (sequence)
          Create a shared `list' object and return a proxy for it.

          Note: Modifications to mutable values or items in dict and
          list proxies will not be propagated through the manager,
          because the proxy has no way of knowing when its values or
          items are modified.  To modify such an item, you can
          re-assign the modified object to the container proxy:

              # create a list proxy and append a mutable object (a dictionary)
              lproxy = manager.list()
              lproxy.append({})
              # now mutate the dictionary
              d = lproxy[0]
              d['a'] = 1
              d['b'] = 2
              # at this point, the changes to d are not yet synced, but by
              # reassigning the dictionary, the proxy is notified of the change
              lproxy[0] = d



* Menu:

* Namespace objects::
* Customized managers::
* Using a remote manager::


File: python.info,  Node: Namespace objects,  Next: Customized managers,  Up: Managers

5.16.6.16 Namespace objects
...........................

A namespace object has no public methods, but does have writable
attributes.  Its representation shows the values of its attributes.

  However, when using a proxy for a namespace object, an attribute
beginning with `'_'' will be an attribute of the proxy and not an
attribute of the referent:

    >>> manager = multiprocessing.Manager()
    >>> Global = manager.Namespace()
    >>> Global.x = 10
    >>> Global.y = 'hello'
    >>> Global._z = 12.3    # this is an attribute of the proxy
    >>> print Global
    Namespace(x=10, y='hello')



File: python.info,  Node: Customized managers,  Next: Using a remote manager,  Prev: Namespace objects,  Up: Managers

5.16.6.17 Customized managers
.............................

To create one's own manager, one creates a subclass of *note
BaseManager: 163c. and uses the *note register(): 1642. classmethod to
register new types or callables with the manager class.  For example:

    from multiprocessing.managers import BaseManager

    class MathsClass(object):
        def add(self, x, y):
            return x + y
        def mul(self, x, y):
            return x * y

    class MyManager(BaseManager):
        pass

    MyManager.register('Maths', MathsClass)

    if __name__ == '__main__':
        manager = MyManager()
        manager.start()
        maths = manager.Maths()
        print maths.add(4, 3)         # prints 7
        print maths.mul(7, 8)         # prints 56



File: python.info,  Node: Using a remote manager,  Prev: Customized managers,  Up: Managers

5.16.6.18 Using a remote manager
................................

It is possible to run a manager server on one machine and have clients
use it from other machines (assuming that the firewalls involved allow
it).

  Running the following commands creates a server for a single shared
queue which remote clients can access:

    >>> from multiprocessing.managers import BaseManager
    >>> import Queue
    >>> queue = Queue.Queue()
    >>> class QueueManager(BaseManager): pass
    >>> QueueManager.register('get_queue', callable=lambda:queue)
    >>> m = QueueManager(address=('', 50000), authkey='abracadabra')
    >>> s = m.get_server()
    >>> s.serve_forever()

One client can access the server as follows:

    >>> from multiprocessing.managers import BaseManager
    >>> class QueueManager(BaseManager): pass
    >>> QueueManager.register('get_queue')
    >>> m = QueueManager(address=('foo.bar.org', 50000), authkey='abracadabra')
    >>> m.connect()
    >>> queue = m.get_queue()
    >>> queue.put('hello')

Another client can also use it:

    >>> from multiprocessing.managers import BaseManager
    >>> class QueueManager(BaseManager): pass
    >>> QueueManager.register('get_queue')
    >>> m = QueueManager(address=('foo.bar.org', 50000), authkey='abracadabra')
    >>> m.connect()
    >>> queue = m.get_queue()
    >>> queue.get()
    'hello'

Local processes can also access that queue, using the code from above
on the client to access it remotely:

    >>> from multiprocessing import Process, Queue
    >>> from multiprocessing.managers import BaseManager
    >>> class Worker(Process):
    ...     def __init__(self, q):
    ...         self.q = q
    ...         super(Worker, self).__init__()
    ...     def run(self):
    ...         self.q.put('local hello')
    ...
    >>> queue = Queue()
    >>> w = Worker(queue)
    >>> w.start()
    >>> class QueueManager(BaseManager): pass
    ...
    >>> QueueManager.register('get_queue', callable=lambda: queue)
    >>> m = QueueManager(address=('', 50000), authkey='abracadabra')
    >>> s = m.get_server()
    >>> s.serve_forever()



File: python.info,  Node: Proxy Objects,  Next: Process Pools,  Prev: Managers,  Up: Reference

5.16.6.19 Proxy Objects
.......................

A proxy is an object which _refers_ to a shared object which lives
(presumably) in a different process.  The shared object is said to be
the _referent_ of the proxy.  Multiple proxy objects may have the same
referent.

  A proxy object has methods which invoke corresponding methods of its
referent (although not every method of the referent will necessarily be
available through the proxy).  A proxy can usually be used in most of
the same ways that its referent can:

    >>> from multiprocessing import Manager
    >>> manager = Manager()
    >>> l = manager.list([i*i for i in range(10)])
    >>> print l
    [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
    >>> print repr(l)
    <ListProxy object, typeid 'list' at 0x...>
    >>> l[4]
    16
    >>> l[2:5]
    [4, 9, 16]

Notice that applying *note str(): 1e7. to a proxy will return the
representation of the referent, whereas applying *note repr(): 145.
will return the representation of the proxy.

  An important feature of proxy objects is that they are picklable so
they can be passed between processes.  Note, however, that if a proxy
is sent to the corresponding manager's process then unpickling it will
produce the referent itself.  This means, for example, that one shared
object can contain a second:

    >>> a = manager.list()
    >>> b = manager.list()
    >>> a.append(b)         # referent of a now contains referent of b
    >>> print a, b
    [[]] []
    >>> b.append('hello')
    >>> print a, b
    [['hello']] ['hello']


     Note: The proxy types in *note multiprocessing: 119. do nothing to
     support comparisons by value.  So, for instance, we have:

         >>> manager.list([1,2,3]) == [1,2,3]
         False

     One should just use a copy of the referent instead when making
     comparisons.

 -- Class: multiprocessing.managers.BaseProxy
     Proxy objects are instances of subclasses of *note BaseProxy: 1643.

      -- Method: _callmethod (methodname[, args[, kwds]])
          Call and return the result of a method of the proxy's
          referent.

          If `proxy' is a proxy whose referent is `obj' then the
          expression

              proxy._callmethod(methodname, args, kwds)

          will evaluate the expression

              getattr(obj, methodname)(*args, **kwds)

          in the manager's process.

          The returned value will be a copy of the result of the call
          or a proxy to a new shared object - see documentation for the
          _method_to_typeid_ argument of *note BaseManager.register():
          1642.

          If an exception is raised by the call, then is re-raised by
          *note _callmethod(): 1654.  If some other exception is raised
          in the manager's process then this is converted into a
          `RemoteError' exception and is raised by *note _callmethod():
          1654.

          Note in particular that an exception will be raised if
          _methodname_ has not been _exposed_

          An example of the usage of *note _callmethod(): 1654.:

              >>> l = manager.list(range(10))
              >>> l._callmethod('__len__')
              10
              >>> l._callmethod('__getslice__', (2, 7))   # equiv to `l[2:7]`
              [2, 3, 4, 5, 6]
              >>> l._callmethod('__getitem__', (20,))     # equiv to `l[20]`
              Traceback (most recent call last):
              ...
              IndexError: list index out of range



      -- Method: _getvalue ()
          Return a copy of the referent.

          If the referent is unpicklable then this will raise an
          exception.

      -- Method: __repr__ ()
          Return a representation of the proxy object.

      -- Method: __str__ ()
          Return the representation of the referent.

* Menu:

* Cleanup: Cleanup<2>.


File: python.info,  Node: Cleanup<2>,  Up: Proxy Objects

5.16.6.20 Cleanup
.................

A proxy object uses a weakref callback so that when it gets garbage
collected it deregisters itself from the manager which owns its
referent.

  A shared object gets deleted from the manager process when there are
no longer any proxies referring to it.


File: python.info,  Node: Process Pools,  Next: Listeners and Clients,  Prev: Proxy Objects,  Up: Reference

5.16.6.21 Process Pools
.......................

One can create a pool of processes which will carry out tasks submitted
to it with the `Pool' class.

 -- Class: multiprocessing.Pool ([processes[, initializer[, initargs[,
          maxtasksperchild]]]])
     A process pool object which controls a pool of worker processes to
     which jobs can be submitted.  It supports asynchronous results
     with timeouts and callbacks and has a parallel map implementation.

     _processes_ is the number of worker processes to use.  If
     _processes_ is `None' then the number returned by `cpu_count()' is
     used.  If _initializer_ is not `None' then each worker process
     will call `initializer(*initargs)' when it starts.

     New in version 2.7: _maxtasksperchild_ is the number of tasks a
     worker process can complete before it will exit and be replaced
     with a fresh worker process, to enable unused resources to be
     freed. The default _maxtasksperchild_ is None, which means worker
     processes will live as long as the pool.

          Note: Worker processes within a `Pool' typically live for the
          complete duration of the Pool's work queue. A frequent
          pattern found in other systems (such as Apache, mod_wsgi,
          etc) to free resources held by workers is to allow a worker
          within a pool to complete only a set amount of work before
          being exiting, being cleaned up and a new process spawned to
          replace the old one. The _maxtasksperchild_ argument to the
          `Pool' exposes this ability to the end user.

      -- Method: apply (func[, args[, kwds]])
          Equivalent of the *note apply(): 2fc. built-in function.  It
          blocks until the result is ready, so *note apply_async():
          165c. is better suited for performing work in parallel.
          Additionally, _func_ is only executed in one of the workers
          of the pool.

      -- Method: apply_async (func[, args[, kwds[, callback]]])
          A variant of the *note apply(): 2fc. method which returns a
          result object.

          If _callback_ is specified then it should be a callable which
          accepts a single argument.  When the result becomes ready
          _callback_ is applied to it (unless the call failed).
          _callback_ should complete immediately since otherwise the
          thread which handles the results will get blocked.

      -- Method: map (func, iterable[, chunksize])
          A parallel equivalent of the *note map(): 2fd. built-in
          function (it supports only one _iterable_ argument though).
          It blocks until the result is ready.

          This method chops the iterable into a number of chunks which
          it submits to the process pool as separate tasks.  The
          (approximate) size of these chunks can be specified by
          setting _chunksize_ to a positive integer.

      -- Method: map_async (func, iterable[, chunksize[, callback]])
          A variant of the *note map(): 165d. method which returns a
          result object.

          If _callback_ is specified then it should be a callable which
          accepts a single argument.  When the result becomes ready
          _callback_ is applied to it (unless the call failed).
          _callback_ should complete immediately since otherwise the
          thread which handles the results will get blocked.

      -- Method: imap (func, iterable[, chunksize])
          An equivalent of *note itertools.imap(): d47.

          The _chunksize_ argument is the same as the one used by the
          *note map(): 165d.  method.  For very long iterables using a
          large value for _chunksize_ can make the job complete *much*
          faster than using the default value of `1'.

          Also if _chunksize_ is `1' then the `next()' method of the
          iterator returned by the *note imap(): 165f. method has an
          optional _timeout_ parameter: `next(timeout)' will raise
          `multiprocessing.TimeoutError' if the result cannot be
          returned within _timeout_ seconds.

      -- Method: imap_unordered (func, iterable[, chunksize])
          The same as *note imap(): 165f. except that the ordering of
          the results from the returned iterator should be considered
          arbitrary.  (Only when there is only one worker process is
          the order guaranteed to be "correct".)

      -- Method: close ()
          Prevents any more tasks from being submitted to the pool.
          Once all the tasks have been completed the worker processes
          will exit.

      -- Method: terminate ()
          Stops the worker processes immediately without completing
          outstanding work.  When the pool object is garbage collected
          *note terminate(): 1662. will be called immediately.

      -- Method: join ()
          Wait for the worker processes to exit.  One must call *note
          close(): 1661. or *note terminate(): 1662. before using *note
          join(): 1663.

 -- Class: multiprocessing.pool.AsyncResult
     The class of the result returned by `Pool.apply_async()' and
     `Pool.map_async()'.

      -- Method: get ([timeout])
          Return the result when it arrives.  If _timeout_ is not
          `None' and the result does not arrive within _timeout_
          seconds then `multiprocessing.TimeoutError' is raised.  If
          the remote call raised an exception then that exception will
          be reraised by *note get(): 1665.

      -- Method: wait ([timeout])
          Wait until the result is available or until _timeout_ seconds
          pass.

      -- Method: ready ()
          Return whether the call has completed.

      -- Method: successful ()
          Return whether the call completed without raising an
          exception.  Will raise *note AssertionError: 7f7. if the
          result is not ready.

  The following example demonstrates the use of a pool:

    from multiprocessing import Pool

    def f(x):
        return x*x

    if __name__ == '__main__':
        pool = Pool(processes=4)              # start 4 worker processes

        result = pool.apply_async(f, (10,))    # evaluate "f(10)" asynchronously
        print result.get(timeout=1)           # prints "100" unless your computer is *very* slow

        print pool.map(f, range(10))          # prints "[0, 1, 4,..., 81]"

        it = pool.imap(f, range(10))
        print it.next()                       # prints "0"
        print it.next()                       # prints "1"
        print it.next(timeout=1)              # prints "4" unless your computer is *very* slow

        import time
        result = pool.apply_async(time.sleep, (10,))
        print result.get(timeout=1)           # raises TimeoutError



File: python.info,  Node: Listeners and Clients,  Next: Authentication keys,  Prev: Process Pools,  Up: Reference

5.16.6.22 Listeners and Clients
...............................

Usually message passing between processes is done using queues or by
using `Connection' objects returned by `Pipe()'.

  However, the *note multiprocessing.connection: 11a. module allows
some extra flexibility.  It basically gives a high level message
oriented API for dealing with sockets or Windows named pipes, and also
has support for _digest authentication_ using the *note hmac: e8.
module.

 -- Function: multiprocessing.connection.deliver_challenge (connection,
          authkey)
     Send a randomly generated message to the other end of the
     connection and wait for a reply.

     If the reply matches the digest of the message using _authkey_ as
     the key then a welcome message is sent to the other end of the
     connection.  Otherwise *note AuthenticationError: 166b. is raised.

 -- Function: multiprocessing.connection.answer_challenge (connection,
          authkey)
     Receive a message, calculate the digest of the message using
     _authkey_ as the key, and then send the digest back.

     If a welcome message is not received, then *note
     AuthenticationError: 166b. is raised.

 -- Function: multiprocessing.connection.Client (address[, family[,
          authenticate[, authkey]]])
     Attempt to set up a connection to the listener which is using
     address _address_, returning a *note Connection: 1613.

     The type of the connection is determined by _family_ argument, but
     this can generally be omitted since it can usually be inferred
     from the format of _address_. (See *note Address Formats: 166e.)

     If _authenticate_ is `True' or _authkey_ is a string then digest
     authentication is used.  The key used for authentication will be
     either _authkey_ or `current_process().authkey)' if _authkey_ is
     `None'.  If authentication fails then *note AuthenticationError:
     166b. is raised.  See *note Authentication keys: 1609.

 -- Class: multiprocessing.connection.Listener ([address[, family[,
          backlog[, authenticate[, authkey]]]]])
     A wrapper for a bound socket or Windows named pipe which is
     'listening' for connections.

     _address_ is the address to be used by the bound socket or named
     pipe of the listener object.

          Note: If an address of '0.0.0.0' is used, the address will
          not be a connectable end point on Windows. If you require a
          connectable end-point, you should use '127.0.0.1'.

     _family_ is the type of socket (or named pipe) to use.  This can
     be one of the strings `'AF_INET'' (for a TCP socket), `'AF_UNIX''
     (for a Unix domain socket) or `'AF_PIPE'' (for a Windows named
     pipe).  Of these only the first is guaranteed to be available.  If
     _family_ is `None' then the family is inferred from the format of
     _address_.  If _address_ is also `None' then a default is chosen.
     This default is the family which is assumed to be the fastest
     available.  See *note Address Formats: 166e.  Note that if
     _family_ is `'AF_UNIX'' and address is `None' then the socket will
     be created in a private temporary directory created using *note
     tempfile.mkstemp(): e69.

     If the listener object uses a socket then _backlog_ (1 by default)
     is passed to the `listen()' method of the socket once it has been
     bound.

     If _authenticate_ is `True' (`False' by default) or _authkey_ is
     not `None' then digest authentication is used.

     If _authkey_ is a string then it will be used as the
     authentication key; otherwise it must be _None_.

     If _authkey_ is `None' and _authenticate_ is `True' then
     `current_process().authkey' is used as the authentication key.  If
     _authkey_ is `None' and _authenticate_ is `False' then no
     authentication is done.  If authentication fails then *note
     AuthenticationError: 166b. is raised.  See *note Authentication
     keys: 1609.

      -- Method: accept ()
          Accept a connection on the bound socket or named pipe of the
          listener object and return a `Connection' object.  If
          authentication is attempted and fails, then *note
          AuthenticationError: 166b. is raised.

      -- Method: close ()
          Close the bound socket or named pipe of the listener object.
          This is called automatically when the listener is garbage
          collected.  However it is advisable to call it explicitly.

     Listener objects have the following read-only properties:

      -- Attribute: address
          The address which is being used by the Listener object.

      -- Attribute: last_accepted
          The address from which the last accepted connection came.  If
          this is unavailable then it is `None'.

  The module defines two exceptions:

 -- Exception: multiprocessing.connection.AuthenticationError
     Exception raised when there is an authentication error.

  *Examples*

  The following server code creates a listener which uses `'secret
password'' as an authentication key.  It then waits for a connection
and sends some data to the client:

    from multiprocessing.connection import Listener
    from array import array

    address = ('localhost', 6000)     # family is deduced to be 'AF_INET'
    listener = Listener(address, authkey='secret password')

    conn = listener.accept()
    print 'connection accepted from', listener.last_accepted

    conn.send([2.25, None, 'junk', float])

    conn.send_bytes('hello')

    conn.send_bytes(array('i', [42, 1729]))

    conn.close()
    listener.close()

The following code connects to the server and receives some data from
the server:

    from multiprocessing.connection import Client
    from array import array

    address = ('localhost', 6000)
    conn = Client(address, authkey='secret password')

    print conn.recv()                 # => [2.25, None, 'junk', float]

    print conn.recv_bytes()            # => 'hello'

    arr = array('i', [0, 0, 0, 0, 0])
    print conn.recv_bytes_into(arr)     # => 8
    print arr                         # => array('i', [42, 1729, 0, 0, 0])

    conn.close()


* Menu:

* Address Formats::


File: python.info,  Node: Address Formats,  Up: Listeners and Clients

5.16.6.23 Address Formats
.........................

   * An `'AF_INET'' address is a tuple of the form `(hostname, port)'
     where _hostname_ is a string and _port_ is an integer.

   * An `'AF_UNIX'' address is a string representing a filename on the
     filesystem.

   *
    An `'AF_PIPE'' address is a string of the form
          `r'\\.\pipe\_PipeName_''.  To use *note Client(): 166d. to
          connect to a named pipe on a remote computer called
          _ServerName_ one should use an address of the form
          `r'\\_ServerName_\pipe\_PipeName_'' instead.

Note that any string beginning with two backslashes is assumed by
default to be an `'AF_PIPE'' address rather than an `'AF_UNIX'' address.


File: python.info,  Node: Authentication keys,  Next: Logging<2>,  Prev: Listeners and Clients,  Up: Reference

5.16.6.24 Authentication keys
.............................

When one uses `Connection.recv()', the data received is automatically
unpickled.  Unfortunately unpickling data from an untrusted source is a
security risk.  Therefore *note Listener: 166f. and *note Client():
166d. use the *note hmac: e8. module to provide digest authentication.

  An authentication key is a string which can be thought of as a
password: once a connection is established both ends will demand proof
that the other knows the authentication key.  (Demonstrating that both
ends are using the same key does *not* involve sending the key over the
connection.)

  If authentication is requested but do authentication key is specified
then the return value of `current_process().authkey' is used (see *note
Process: 15ed.).  This value will automatically inherited by any *note
Process: 15ed. object that the current process creates.  This means
that (by default) all processes of a multi-process program will share a
single authentication key which can be used when setting up connections
between themselves.

  Suitable authentication keys can also be generated by using *note
os.urandom(): d33.


File: python.info,  Node: Logging<2>,  Next: The multiprocessing dummy module,  Prev: Authentication keys,  Up: Reference

5.16.6.25 Logging
.................

Some support for logging is available.  Note, however, that the *note
logging: 101.  package does not use process shared locks so it is
possible (depending on the handler type) for messages from different
processes to get mixed up.

 -- Function: multiprocessing.get_logger ()
     Returns the logger used by *note multiprocessing: 119.  If
     necessary, a new one will be created.

     When first created the logger has level `logging.NOTSET' and no
     default handler. Messages sent to this logger will not by default
     propagate to the root logger.

     Note that on Windows child processes will only inherit the level
     of the parent process's logger - any other customization of the
     logger will not be inherited.

 -- Function: multiprocessing.log_to_stderr ()
     This function performs a call to *note get_logger(): 1677. but in
     addition to returning the logger created by get_logger, it adds a
     handler which sends output to *note sys.stderr: 634. using format
     `'[%(levelname)s/%(processName)s] %(message)s''.

  Below is an example session with logging turned on:

    >>> import multiprocessing, logging
    >>> logger = multiprocessing.log_to_stderr()
    >>> logger.setLevel(logging.INFO)
    >>> logger.warning('doomed')
    [WARNING/MainProcess] doomed
    >>> m = multiprocessing.Manager()
    [INFO/SyncManager-...] child process calling self.run()
    [INFO/SyncManager-...] created temp directory /.../pymp-...
    [INFO/SyncManager-...] manager serving at '/.../listener-...'
    >>> del m
    [INFO/MainProcess] sending shutdown message to manager
    [INFO/SyncManager-...] manager exiting with exitcode 0

In addition to having these two logging functions, the multiprocessing
also exposes two additional logging level attributes. These are
`SUBWARNING' and `SUBDEBUG'. The table below illustrates where theses
fit in the normal level hierarchy.

Level                Numeric value
------------------------------------------ 
`SUBWARNING'         25
`SUBDEBUG'           5

  For a full table of logging levels, see the *note logging: 101.
module.

  These additional logging levels are used primarily for certain debug
messages within the multiprocessing module. Below is the same example
as above, except with `SUBDEBUG' enabled:

    >>> import multiprocessing, logging
    >>> logger = multiprocessing.log_to_stderr()
    >>> logger.setLevel(multiprocessing.SUBDEBUG)
    >>> logger.warning('doomed')
    [WARNING/MainProcess] doomed
    >>> m = multiprocessing.Manager()
    [INFO/SyncManager-...] child process calling self.run()
    [INFO/SyncManager-...] created temp directory /.../pymp-...
    [INFO/SyncManager-...] manager serving at '/.../pymp-djGBXN/listener-...'
    >>> del m
    [SUBDEBUG/MainProcess] finalizer calling ...
    [INFO/MainProcess] sending shutdown message to manager
    [DEBUG/SyncManager-...] manager received shutdown message
    [SUBDEBUG/SyncManager-...] calling <Finalize object, callback=unlink, ...
    [SUBDEBUG/SyncManager-...] finalizer calling <built-in function unlink> ...
    [SUBDEBUG/SyncManager-...] calling <Finalize object, dead>
    [SUBDEBUG/SyncManager-...] finalizer calling <function rmtree at 0x5aa730> ...
    [INFO/SyncManager-...] manager exiting with exitcode 0



File: python.info,  Node: The multiprocessing dummy module,  Prev: Logging<2>,  Up: Reference

5.16.6.26 The `multiprocessing.dummy' module
............................................

*note multiprocessing.dummy: 11b. replicates the API of *note
multiprocessing: 119. but is no more than a wrapper around the *note
threading: 179. module.


File: python.info,  Node: Programming guidelines,  Next: Examples<7>,  Prev: Reference,  Up: multiprocessing --- Process-based "threading" interface

5.16.6.27 Programming guidelines
................................

There are certain guidelines and idioms which should be adhered to when
using *note multiprocessing: 119.

* Menu:

* All platforms::
* Windows::


File: python.info,  Node: All platforms,  Next: Windows,  Up: Programming guidelines

5.16.6.28 All platforms
.......................

Avoid shared state

     As far as possible one should try to avoid shifting large amounts
     of data between processes.

     It is probably best to stick to using queues or pipes for
     communication between processes rather than using the lower level
     synchronization primitives from the *note threading: 179. module.

  Picklability

     Ensure that the arguments to the methods of proxies are picklable.

  Thread safety of proxies

     Do not use a proxy object from more than one thread unless you
     protect it with a lock.

     (There is never a problem with different processes using the
     _same_ proxy.)

  Joining zombie processes

     On Unix when a process finishes but has not been joined it becomes
     a zombie.  There should never be very many because each time a new
     process starts (or `active_children()' is called) all completed
     processes which have not yet been joined will be joined.  Also
     calling a finished process's `Process.is_alive()' will join the
     process.  Even so it is probably good practice to explicitly join
     all the processes that you start.

  Better to inherit than pickle/unpickle

     On Windows many types from *note multiprocessing: 119. need to be
     picklable so that child processes can use them.  However, one
     should generally avoid sending shared objects to other processes
     using pipes or queues.  Instead you should arrange the program so
     that a process which needs access to a shared resource created
     elsewhere can inherit it from an ancestor process.

  Avoid terminating processes

     Using the `Process.terminate()' method to stop a process is liable
     to cause any shared resources (such as locks, semaphores, pipes
     and queues) currently being used by the process to become broken
     or unavailable to other processes.

     Therefore it is probably best to only consider using
     `Process.terminate()' on processes which never use any shared
     resources.

  Joining processes that use queues

     Bear in mind that a process that has put items in a queue will
     wait before terminating until all the buffered items are fed by
     the "feeder" thread to the underlying pipe.  (The child process
     can call the *note cancel_join_thread(): 161d. method of the queue
     to avoid this behaviour.)

     This means that whenever you use a queue you need to make sure
     that all items which have been put on the queue will eventually be
     removed before the process is joined.  Otherwise you cannot be
     sure that processes which have put items on the queue will
     terminate.  Remember also that non-daemonic processes will be
     automatically be joined.

     An example which will deadlock is the following:

         from multiprocessing import Process, Queue

         def f(q):
             q.put('X' * 1000000)

         if __name__ == '__main__':
             queue = Queue()
             p = Process(target=f, args=(queue,))
             p.start()
             p.join()                    # this deadlocks
             obj = queue.get()

     A fix here would be to swap the last two lines round (or simply
     remove the `p.join()' line).

  Explicitly pass resources to child processes

     On Unix a child process can make use of a shared resource created
     in a parent process using a global resource.  However, it is
     better to pass the object as an argument to the constructor for
     the child process.

     Apart from making the code (potentially) compatible with Windows
     this also ensures that as long as the child process is still alive
     the object will not be garbage collected in the parent process.
     This might be important if some resource is freed when the object
     is garbage collected in the parent process.

     So for instance

         from multiprocessing import Process, Lock

         def f():
             ... do something using "lock" ...

         if __name__ == '__main__':
            lock = Lock()
            for i in range(10):
                 Process(target=f).start()

     should be rewritten as

         from multiprocessing import Process, Lock

         def f(l):
             ... do something using "l" ...

         if __name__ == '__main__':
            lock = Lock()
            for i in range(10):
                 Process(target=f, args=(lock,)).start()



  Beware of replacing *note sys.stdin: 623. with a "file like object"

     *note multiprocessing: 119. originally unconditionally called:

         os.close(sys.stdin.fileno())

     in the `multiprocessing.Process._bootstrap()' method -- this
     resulted in issues with processes-in-processes. This has been
     changed to:

         sys.stdin.close()
         sys.stdin = open(os.devnull)

     Which solves the fundamental issue of processes colliding with
     each other resulting in a bad file descriptor error, but
     introduces a potential danger to applications which replace *note
     sys.stdin(): 623. with a "file-like object" with output buffering.
     This danger is that if multiple processes call `close()' on this
     file-like object, it could result in the same data being flushed
     to the object multiple times, resulting in corruption.

     If you write a file-like object and implement your own caching,
     you can make it fork-safe by storing the pid whenever you append
     to the cache, and discarding the cache when the pid changes. For
     example:

         @property
         def cache(self):
             pid = os.getpid()
             if pid != self._pid:
                 self._pid = pid
                 self._cache = []
             return self._cache

     For more information, see issue 5155(1), issue 5313(2) and issue
     5331(3)

  ---------- Footnotes ----------

  (1) http://bugs.python.org/issue5155

  (2) http://bugs.python.org/issue5313

  (3) http://bugs.python.org/issue5331


File: python.info,  Node: Windows,  Prev: All platforms,  Up: Programming guidelines

5.16.6.29 Windows
.................

Since Windows lacks *note os.fork(): 241. it has a few extra
restrictions:

  More picklability

     Ensure that all arguments to `Process.__init__()' are picklable.
     This means, in particular, that bound or unbound methods cannot be
     used directly as the `target' argument on Windows -- just define a
     function and use that instead.

     Also, if you subclass `Process' then make sure that instances will
     be picklable when the `Process.start()' method is called.

  Global variables

     Bear in mind that if code run in a child process tries to access a
     global variable, then the value it sees (if any) may not be the
     same as the value in the parent process at the time that
     `Process.start()' was called.

     However, global variables which are just module level constants
     cause no problems.

  Safe importing of main module

     Make sure that the main module can be safely imported by a new
     Python interpreter without causing unintended side effects (such a
     starting a new process).

     For example, under Windows running the following module would fail
     with a *note RuntimeError: 394.:

         from multiprocessing import Process

         def foo():
             print 'hello'

         p = Process(target=foo)
         p.start()

     Instead one should protect the "entry point" of the program by
     using `if __name__ == '__main__':' as follows:

         from multiprocessing import Process, freeze_support

         def foo():
             print 'hello'

         if __name__ == '__main__':
             freeze_support()
             p = Process(target=foo)
             p.start()

     (The `freeze_support()' line can be omitted if the program will be
     run normally instead of frozen.)

     This allows the newly spawned Python interpreter to safely import
     the module and then run the module's `foo()' function.

     Similar restrictions apply if a pool or manager is created in the
     main module.


File: python.info,  Node: Examples<7>,  Prev: Programming guidelines,  Up: multiprocessing --- Process-based "threading" interface

5.16.6.30 Examples
..................

Demonstration of how to create and use customized managers and proxies:

    #
    # This module shows how to use arbitrary callables with a subclass of
    # `BaseManager`.
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    from multiprocessing import freeze_support
    from multiprocessing.managers import BaseManager, BaseProxy
    import operator

    ##

    class Foo(object):
        def f(self):
            print 'you called Foo.f()'
        def g(self):
            print 'you called Foo.g()'
        def _h(self):
            print 'you called Foo._h()'

    # A simple generator function
    def baz():
        for i in xrange(10):
            yield i*i

    # Proxy type for generator objects
    class GeneratorProxy(BaseProxy):
        _exposed_ = ('next', '__next__')
        def __iter__(self):
            return self
        def next(self):
            return self._callmethod('next')
        def __next__(self):
            return self._callmethod('__next__')

    # Function to return the operator module
    def get_operator_module():
        return operator

    ##

    class MyManager(BaseManager):
        pass

    # register the Foo class; make `f()` and `g()` accessible via proxy
    MyManager.register('Foo1', Foo)

    # register the Foo class; make `g()` and `_h()` accessible via proxy
    MyManager.register('Foo2', Foo, exposed=('g', '_h'))

    # register the generator function baz; use `GeneratorProxy` to make proxies
    MyManager.register('baz', baz, proxytype=GeneratorProxy)

    # register get_operator_module(); make public functions accessible via proxy
    MyManager.register('operator', get_operator_module)

    ##

    def test():
        manager = MyManager()
        manager.start()

        print '-' * 20

        f1 = manager.Foo1()
        f1.f()
        f1.g()
        assert not hasattr(f1, '_h')
        assert sorted(f1._exposed_) == sorted(['f', 'g'])

        print '-' * 20

        f2 = manager.Foo2()
        f2.g()
        f2._h()
        assert not hasattr(f2, 'f')
        assert sorted(f2._exposed_) == sorted(['g', '_h'])

        print '-' * 20

        it = manager.baz()
        for i in it:
            print '<%d>' % i,
        print

        print '-' * 20

        op = manager.operator()
        print 'op.add(23, 45) =', op.add(23, 45)
        print 'op.pow(2, 94) =', op.pow(2, 94)
        print 'op.getslice(range(10), 2, 6) =', op.getslice(range(10), 2, 6)
        print 'op.repeat(range(5), 3) =', op.repeat(range(5), 3)
        print 'op._exposed_ =', op._exposed_

    ##

    if __name__ == '__main__':
        freeze_support()
        test()

Using `Pool':

    #
    # A test of `multiprocessing.Pool` class
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import multiprocessing
    import time
    import random
    import sys

    #
    # Functions used by test code
    #

    def calculate(func, args):
        result = func(*args)
        return '%s says that %s%s = %s' % (
            multiprocessing.current_process().name,
            func.__name__, args, result
            )

    def calculatestar(args):
        return calculate(*args)

    def mul(a, b):
        time.sleep(0.5*random.random())
        return a * b

    def plus(a, b):
        time.sleep(0.5*random.random())
        return a + b

    def f(x):
        return 1.0 / (x-5.0)

    def pow3(x):
        return x**3

    def noop(x):
        pass

    #
    # Test code
    #

    def test():
        print 'cpu_count() = %d\n' % multiprocessing.cpu_count()

        #
        # Create pool
        #

        PROCESSES = 4
        print 'Creating pool with %d processes\n' % PROCESSES
        pool = multiprocessing.Pool(PROCESSES)
        print 'pool = %s' % pool
        print

        #
        # Tests
        #

        TASKS = [(mul, (i, 7)) for i in range(10)] + \
                [(plus, (i, 8)) for i in range(10)]

        results = [pool.apply_async(calculate, t) for t in TASKS]
        imap_it = pool.imap(calculatestar, TASKS)
        imap_unordered_it = pool.imap_unordered(calculatestar, TASKS)

        print 'Ordered results using pool.apply_async():'
        for r in results:
            print '\t', r.get()
        print

        print 'Ordered results using pool.imap():'
        for x in imap_it:
            print '\t', x
        print

        print 'Unordered results using pool.imap_unordered():'
        for x in imap_unordered_it:
            print '\t', x
        print

        print 'Ordered results using pool.map() --- will block till complete:'
        for x in pool.map(calculatestar, TASKS):
            print '\t', x
        print

        #
        # Simple benchmarks
        #

        N = 100000
        print 'def pow3(x): return x**3'

        t = time.time()
        A = map(pow3, xrange(N))
        print '\tmap(pow3, xrange(%d)):\n\t\t%s seconds' % \
              (N, time.time() - t)

        t = time.time()
        B = pool.map(pow3, xrange(N))
        print '\tpool.map(pow3, xrange(%d)):\n\t\t%s seconds' % \
              (N, time.time() - t)

        t = time.time()
        C = list(pool.imap(pow3, xrange(N), chunksize=N//8))
        print '\tlist(pool.imap(pow3, xrange(%d), chunksize=%d)):\n\t\t%s' \
              ' seconds' % (N, N//8, time.time() - t)

        assert A == B == C, (len(A), len(B), len(C))
        print

        L = [None] * 1000000
        print 'def noop(x): pass'
        print 'L = [None] * 1000000'

        t = time.time()
        A = map(noop, L)
        print '\tmap(noop, L):\n\t\t%s seconds' % \
              (time.time() - t)

        t = time.time()
        B = pool.map(noop, L)
        print '\tpool.map(noop, L):\n\t\t%s seconds' % \
              (time.time() - t)

        t = time.time()
        C = list(pool.imap(noop, L, chunksize=len(L)//8))
        print '\tlist(pool.imap(noop, L, chunksize=%d)):\n\t\t%s seconds' % \
              (len(L)//8, time.time() - t)

        assert A == B == C, (len(A), len(B), len(C))
        print

        del A, B, C, L

        #
        # Test error handling
        #

        print 'Testing error handling:'

        try:
            print pool.apply(f, (5,))
        except ZeroDivisionError:
            print '\tGot ZeroDivisionError as expected from pool.apply()'
        else:
            raise AssertionError('expected ZeroDivisionError')

        try:
            print pool.map(f, range(10))
        except ZeroDivisionError:
            print '\tGot ZeroDivisionError as expected from pool.map()'
        else:
            raise AssertionError('expected ZeroDivisionError')

        try:
            print list(pool.imap(f, range(10)))
        except ZeroDivisionError:
            print '\tGot ZeroDivisionError as expected from list(pool.imap())'
        else:
            raise AssertionError('expected ZeroDivisionError')

        it = pool.imap(f, range(10))
        for i in range(10):
            try:
                x = it.next()
            except ZeroDivisionError:
                if i == 5:
                    pass
            except StopIteration:
                break
            else:
                if i == 5:
                    raise AssertionError('expected ZeroDivisionError')

        assert i == 9
        print '\tGot ZeroDivisionError as expected from IMapIterator.next()'
        print

        #
        # Testing timeouts
        #

        print 'Testing ApplyResult.get() with timeout:',
        res = pool.apply_async(calculate, TASKS[0])
        while 1:
            sys.stdout.flush()
            try:
                sys.stdout.write('\n\t%s' % res.get(0.02))
                break
            except multiprocessing.TimeoutError:
                sys.stdout.write('.')
        print
        print

        print 'Testing IMapIterator.next() with timeout:',
        it = pool.imap(calculatestar, TASKS)
        while 1:
            sys.stdout.flush()
            try:
                sys.stdout.write('\n\t%s' % it.next(0.02))
            except StopIteration:
                break
            except multiprocessing.TimeoutError:
                sys.stdout.write('.')
        print
        print

        #
        # Testing callback
        #

        print 'Testing callback:'

        A = []
        B = [56, 0, 1, 8, 27, 64, 125, 216, 343, 512, 729]

        r = pool.apply_async(mul, (7, 8), callback=A.append)
        r.wait()

        r = pool.map_async(pow3, range(10), callback=A.extend)
        r.wait()

        if A == B:
            print '\tcallbacks succeeded\n'
        else:
            print '\t*** callbacks failed\n\t\t%s != %s\n' % (A, B)

        #
        # Check there are no outstanding tasks
        #

        assert not pool._cache, 'cache = %r' % pool._cache

        #
        # Check close() methods
        #

        print 'Testing close():'

        for worker in pool._pool:
            assert worker.is_alive()

        result = pool.apply_async(time.sleep, [0.5])
        pool.close()
        pool.join()

        assert result.get() is None

        for worker in pool._pool:
            assert not worker.is_alive()

        print '\tclose() succeeded\n'

        #
        # Check terminate() method
        #

        print 'Testing terminate():'

        pool = multiprocessing.Pool(2)
        DELTA = 0.1
        ignore = pool.apply(pow3, [2])
        results = [pool.apply_async(time.sleep, [DELTA]) for i in range(100)]
        pool.terminate()
        pool.join()

        for worker in pool._pool:
            assert not worker.is_alive()

        print '\tterminate() succeeded\n'

        #
        # Check garbage collection
        #

        print 'Testing garbage collection:'

        pool = multiprocessing.Pool(2)
        DELTA = 0.1
        processes = pool._pool
        ignore = pool.apply(pow3, [2])
        results = [pool.apply_async(time.sleep, [DELTA]) for i in range(100)]

        results = pool = None

        time.sleep(DELTA * 2)

        for worker in processes:
            assert not worker.is_alive()

        print '\tgarbage collection succeeded\n'


    if __name__ == '__main__':
        multiprocessing.freeze_support()

        assert len(sys.argv) in (1, 2)

        if len(sys.argv) == 1 or sys.argv[1] == 'processes':
            print ' Using processes '.center(79, '-')
        elif sys.argv[1] == 'threads':
            print ' Using threads '.center(79, '-')
            import multiprocessing.dummy as multiprocessing
        else:
            print 'Usage:\n\t%s [processes | threads]' % sys.argv[0]
            raise SystemExit(2)

        test()

Synchronization types like locks, conditions and queues:

    #
    # A test file for the `multiprocessing` package
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import time, sys, random
    from Queue import Empty

    import multiprocessing               # may get overwritten


    #### TEST_VALUE

    def value_func(running, mutex):
        random.seed()
        time.sleep(random.random()*4)

        mutex.acquire()
        print '\n\t\t\t' + str(multiprocessing.current_process()) + ' has finished'
        running.value -= 1
        mutex.release()

    def test_value():
        TASKS = 10
        running = multiprocessing.Value('i', TASKS)
        mutex = multiprocessing.Lock()

        for i in range(TASKS):
            p = multiprocessing.Process(target=value_func, args=(running, mutex))
            p.start()

        while running.value > 0:
            time.sleep(0.08)
            mutex.acquire()
            print running.value,
            sys.stdout.flush()
            mutex.release()

        print
        print 'No more running processes'


    #### TEST_QUEUE

    def queue_func(queue):
        for i in range(30):
            time.sleep(0.5 * random.random())
            queue.put(i*i)
        queue.put('STOP')

    def test_queue():
        q = multiprocessing.Queue()

        p = multiprocessing.Process(target=queue_func, args=(q,))
        p.start()

        o = None
        while o != 'STOP':
            try:
                o = q.get(timeout=0.3)
                print o,
                sys.stdout.flush()
            except Empty:
                print 'TIMEOUT'

        print


    #### TEST_CONDITION

    def condition_func(cond):
        cond.acquire()
        print '\t' + str(cond)
        time.sleep(2)
        print '\tchild is notifying'
        print '\t' + str(cond)
        cond.notify()
        cond.release()

    def test_condition():
        cond = multiprocessing.Condition()

        p = multiprocessing.Process(target=condition_func, args=(cond,))
        print cond

        cond.acquire()
        print cond
        cond.acquire()
        print cond

        p.start()

        print 'main is waiting'
        cond.wait()
        print 'main has woken up'

        print cond
        cond.release()
        print cond
        cond.release()

        p.join()
        print cond


    #### TEST_SEMAPHORE

    def semaphore_func(sema, mutex, running):
        sema.acquire()

        mutex.acquire()
        running.value += 1
        print running.value, 'tasks are running'
        mutex.release()

        random.seed()
        time.sleep(random.random()*2)

        mutex.acquire()
        running.value -= 1
        print '%s has finished' % multiprocessing.current_process()
        mutex.release()

        sema.release()

    def test_semaphore():
        sema = multiprocessing.Semaphore(3)
        mutex = multiprocessing.RLock()
        running = multiprocessing.Value('i', 0)

        processes = [
            multiprocessing.Process(target=semaphore_func,
                                    args=(sema, mutex, running))
            for i in range(10)
            ]

        for p in processes:
            p.start()

        for p in processes:
            p.join()


    #### TEST_JOIN_TIMEOUT

    def join_timeout_func():
        print '\tchild sleeping'
        time.sleep(5.5)
        print '\n\tchild terminating'

    def test_join_timeout():
        p = multiprocessing.Process(target=join_timeout_func)
        p.start()

        print 'waiting for process to finish'

        while 1:
            p.join(timeout=1)
            if not p.is_alive():
                break
            print '.',
            sys.stdout.flush()


    #### TEST_EVENT

    def event_func(event):
        print '\t%r is waiting' % multiprocessing.current_process()
        event.wait()
        print '\t%r has woken up' % multiprocessing.current_process()

    def test_event():
        event = multiprocessing.Event()

        processes = [multiprocessing.Process(target=event_func, args=(event,))
                     for i in range(5)]

        for p in processes:
            p.start()

        print 'main is sleeping'
        time.sleep(2)

        print 'main is setting event'
        event.set()

        for p in processes:
            p.join()


    #### TEST_SHAREDVALUES

    def sharedvalues_func(values, arrays, shared_values, shared_arrays):
        for i in range(len(values)):
            v = values[i][1]
            sv = shared_values[i].value
            assert v == sv

        for i in range(len(values)):
            a = arrays[i][1]
            sa = list(shared_arrays[i][:])
            assert a == sa

        print 'Tests passed'

    def test_sharedvalues():
        values = [
            ('i', 10),
            ('h', -2),
            ('d', 1.25)
            ]
        arrays = [
            ('i', range(100)),
            ('d', [0.25 * i for i in range(100)]),
            ('H', range(1000))
            ]

        shared_values = [multiprocessing.Value(id, v) for id, v in values]
        shared_arrays = [multiprocessing.Array(id, a) for id, a in arrays]

        p = multiprocessing.Process(
            target=sharedvalues_func,
            args=(values, arrays, shared_values, shared_arrays)
            )
        p.start()
        p.join()

        assert p.exitcode == 0


    ####

    def test(namespace=multiprocessing):
        global multiprocessing

        multiprocessing = namespace

        for func in [ test_value, test_queue, test_condition,
                      test_semaphore, test_join_timeout, test_event,
                      test_sharedvalues ]:

            print '\n\t######## %s\n' % func.__name__
            func()

        ignore = multiprocessing.active_children()      # cleanup any old processes
        if hasattr(multiprocessing, '_debug_info'):
            info = multiprocessing._debug_info()
            if info:
                print info
                raise ValueError('there should be no positive refcounts left')


    if __name__ == '__main__':
        multiprocessing.freeze_support()

        assert len(sys.argv) in (1, 2)

        if len(sys.argv) == 1 or sys.argv[1] == 'processes':
            print ' Using processes '.center(79, '-')
            namespace = multiprocessing
        elif sys.argv[1] == 'manager':
            print ' Using processes and a manager '.center(79, '-')
            namespace = multiprocessing.Manager()
            namespace.Process = multiprocessing.Process
            namespace.current_process = multiprocessing.current_process
            namespace.active_children = multiprocessing.active_children
        elif sys.argv[1] == 'threads':
            print ' Using threads '.center(79, '-')
            import multiprocessing.dummy as namespace
        else:
            print 'Usage:\n\t%s [processes | manager | threads]' % sys.argv[0]
            raise SystemExit(2)

        test(namespace)

An example showing how to use queues to feed tasks to a collection of
worker processes and collect the results:

    #
    # Simple example which uses a pool of workers to carry out some tasks.
    #
    # Notice that the results will probably not come out of the output
    # queue in the same in the same order as the corresponding tasks were
    # put on the input queue.  If it is important to get the results back
    # in the original order then consider using `Pool.map()` or
    # `Pool.imap()` (which will save on the amount of code needed anyway).
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import time
    import random

    from multiprocessing import Process, Queue, current_process, freeze_support

    #
    # Function run by worker processes
    #

    def worker(input, output):
        for func, args in iter(input.get, 'STOP'):
            result = calculate(func, args)
            output.put(result)

    #
    # Function used to calculate result
    #

    def calculate(func, args):
        result = func(*args)
        return '%s says that %s%s = %s' % \
            (current_process().name, func.__name__, args, result)

    #
    # Functions referenced by tasks
    #

    def mul(a, b):
        time.sleep(0.5*random.random())
        return a * b

    def plus(a, b):
        time.sleep(0.5*random.random())
        return a + b

    #
    #
    #

    def test():
        NUMBER_OF_PROCESSES = 4
        TASKS1 = [(mul, (i, 7)) for i in range(20)]
        TASKS2 = [(plus, (i, 8)) for i in range(10)]

        # Create queues
        task_queue = Queue()
        done_queue = Queue()

        # Submit tasks
        for task in TASKS1:
            task_queue.put(task)

        # Start worker processes
        for i in range(NUMBER_OF_PROCESSES):
            Process(target=worker, args=(task_queue, done_queue)).start()

        # Get and print results
        print 'Unordered results:'
        for i in range(len(TASKS1)):
            print '\t', done_queue.get()

        # Add more tasks using `put()`
        for task in TASKS2:
            task_queue.put(task)

        # Get and print some more results
        for i in range(len(TASKS2)):
            print '\t', done_queue.get()

        # Tell child processes to stop
        for i in range(NUMBER_OF_PROCESSES):
            task_queue.put('STOP')


    if __name__ == '__main__':
        freeze_support()
        test()

An example of how a pool of worker processes can each run a
`SimpleHTTPServer.HttpServer' instance while sharing a single listening
socket.

    #
    # Example where a pool of http servers share a single listening socket
    #
    # On Windows this module depends on the ability to pickle a socket
    # object so that the worker processes can inherit a copy of the server
    # object.  (We import `multiprocessing.reduction` to enable this pickling.)
    #
    # Not sure if we should synchronize access to `socket.accept()` method by
    # using a process-shared lock -- does not seem to be necessary.
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import os
    import sys

    from multiprocessing import Process, current_process, freeze_support
    from BaseHTTPServer import HTTPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler

    if sys.platform == 'win32':
        import multiprocessing.reduction    # make sockets pickable/inheritable


    def note(format, *args):
        sys.stderr.write('[%s]\t%s\n' % (current_process().name, format%args))


    class RequestHandler(SimpleHTTPRequestHandler):
        # we override log_message() to show which process is handling the request
        def log_message(self, format, *args):
            note(format, *args)

    def serve_forever(server):
        note('starting server')
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            pass


    def runpool(address, number_of_processes):
        # create a single server object -- children will each inherit a copy
        server = HTTPServer(address, RequestHandler)

        # create child processes to act as workers
        for i in range(number_of_processes-1):
            Process(target=serve_forever, args=(server,)).start()

        # main process also acts as a worker
        serve_forever(server)


    def test():
        DIR = os.path.join(os.path.dirname(__file__), '..')
        ADDRESS = ('localhost', 8000)
        NUMBER_OF_PROCESSES = 4

        print 'Serving at http://%s:%d using %d worker processes' % \
              (ADDRESS[0], ADDRESS[1], NUMBER_OF_PROCESSES)
        print 'To exit press Ctrl-' + ['C', 'Break'][sys.platform=='win32']

        os.chdir(DIR)
        runpool(ADDRESS, NUMBER_OF_PROCESSES)


    if __name__ == '__main__':
        freeze_support()
        test()

Some simple benchmarks comparing *note multiprocessing: 119. with *note
threading: 179.:

    #
    # Simple benchmarks for the multiprocessing package
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import time, sys, multiprocessing, threading, Queue, gc

    if sys.platform == 'win32':
        _timer = time.clock
    else:
        _timer = time.time

    delta = 1


    #### TEST_QUEUESPEED

    def queuespeed_func(q, c, iterations):
        a = '0' * 256
        c.acquire()
        c.notify()
        c.release()

        for i in xrange(iterations):
            q.put(a)

        q.put('STOP')

    def test_queuespeed(Process, q, c):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            p = Process(target=queuespeed_func, args=(q, c, iterations))
            c.acquire()
            p.start()
            c.wait()
            c.release()

            result = None
            t = _timer()

            while result != 'STOP':
                result = q.get()

            elapsed = _timer() - t

            p.join()

        print iterations, 'objects passed through the queue in', elapsed, 'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_PIPESPEED

    def pipe_func(c, cond, iterations):
        a = '0' * 256
        cond.acquire()
        cond.notify()
        cond.release()

        for i in xrange(iterations):
            c.send(a)

        c.send('STOP')

    def test_pipespeed():
        c, d = multiprocessing.Pipe()
        cond = multiprocessing.Condition()
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            p = multiprocessing.Process(target=pipe_func,
                                        args=(d, cond, iterations))
            cond.acquire()
            p.start()
            cond.wait()
            cond.release()

            result = None
            t = _timer()

            while result != 'STOP':
                result = c.recv()

            elapsed = _timer() - t
            p.join()

        print iterations, 'objects passed through connection in',elapsed,'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_SEQSPEED

    def test_seqspeed(seq):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            t = _timer()

            for i in xrange(iterations):
                a = seq[5]

            elapsed = _timer()-t

        print iterations, 'iterations in', elapsed, 'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_LOCK

    def test_lockspeed(l):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            t = _timer()

            for i in xrange(iterations):
                l.acquire()
                l.release()

            elapsed = _timer()-t

        print iterations, 'iterations in', elapsed, 'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_CONDITION

    def conditionspeed_func(c, N):
        c.acquire()
        c.notify()

        for i in xrange(N):
            c.wait()
            c.notify()

        c.release()

    def test_conditionspeed(Process, c):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            c.acquire()
            p = Process(target=conditionspeed_func, args=(c, iterations))
            p.start()

            c.wait()

            t = _timer()

            for i in xrange(iterations):
                c.notify()
                c.wait()

            elapsed = _timer()-t

            c.release()
            p.join()

        print iterations * 2, 'waits in', elapsed, 'seconds'
        print 'average number/sec:', iterations * 2 / elapsed

    ####

    def test():
        manager = multiprocessing.Manager()

        gc.disable()

        print '\n\t######## testing Queue.Queue\n'
        test_queuespeed(threading.Thread, Queue.Queue(),
                        threading.Condition())
        print '\n\t######## testing multiprocessing.Queue\n'
        test_queuespeed(multiprocessing.Process, multiprocessing.Queue(),
                        multiprocessing.Condition())
        print '\n\t######## testing Queue managed by server process\n'
        test_queuespeed(multiprocessing.Process, manager.Queue(),
                        manager.Condition())
        print '\n\t######## testing multiprocessing.Pipe\n'
        test_pipespeed()

        print

        print '\n\t######## testing list\n'
        test_seqspeed(range(10))
        print '\n\t######## testing list managed by server process\n'
        test_seqspeed(manager.list(range(10)))
        print '\n\t######## testing Array("i", ..., lock=False)\n'
        test_seqspeed(multiprocessing.Array('i', range(10), lock=False))
        print '\n\t######## testing Array("i", ..., lock=True)\n'
        test_seqspeed(multiprocessing.Array('i', range(10), lock=True))

        print

        print '\n\t######## testing threading.Lock\n'
        test_lockspeed(threading.Lock())
        print '\n\t######## testing threading.RLock\n'
        test_lockspeed(threading.RLock())
        print '\n\t######## testing multiprocessing.Lock\n'
        test_lockspeed(multiprocessing.Lock())
        print '\n\t######## testing multiprocessing.RLock\n'
        test_lockspeed(multiprocessing.RLock())
        print '\n\t######## testing lock managed by server process\n'
        test_lockspeed(manager.Lock())
        print '\n\t######## testing rlock managed by server process\n'
        test_lockspeed(manager.RLock())

        print

        print '\n\t######## testing threading.Condition\n'
        test_conditionspeed(threading.Thread, threading.Condition())
        print '\n\t######## testing multiprocessing.Condition\n'
        test_conditionspeed(multiprocessing.Process, multiprocessing.Condition())
        print '\n\t######## testing condition managed by a server process\n'
        test_conditionspeed(multiprocessing.Process, manager.Condition())

        gc.enable()

    if __name__ == '__main__':
        multiprocessing.freeze_support()
        test()



File: python.info,  Node: mmap --- Memory-mapped file support,  Next: readline --- GNU readline interface,  Prev: multiprocessing --- Process-based "threading" interface,  Up: Optional Operating System Services

5.16.7 `mmap' -- Memory-mapped file support
-------------------------------------------

Memory-mapped file objects behave like both strings and like file
objects.  Unlike normal string objects, however, these are mutable.
You can use mmap objects in most places where strings are expected; for
example, you can use the *note re: 143. module to search through a
memory-mapped file.  Since they're mutable, you can change a single
character by doing `obj[index] = 'a'', or change a substring by
assigning to a slice: `obj[i1:i2] = '...''.  You can also read and
write data starting at the current file position, and *note seek():
1680. through the file to different positions.

  A memory-mapped file is created by the *note mmap: 114. constructor,
which is different on Unix and on Windows.  In either case you must
provide a file descriptor for a file opened for update. If you wish to
map an existing Python file object, use its `fileno()' method to obtain
the correct value for the _fileno_ parameter.  Otherwise, you can open
the file using the *note os.open(): 5d5. function, which returns a file
descriptor directly (the file still needs to be closed when done).

     Note: If you want to create a memory-mapping for a writable,
     buffered file, you should *note flush(): 119a. the file first.
     This is necessary to ensure that local modifications to the
     buffers are actually available to the mapping.

  For both the Unix and Windows versions of the constructor, _access_
may be specified as an optional keyword parameter. _access_ accepts one
of three values: `ACCESS_READ', `ACCESS_WRITE', or `ACCESS_COPY' to
specify read-only, write-through or copy-on-write memory respectively.
_access_ can be used on both Unix and Windows.  If _access_ is not
specified, Windows mmap returns a write-through mapping.  The initial
memory values for all three access types are taken from the specified
file.  Assignment to an `ACCESS_READ' memory map raises a *note
TypeError: 215. exception.  Assignment to an `ACCESS_WRITE' memory map
affects both memory and the underlying file.  Assignment to an
`ACCESS_COPY' memory map affects memory but does not update the
underlying file.

  Changed in version 2.5: To map anonymous memory, -1 should be passed
as the fileno along with the length.

  Changed in version 2.6: mmap.mmap has formerly been a factory
function creating mmap objects. Now mmap.mmap is the class itself.

 -- Class: mmap.mmap (fileno, length[, tagname[, access[, offset]]])
     *(Windows version)* Maps _length_ bytes from the file specified by
     the file handle _fileno_, and creates a mmap object.  If _length_
     is larger than the current size of the file, the file is extended
     to contain _length_ bytes.  If _length_ is `0', the maximum length
     of the map is the current size of the file, except that if the
     file is empty Windows raises an exception (you cannot create an
     empty mapping on Windows).

     _tagname_, if specified and not `None', is a string giving a tag
     name for the mapping.  Windows allows you to have many different
     mappings against the same file.  If you specify the name of an
     existing tag, that tag is opened, otherwise a new tag of this name
     is created.  If this parameter is omitted or `None', the mapping
     is created without a name.  Avoiding the use of the tag parameter
     will assist in keeping your code portable between Unix and Windows.

     _offset_ may be specified as a non-negative integer offset. mmap
     references will be relative to the offset from the beginning of
     the file. _offset_ defaults to 0.  _offset_ must be a multiple of
     the ALLOCATIONGRANULARITY.

 -- Class: mmap.mmap (fileno, length[, flags[, prot[, access[,
          offset]]]])
     *(Unix version)* Maps _length_ bytes from the file specified by
     the file descriptor _fileno_, and returns a mmap object.  If
     _length_ is `0', the maximum length of the map will be the current
     size of the file when *note mmap: 114. is called.

     _flags_ specifies the nature of the mapping. `MAP_PRIVATE' creates
     a private copy-on-write mapping, so changes to the contents of the
     mmap object will be private to this process, and `MAP_SHARED'
     creates a mapping that's shared with all other processes mapping
     the same areas of the file.  The default value is `MAP_SHARED'.

     _prot_, if specified, gives the desired memory protection; the two
     most useful values are `PROT_READ' and `PROT_WRITE', to specify
     that the pages may be read or written.  _prot_ defaults to
     `PROT_READ | PROT_WRITE'.

     _access_ may be specified in lieu of _flags_ and _prot_ as an
     optional keyword parameter.  It is an error to specify both
     _flags_, _prot_ and _access_.  See the description of _access_
     above for information on how to use this parameter.

     _offset_ may be specified as a non-negative integer offset. mmap
     references will be relative to the offset from the beginning of
     the file. _offset_ defaults to 0.  _offset_ must be a multiple of
     the PAGESIZE or ALLOCATIONGRANULARITY.

     To ensure validity of the created memory mapping the file specified
     by the descriptor _fileno_ is internally automatically synchronized
     with physical backing store on Mac OS X and OpenVMS.

     This example shows a simple way of using *note mmap: 114.:

         import mmap

         # write a simple example file
         with open("hello.txt", "wb") as f:
             f.write("Hello Python!\n")

         with open("hello.txt", "r+b") as f:
             # memory-map the file, size 0 means whole file
             mm = mmap.mmap(f.fileno(), 0)
             # read content via standard file methods
             print mm.readline()  # prints "Hello Python!"
             # read content via slice notation
             print mm[:5]  # prints "Hello"
             # update content using slice notation;
             # note that new content must have same size
             mm[6:] = " world!\n"
             # ... and read again using standard file methods
             mm.seek(0)
             print mm.readline()  # prints "Hello  world!"
             # close the map
             mm.close()

     The next example demonstrates how to create an anonymous map and
     exchange data between the parent and child processes:

         import mmap
         import os

         mm = mmap.mmap(-1, 13)
         mm.write("Hello world!")

         pid = os.fork()

         if pid == 0: # In a child process
             mm.seek(0)
             print mm.readline()

             mm.close()

     Memory-mapped file objects support the following methods:

      -- Method: mmap.close ()
          Close the file.  Subsequent calls to other methods of the
          object will result in an exception being raised.

      -- Method: mmap.find (string[, start[, end]])
          Returns the lowest index in the object where the substring
          _string_ is found, such that _string_ is contained in the
          range [_start_, _end_].  Optional arguments _start_ and _end_
          are interpreted as in slice notation.  Returns `-1' on
          failure.

      -- Method: mmap.flush ([offset, size])
          Flushes changes made to the in-memory copy of a file back to
          disk. Without use of this call there is no guarantee that
          changes are written back before the object is destroyed.  If
          _offset_ and _size_ are specified, only changes to the given
          range of bytes will be flushed to disk; otherwise, the whole
          extent of the mapping is flushed.

          *(Windows version)* A nonzero value returned indicates
          success; zero indicates failure.

          *(Unix version)* A zero value is returned to indicate
          success. An exception is raised when the call failed.

      -- Method: mmap.move (dest, src, count)
          Copy the _count_ bytes starting at offset _src_ to the
          destination index _dest_.  If the mmap was created with
          `ACCESS_READ', then calls to move will raise a *note
          TypeError: 215. exception.

      -- Method: mmap.read (num)
          Return a string containing up to _num_ bytes starting from
          the current file position; the file position is updated to
          point after the bytes that were returned.

      -- Method: mmap.read_byte ()
          Returns a string of length 1 containing the character at the
          current file position, and advances the file position by 1.

      -- Method: mmap.readline ()
          Returns a single line, starting at the current file position
          and up to the next newline.

      -- Method: mmap.resize (newsize)
          Resizes the map and the underlying file, if any. If the mmap
          was created with `ACCESS_READ' or `ACCESS_COPY', resizing the
          map will raise a *note TypeError: 215. exception.

      -- Method: mmap.rfind (string[, start[, end]])
          Returns the highest index in the object where the substring
          _string_ is found, such that _string_ is contained in the
          range [_start_, _end_].  Optional arguments _start_ and _end_
          are interpreted as in slice notation.  Returns `-1' on
          failure.

      -- Method: mmap.seek (pos[, whence])
          Set the file's current position.  _whence_ argument is
          optional and defaults to `os.SEEK_SET' or `0' (absolute file
          positioning); other values are `os.SEEK_CUR' or `1' (seek
          relative to the current position) and `os.SEEK_END' or `2'
          (seek relative to the file's end).

      -- Method: mmap.size ()
          Return the length of the file, which can be larger than the
          size of the memory-mapped area.

      -- Method: mmap.tell ()
          Returns the current position of the file pointer.

      -- Method: mmap.write (string)
          Write the bytes in _string_ into memory at the current
          position of the file pointer; the file position is updated to
          point after the bytes that were written. If the mmap was
          created with `ACCESS_READ', then writing to it will raise a
          *note TypeError: 215. exception.

      -- Method: mmap.write_byte (byte)
          Write the single-character string _byte_ into memory at the
          current position of the file pointer; the file position is
          advanced by `1'. If the mmap was created with `ACCESS_READ',
          then writing to it will raise a *note TypeError: 215.
          exception.


File: python.info,  Node: readline --- GNU readline interface,  Next: rlcompleter --- Completion function for GNU readline,  Prev: mmap --- Memory-mapped file support,  Up: Optional Operating System Services

5.16.8 `readline' -- GNU readline interface
-------------------------------------------

The *note readline: 144. module defines a number of functions to
facilitate completion and reading/writing of history files from the
Python interpreter.  This module can be used directly or via the *note
rlcompleter: 149. module.  Settings made using  this module affect the
behaviour of both the interpreter's interactive prompt  and the prompts
offered by the *note raw_input(): 854. and *note input(): 3b8. built-in
functions.

     Note: On MacOS X the *note readline: 144. module can be
     implemented using the `libedit' library instead of GNU readline.

     The configuration file for `libedit' is different from that of GNU
     readline. If you programmatically load configuration strings you
     can check for the text "libedit" in `readline.__doc__' to
     differentiate between GNU readline and libedit.

  The *note readline: 144. module defines the following functions:

 -- Function: readline.parse_and_bind (string)
     Parse and execute single line of a readline init file.

 -- Function: readline.get_line_buffer ()
     Return the current contents of the line buffer.

 -- Function: readline.insert_text (string)
     Insert text into the command line.

 -- Function: readline.read_init_file ([filename])
     Parse a readline initialization file. The default filename is the
     last filename used.

 -- Function: readline.read_history_file ([filename])
     Load a readline history file. The default filename is `~/.history'.

 -- Function: readline.write_history_file ([filename])
     Save a readline history file. The default filename is `~/.history'.

 -- Function: readline.clear_history ()
     Clear the current history.  (Note: this function is not available
     if the installed version of GNU readline doesn't support it.)

     New in version 2.4.

 -- Function: readline.get_history_length ()
     Return the desired length of the history file.  Negative values
     imply unlimited history file size.

 -- Function: readline.set_history_length (length)
     Set the number of lines to save in the history file. *note
     write_history_file(): 1696.  uses this value to truncate the
     history file when saving.  Negative values imply unlimited history
     file size.

 -- Function: readline.get_current_history_length ()
     Return the number of lines currently in the history.  (This is
     different from *note get_history_length(): 1698, which returns the
     maximum number of lines that will be written to a history file.)

     New in version 2.3.

 -- Function: readline.get_history_item (index)
     Return the current contents of history item at _index_.

     New in version 2.3.

 -- Function: readline.remove_history_item (pos)
     Remove history item specified by its position from the history.

     New in version 2.4.

 -- Function: readline.replace_history_item (pos, line)
     Replace history item specified by its position with the given line.

     New in version 2.4.

 -- Function: readline.redisplay ()
     Change what's displayed on the screen to reflect the current
     contents of the line buffer.

     New in version 2.3.

 -- Function: readline.set_startup_hook ([function])
     Set or remove the startup_hook function.  If _function_ is
     specified, it will be used as the new startup_hook function; if
     omitted or `None', any hook function already installed is removed.
     The startup_hook function is called with no arguments just before
     readline prints the first prompt.

 -- Function: readline.set_pre_input_hook ([function])
     Set or remove the pre_input_hook function.  If _function_ is
     specified, it will be used as the new pre_input_hook function; if
     omitted or `None', any hook function already installed is removed.
     The pre_input_hook function is called with no arguments after the
     first prompt has been printed and just before readline starts
     reading input characters.

 -- Function: readline.set_completer ([function])
     Set or remove the completer function.  If _function_ is specified,
     it will be used as the new completer function; if omitted or
     `None', any completer function already installed is removed.  The
     completer function is called as `function(text, state)', for
     _state_ in `0', `1', `2', ..., until it returns a non-string
     value.  It should return the next possible completion starting
     with _text_.

 -- Function: readline.get_completer ()
     Get the completer function, or `None' if no completer function has
     been set.

     New in version 2.3.

 -- Function: readline.get_completion_type ()
     Get the type of completion being attempted.

     New in version 2.6.

 -- Function: readline.get_begidx ()
     Get the beginning index of the readline tab-completion scope.

 -- Function: readline.get_endidx ()
     Get the ending index of the readline tab-completion scope.

 -- Function: readline.set_completer_delims (string)
     Set the readline word delimiters for tab-completion.

 -- Function: readline.get_completer_delims ()
     Get the readline word delimiters for tab-completion.

 -- Function: readline.set_completion_display_matches_hook ([function])
     Set or remove the completion display function.  If _function_ is
     specified, it will be used as the new completion display function;
     if omitted or `None', any completion display function already
     installed is removed.  The completion display function is called as
     `function(substitution, [matches], longest_match_length)' once
     each time matches need to be displayed.

     New in version 2.6.

 -- Function: readline.add_history (line)
     Append a line to the history buffer, as if it was the last line
     typed.

See also
........

Module *note rlcompleter: 149.
     Completion of Python identifiers at the interactive prompt.

* Menu:

* Example: Example<7>.


File: python.info,  Node: Example<7>,  Up: readline --- GNU readline interface

5.16.8.1 Example
................

The following example demonstrates how to use the *note readline: 144.
module's history reading and writing functions to automatically load
and save a history file named `.pyhist' from the user's home directory.
The code below would normally be executed automatically during
interactive sessions from the user's *note PYTHONSTARTUP: 50b. file.

    import os
    import readline
    histfile = os.path.join(os.path.expanduser("~"), ".pyhist")
    try:
        readline.read_history_file(histfile)
    except IOError:
        pass
    import atexit
    atexit.register(readline.write_history_file, histfile)
    del os, histfile

The following example extends the *note code.InteractiveConsole: 16ac.
class to support history save/restore.

    import code
    import readline
    import atexit
    import os

    class HistoryConsole(code.InteractiveConsole):
        def __init__(self, locals=None, filename="<console>",
                     histfile=os.path.expanduser("~/.console-history")):
            code.InteractiveConsole.__init__(self, locals, filename)
            self.init_history(histfile)

        def init_history(self, histfile):
            readline.parse_and_bind("tab: complete")
            if hasattr(readline, "read_history_file"):
                try:
                    readline.read_history_file(histfile)
                except IOError:
                    pass
                atexit.register(self.save_history, histfile)

        def save_history(self, histfile):
            readline.write_history_file(histfile)



File: python.info,  Node: rlcompleter --- Completion function for GNU readline,  Prev: readline --- GNU readline interface,  Up: Optional Operating System Services

5.16.9 `rlcompleter' -- Completion function for GNU readline
------------------------------------------------------------

*Source code:* Lib/rlcompleter.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  The *note rlcompleter: 149. module defines a completion function
suitable for the *note readline: 144. module by completing valid Python
identifiers and keywords.

  When this module is imported on a Unix platform with the *note
readline: 144. module available, an instance of the `Completer' class
is automatically created and its `complete()' method is set as the
*note readline: 144. completer.

  Example:

    >>> import rlcompleter
    >>> import readline
    >>> readline.parse_and_bind("tab: complete")
    >>> readline. <TAB PRESSED>
    readline.__doc__          readline.get_line_buffer(  readline.read_init_file(
    readline.__file__         readline.insert_text(      readline.set_completer(
    readline.__name__         readline.parse_and_bind(
    >>> readline.

The *note rlcompleter: 149. module is designed for use with Python's
interactive mode.  A user can add the following lines to his or her
initialization file (identified by the *note PYTHONSTARTUP: 50b.
environment variable) to get automatic `Tab' completion:

    try:
        import readline
    except ImportError:
        print "Module readline not available."
    else:
        import rlcompleter
        readline.parse_and_bind("tab: complete")

On platforms without *note readline: 144, the `Completer' class defined
by this module can still be used for custom purposes.

* Menu:

* Completer Objects::

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/rlcompleter.py


File: python.info,  Node: Completer Objects,  Up: rlcompleter --- Completion function for GNU readline

5.16.9.1 Completer Objects
..........................

Completer objects have the following method:

 -- Method: Completer.complete (text, state)
     Return the _state_th completion for _text_.

     If called for _text_ that doesn't include a period character
     (`'.''), it will complete from names currently defined in *note
     __main__: 2, *note __builtin__: 0. and keywords (as defined by the
     *note keyword: fd. module).

     If called for a dotted name, it will try to evaluate anything
     without obvious side-effects (functions will not be evaluated, but
     it can generate calls to *note __getattr__(): 32a.) up to the last
     part, and find matches for the rest via the *note dir(): 333.
     function.  Any exception raised during the evaluation of the
     expression is caught, silenced and *note None: 393. is returned.


File: python.info,  Node: Interprocess Communication and Networking,  Next: Internet Data Handling,  Prev: Optional Operating System Services,  Up: The Python Standard Library

5.17 Interprocess Communication and Networking
==============================================

The modules described in this chapter provide mechanisms for different
processes to communicate.

  Some modules only work for two processes that are on the same
machine, e.g.  *note signal: 155. and *note subprocess: 167.  Other
modules support networking protocols that two or more processes can
used to communicate across machines.

  The list of modules described in this chapter is:

* Menu:

* subprocess: subprocess --- Subprocess management. Subprocess management
* socket: socket --- Low-level networking interface. Low-level networking interface
* ssl: ssl --- TLS/SSL wrapper for socket objects. TLS/SSL wrapper for socket objects
* signal: signal --- Set handlers for asynchronous events. Set handlers for asynchronous events
* popen2: popen2 --- Subprocesses with accessible I/O streams. Subprocesses with accessible I/O streams
* asyncore: asyncore --- Asynchronous socket handler. Asynchronous socket handler
* asynchat: asynchat --- Asynchronous socket command/response handler. Asynchronous socket command/response handler

subprocess --- Subprocess management

* Using the subprocess Module::
* Popen Objects::
* Windows Popen Helpers::
* Replacing Older Functions with the subprocess Module::
* Notes::

Using the subprocess Module

* Frequently Used Arguments::
* Popen Constructor::
* Exceptions: Exceptions<4>.
* Security::

Windows Popen Helpers

* Constants: Constants<4>.

Replacing Older Functions with the subprocess Module

* Replacing /bin/sh shell backquote::
* Replacing shell pipeline::
* Replacing os.system(): Replacing os system.
* Replacing the os.spawn family: Replacing the os spawn family.
* Replacing os.popen(), os.popen2(), os.popen3(): Replacing os popen os popen2 os popen3.
* Replacing functions from the popen2 module::

Notes

* Converting an argument sequence to a string on Windows::

socket --- Low-level networking interface

* Socket Objects::
* Example: Example<8>.

ssl --- TLS/SSL wrapper for socket objects

* Functions, Constants, and Exceptions: Functions Constants and Exceptions.
* SSLSocket Objects::
* Certificates::
* Examples: Examples<8>.

Examples

* Testing for SSL support::
* Client-side operation::
* Server-side operation::

signal --- Set handlers for asynchronous events

* Example: Example<9>.

popen2 --- Subprocesses with accessible I/O streams

* Popen3 and Popen4 Objects::
* Flow Control Issues::

asyncore --- Asynchronous socket handler

* asyncore Example basic HTTP client::
* asyncore Example basic echo server::

asynchat --- Asynchronous socket command/response handler

* asynchat - Auxiliary Classes::
* asynchat Example::


File: python.info,  Node: subprocess --- Subprocess management,  Next: socket --- Low-level networking interface,  Up: Interprocess Communication and Networking

5.17.1 `subprocess' -- Subprocess management
--------------------------------------------

New in version 2.4.

  The *note subprocess: 167. module allows you to spawn new processes,
connect to their input/output/error pipes, and obtain their return
codes.  This module intends to replace several other, older modules and
functions, such as:

    os.system
    os.spawn*
    os.popen*
    popen2.*
    commands.*

Information about how the *note subprocess: 167. module can be used to
replace these modules and functions can be found in the following
sections.

See also
........

PEP 324(1) - PEP proposing the subprocess module

* Menu:

* Using the subprocess Module::
* Popen Objects::
* Windows Popen Helpers::
* Replacing Older Functions with the subprocess Module::
* Notes::

Using the subprocess Module

* Frequently Used Arguments::
* Popen Constructor::
* Exceptions: Exceptions<4>.
* Security::

Windows Popen Helpers

* Constants: Constants<4>.

Replacing Older Functions with the subprocess Module

* Replacing /bin/sh shell backquote::
* Replacing shell pipeline::
* Replacing os.system(): Replacing os system.
* Replacing the os.spawn family: Replacing the os spawn family.
* Replacing os.popen(), os.popen2(), os.popen3(): Replacing os popen os popen2 os popen3.
* Replacing functions from the popen2 module::

Notes

* Converting an argument sequence to a string on Windows::

  ---------- Footnotes ----------

  (1) http://www.python.org/dev/peps/pep-0324


File: python.info,  Node: Using the subprocess Module,  Next: Popen Objects,  Up: subprocess --- Subprocess management

5.17.1.1 Using the `subprocess' Module
......................................

The recommended approach to invoking subprocesses is to use the
following convenience functions for all use cases they can handle. For
more advanced use cases, the underlying *note Popen: 16b8. interface
can be used directly.

 -- Function: subprocess.call (args, *, stdin=None, stdout=None,
          stderr=None, shell=False)
     Run the command described by _args_.  Wait for command to
     complete, then return the `returncode' attribute.

     The arguments shown above are merely the most common ones,
     described below in *note Frequently Used Arguments: 16ba. (hence
     the slightly odd notation in the abbreviated signature). The full
     function signature is the same as that of the *note Popen: 16b8.
     constructor - this functions passes all supplied arguments
     directly through to that interface.

     Examples:

         >>> subprocess.call(["ls", "-l"])
         0

         >>> subprocess.call("exit 1", shell=True)
         1


          Warning: Invoking the system shell with `shell=True' can be a
          security hazard if combined with untrusted input. See the
          warning under *note Frequently Used Arguments: 16ba. for
          details.

          Note: Do not use `stdout=PIPE' or `stderr=PIPE' with this
          function. As the pipes are not being read in the current
          process, the child process may block if it generates enough
          output to a pipe to fill up the OS pipe buffer.

 -- Function: subprocess.check_call (args, *, stdin=None, stdout=None,
          stderr=None, shell=False)
     Run command with arguments.  Wait for command to complete. If the
     return code was zero then return, otherwise raise *note
     CalledProcessError: 25f. The *note CalledProcessError: 25f. object
     will have the return code in the `returncode' attribute.

     The arguments shown above are merely the most common ones,
     described below in *note Frequently Used Arguments: 16ba. (hence
     the slightly odd notation in the abbreviated signature). The full
     function signature is the same as that of the *note Popen: 16b8.
     constructor - this functions passes all supplied arguments
     directly through to that interface.

     Examples:

         >>> subprocess.check_call(["ls", "-l"])
         0

         >>> subprocess.check_call("exit 1", shell=True)
         Traceback (most recent call last):
            ...
         subprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1

     New in version 2.5.

          Warning: Invoking the system shell with `shell=True' can be a
          security hazard if combined with untrusted input. See the
          warning under *note Frequently Used Arguments: 16ba. for
          details.

          Note: Do not use `stdout=PIPE' or `stderr=PIPE' with this
          function. As the pipes are not being read in the current
          process, the child process may block if it generates enough
          output to a pipe to fill up the OS pipe buffer.

 -- Function: subprocess.check_output (args, *, stdin=None,
          stderr=None, shell=False, universal_newlines=False)
     Run command with arguments and return its output as a byte string.

     If the return code was non-zero it raises a *note
     CalledProcessError: 25f. The *note CalledProcessError: 25f. object
     will have the return code in the `returncode' attribute and any
     output in the `output' attribute.

     The arguments shown above are merely the most common ones,
     described below in *note Frequently Used Arguments: 16ba. (hence
     the slightly odd notation in the abbreviated signature). The full
     function signature is largely the same as that of the *note Popen:
     16b8. constructor, except that _stdout_ is not permitted as it is
     used internally. All other supplied arguments are passed directly
     through to the *note Popen: 16b8. constructor.

     Examples:

         >>> subprocess.check_output(["echo", "Hello World!"])
         'Hello World!\n'

         >>> subprocess.check_output("exit 1", shell=True)
         Traceback (most recent call last):
            ...
         subprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1

     To also capture standard error in the result, use
     `stderr=subprocess.STDOUT':

         >>> subprocess.check_output(
         ...     "ls non_existent_file; exit 0",
         ...     stderr=subprocess.STDOUT,
         ...     shell=True)
         'ls: non_existent_file: No such file or directory\n'

     New in version 2.7.

          Warning: Invoking the system shell with `shell=True' can be a
          security hazard if combined with untrusted input. See the
          warning under *note Frequently Used Arguments: 16ba. for
          details.

          Note: Do not use `stderr=PIPE' with this function. As the
          pipe is not being read in the current process, the child
          process may block if it generates enough output to the pipe
          to fill up the OS pipe buffer.

 -- Data: subprocess.PIPE
     Special value that can be used as the _stdin_, _stdout_ or
     _stderr_ argument to *note Popen: 16b8. and indicates that a pipe
     to the standard stream should be opened.

 -- Data: subprocess.STDOUT
     Special value that can be used as the _stderr_ argument to *note
     Popen: 16b8. and indicates that standard error should go into the
     same handle as standard output.

 -- Exception: subprocess.CalledProcessError
     Exception raised when a process run by *note check_call(): 16bb. or
     *note check_output(): 25e. returns a non-zero exit status.

      -- Attribute: returncode
          Exit status of the child process.

      -- Attribute: cmd
          Command that was used to spawn the child process.

      -- Attribute: output
          Output of the child process if this exception is raised by
          *note check_output(): 25e.  Otherwise, `None'.

* Menu:

* Frequently Used Arguments::
* Popen Constructor::
* Exceptions: Exceptions<4>.
* Security::


File: python.info,  Node: Frequently Used Arguments,  Next: Popen Constructor,  Up: Using the subprocess Module

5.17.1.2 Frequently Used Arguments
..................................

To support a wide variety of use cases, the *note Popen: 16b8.
constructor (and the convenience functions) accept a large number of
optional arguments. For most typical use cases, many of these arguments
can be safely left at their default values. The arguments that are most
commonly needed are:

     _args_ is required for all calls and should be a string, or a
     sequence of program arguments. Providing a sequence of arguments
     is generally preferred, as it allows the module to take care of
     any required escaping and quoting of arguments (e.g. to permit
     spaces in file names). If passing a single string, either _shell_
     must be *note True: 3a9. (see below) or else the string must
     simply name the program to be executed without specifying any
     arguments.

     _stdin_, _stdout_ and _stderr_ specify the executed program's
     standard input, standard output and standard error file handles,
     respectively.  Valid values are *note PIPE: 16bc, an existing file
     descriptor (a positive integer), an existing file object, and
     `None'.  *note PIPE: 16bc. indicates that a new pipe to the child
     should be created.  With the default settings of `None', no
     redirection will occur; the child's file handles will be inherited
     from the parent.  Additionally, _stderr_ can be *note STDOUT:
     16bd, which indicates that the stderr data from the child process
     should be captured into the same file handle as for stdout.

     When _stdout_ or _stderr_ are pipes and _universal_newlines_ is
     `True' then all line endings will be converted to `'\n'' as
     described for the *note universal newlines: 30e. `'U'' mode
     argument to *note open(): 2d3.

     If _shell_ is `True', the specified command will be executed
     through the shell.  This can be useful if you are using Python
     primarily for the enhanced control flow it offers over most system
     shells and still want convenient access to other shell features
     such as shell pipes, filename wildcards, environment variable
     expansion, and expansion of `~' to a user's home directory.
     However, note that Python itself offers implementations of many
     shell-like features (in particular, *note glob: e3, *note fnmatch:
     d2, *note os.walk(): 34c, *note os.path.expandvars(): 34d, *note
     os.path.expanduser(): dda, and *note shutil: 154.).

          Warning: Executing shell commands that incorporate
          unsanitized input from an untrusted source makes a program
          vulnerable to shell injection(1), a serious security flaw
          which can result in arbitrary command execution.  For this
          reason, the use of `shell=True' is *strongly discouraged* in
          cases where the command string is constructed from external
          input:

              >>> from subprocess import call
              >>> filename = input("What file would you like to display?\n")
              What file would you like to display?
              non_existent; rm -rf / #
              >>> call("cat " + filename, shell=True) # Uh-oh. This will end badly...

          `shell=False' disables all shell based features, but does not
          suffer from this vulnerability; see the Note in the *note
          Popen: 16b8. constructor documentation for helpful hints in
          getting `shell=False' to work.

          When using `shell=True', *note pipes.quote(): 16c2. can be
          used to properly escape whitespace and shell metacharacters
          in strings that are going to be used to construct shell
          commands.

  These options, along with all of the other options, are described in
more detail in the *note Popen: 16b8. constructor documentation.

  ---------- Footnotes ----------

  (1) http://en.wikipedia.org/wiki/Shell_injection#Shell_injection


File: python.info,  Node: Popen Constructor,  Next: Exceptions<4>,  Prev: Frequently Used Arguments,  Up: Using the subprocess Module

5.17.1.3 Popen Constructor
..........................

The underlying process creation and management in this module is
handled by the *note Popen: 16b8. class. It offers a lot of flexibility
so that developers are able to handle the less common cases not covered
by the convenience functions.

 -- Class: subprocess.Popen (args, bufsize=0, executable=None,
          stdin=None, stdout=None, stderr=None, preexec_fn=None,
          close_fds=False, shell=False, cwd=None, env=None,
          universal_newlines=False, startupinfo=None, creationflags=0)
     Execute a child program in a new process.  On Unix, the class uses
     *note os.execvp(): 1141.-like behavior to execute the child
     program.  On Windows, the class uses the Windows `CreateProcess()'
     function.  The arguments to *note Popen: 16b8. are as follows.

     _args_ should be a sequence of program arguments or else a single
     string.  By default, the program to execute is the first item in
     _args_ if _args_ is a sequence.  If _args_ is a string, the
     interpretation is platform-dependent and described below.  See the
     _shell_ and _executable_ arguments for additional differences from
     the default behavior.  Unless otherwise stated, it is recommended
     to pass _args_ as a sequence.

     On Unix, if _args_ is a string, the string is interpreted as the
     name or path of the program to execute.  However, this can only be
     done if not passing arguments to the program.

          Note: *note shlex.split(): 16c4. can be useful when
          determining the correct tokenization for _args_, especially
          in complex cases:

              >>> import shlex, subprocess
              >>> command_line = raw_input()
              /bin/vikings -input eggs.txt -output "spam spam.txt" -cmd "echo '$MONEY'"
              >>> args = shlex.split(command_line)
              >>> print args
              ['/bin/vikings', '-input', 'eggs.txt', '-output', 'spam spam.txt', '-cmd', "echo '$MONEY'"]
              >>> p = subprocess.Popen(args) # Success!

          Note in particular that options (such as _-input_) and
          arguments (such as _eggs.txt_) that are separated by
          whitespace in the shell go in separate list elements, while
          arguments that need quoting or backslash escaping when used
          in the shell (such as filenames containing spaces or the
          _echo_ command shown above) are single list elements.

     On Windows, if _args_ is a sequence, it will be converted to a
     string in a manner described in *note Converting an argument
     sequence to a string on Windows: 16c5.  This is because the
     underlying `CreateProcess()' operates on strings.

     The _shell_ argument (which defaults to _False_) specifies whether
     to use the shell as the program to execute.  If _shell_ is _True_,
     it is recommended to pass _args_ as a string rather than as a
     sequence.

     On Unix with `shell=True', the shell defaults to `/bin/sh'.  If
     _args_ is a string, the string specifies the command to execute
     through the shell.  This means that the string must be formatted
     exactly as it would be when typed at the shell prompt.  This
     includes, for example, quoting or backslash escaping filenames
     with spaces in them.  If _args_ is a sequence, the first item
     specifies the command string, and any additional items will be
     treated as additional arguments to the shell itself.  That is to
     say, *note Popen: 16b8. does the equivalent of:

         Popen(['/bin/sh', '-c', args[0], args[1], ...])

     On Windows with `shell=True', the `COMSPEC' environment variable
     specifies the default shell.  The only time you need to specify
     `shell=True' on Windows is when the command you wish to execute is
     built into the shell (e.g. *dir* or *copy*).  You do not need
     `shell=True' to run a batch file or console-based executable.

          Warning: Passing `shell=True' can be a security hazard if
          combined with untrusted input.  See the warning under *note
          Frequently Used Arguments: 16ba.  for details.

     _bufsize_, if given, has the same meaning as the corresponding
     argument to the built-in open() function: `0' means unbuffered,
     `1' means line buffered, any other positive value means use a
     buffer of (approximately) that size.  A negative _bufsize_ means
     to use the system default, which usually means fully buffered.
     The default value for _bufsize_ is `0' (unbuffered).

          Note: If you experience performance issues, it is recommended
          that you try to enable buffering by setting _bufsize_ to
          either -1 or a large enough positive value (such as 4096).

     The _executable_ argument specifies a replacement program to
     execute.   It is very seldom needed.  When `shell=False',
     _executable_ replaces the program to execute specified by _args_.
     However, the original _args_ is still passed to the program.  Most
     programs treat the program specified by _args_ as the command
     name, which can then be different from the program actually
     executed.  On Unix, the _args_ name becomes the display name for
     the executable in utilities such as *ps*.  If `shell=True', on
     Unix the _executable_ argument specifies a replacement shell for
     the default `/bin/sh'.

     _stdin_, _stdout_ and _stderr_ specify the executed program's
     standard input, standard output and standard error file handles,
     respectively.  Valid values are *note PIPE: 16bc, an existing file
     descriptor (a positive integer), an existing file object, and
     `None'.  *note PIPE: 16bc. indicates that a new pipe to the child
     should be created.  With the default settings of `None', no
     redirection will occur; the child's file handles will be inherited
     from the parent.  Additionally, _stderr_ can be *note STDOUT:
     16bd, which indicates that the stderr data from the child process
     should be captured into the same file handle as for stdout.

     If _preexec_fn_ is set to a callable object, this object will be
     called in the child process just before the child is executed.
     (Unix only)

     If _close_fds_ is true, all file descriptors except `0', `1' and
     `2' will be closed before the child process is executed. (Unix
     only).  Or, on Windows, if _close_fds_ is true then no handles
     will be inherited by the child process.  Note that on Windows, you
     cannot set _close_fds_ to true and also redirect the standard
     handles by setting _stdin_, _stdout_ or _stderr_.

     If _cwd_ is not `None', the child's current directory will be
     changed to _cwd_ before it is executed.  Note that this directory
     is not considered when searching the executable, so you can't
     specify the program's path relative to _cwd_.

     If _env_ is not `None', it must be a mapping that defines the
     environment variables for the new process; these are used instead
     of inheriting the current process' environment, which is the
     default behavior.

          Note: If specified, _env_ must provide any variables required
          for the program to execute.  On Windows, in order to run a
          side-by-side assembly(1) the specified _env_ *must* include a
          valid `SystemRoot'.

     If _universal_newlines_ is `True', the file objects _stdout_ and
     _stderr_ are opened as text files in *note universal newlines:
     30e. mode.  Lines may be terminated by any of `'\n'', the Unix
     end-of-line convention, `'\r'', the old Macintosh convention or
     `'\r\n'', the Windows convention. All of these external
     representations are seen as `'\n'' by the Python program.

          Note: This feature is only available if Python is built with
          universal newline support (the default).  Also, the newlines
          attribute of the file objects *note stdout: 16c6, *note
          stdin: 16c7. and *note stderr: 16c8. are not updated by the
          communicate() method.

     If given, _startupinfo_ will be a *note STARTUPINFO: 16c9. object,
     which is passed to the underlying `CreateProcess' function.
     _creationflags_, if given, can be *note CREATE_NEW_CONSOLE: 16ca.
     or *note CREATE_NEW_PROCESS_GROUP: 16cb. (Windows only)

  ---------- Footnotes ----------

  (1) http://en.wikipedia.org/wiki/Side-by-Side_Assembly


File: python.info,  Node: Exceptions<4>,  Next: Security,  Prev: Popen Constructor,  Up: Using the subprocess Module

5.17.1.4 Exceptions
...................

Exceptions raised in the child process, before the new program has
started to execute, will be re-raised in the parent.  Additionally, the
exception object will have one extra attribute called
`child_traceback', which is a string containing traceback information
from the child's point of view.

  The most common exception raised is *note OSError: 22e.  This occurs,
for example, when trying to execute a non-existent file.  Applications
should prepare for *note OSError: 22e. exceptions.

  A *note ValueError: 233. will be raised if *note Popen: 16b8. is
called with invalid arguments.

  *note check_call(): 16bb. and *note check_output(): 25e. will raise
*note CalledProcessError: 25f. if the called process returns a non-zero
return code.


File: python.info,  Node: Security,  Prev: Exceptions<4>,  Up: Using the subprocess Module

5.17.1.5 Security
.................

Unlike some other popen functions, this implementation will never call a
system shell implicitly.  This means that all characters, including
shell metacharacters, can safely be passed to child processes.
Obviously, if the shell is invoked explicitly, then it is the
application's responsibility to ensure that all whitespace and
metacharacters are quoted appropriately.


File: python.info,  Node: Popen Objects,  Next: Windows Popen Helpers,  Prev: Using the subprocess Module,  Up: subprocess --- Subprocess management

5.17.1.6 Popen Objects
......................

Instances of the *note Popen: 16b8. class have the following methods:

 -- Method: Popen.poll ()
     Check if child process has terminated.  Set and return *note
     returncode: 16d0.  attribute.

 -- Method: Popen.wait ()
     Wait for child process to terminate.  Set and return *note
     returncode: 16d0.  attribute.

          Warning: This will deadlock when using `stdout=PIPE' and/or
          `stderr=PIPE' and the child process generates enough output to
          a pipe such that it blocks waiting for the OS pipe buffer to
          accept more data.  Use *note communicate(): 16d2. to avoid
          that.

 -- Method: Popen.communicate (input=None)
     Interact with process: Send data to stdin.  Read data from stdout
     and stderr, until end-of-file is reached.  Wait for process to
     terminate. The optional _input_ argument should be a string to be
     sent to the child process, or `None', if no data should be sent to
     the child.

     *note communicate(): 16d2. returns a tuple `(stdoutdata,
     stderrdata)'.

     Note that if you want to send data to the process's stdin, you
     need to create the Popen object with `stdin=PIPE'.  Similarly, to
     get anything other than `None' in the result tuple, you need to
     give `stdout=PIPE' and/or `stderr=PIPE' too.

          Note: The data read is buffered in memory, so do not use this
          method if the data size is large or unlimited.

 -- Method: Popen.send_signal (signal)
     Sends the signal _signal_ to the child.

          Note: On Windows, SIGTERM is an alias for *note terminate():
          16d4. CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to
          processes started with a _creationflags_ parameter which
          includes `CREATE_NEW_PROCESS_GROUP'.

     New in version 2.6.

 -- Method: Popen.terminate ()
     Stop the child. On Posix OSs the method sends SIGTERM to the
     child. On Windows the Win32 API function `TerminateProcess()' is
     called to stop the child.

     New in version 2.6.

 -- Method: Popen.kill ()
     Kills the child. On Posix OSs the function sends SIGKILL to the
     child.  On Windows *note kill(): 16d5. is an alias for *note
     terminate(): 16d4.

     New in version 2.6.

  The following attributes are also available:

     Warning: Use *note communicate(): 16d2. rather than *note
     .stdin.write: 16c7, *note .stdout.read: 16c6. or *note
     .stderr.read: 16c8. to avoid deadlocks due to any of the other OS
     pipe buffers filling up and blocking the child process.

 -- Attribute: Popen.stdin
     If the _stdin_ argument was *note PIPE: 16bc, this attribute is a
     file object that provides input to the child process.  Otherwise,
     it is `None'.

 -- Attribute: Popen.stdout
     If the _stdout_ argument was *note PIPE: 16bc, this attribute is a
     file object that provides output from the child process.
     Otherwise, it is `None'.

 -- Attribute: Popen.stderr
     If the _stderr_ argument was *note PIPE: 16bc, this attribute is a
     file object that provides error output from the child process.
     Otherwise, it is `None'.

 -- Attribute: Popen.pid
     The process ID of the child process.

     Note that if you set the _shell_ argument to `True', this is the
     process ID of the spawned shell.

 -- Attribute: Popen.returncode
     The child return code, set by *note poll(): 16cf. and *note
     wait(): 16d1. (and indirectly by *note communicate(): 16d2.).  A
     `None' value indicates that the process hasn't terminated yet.

     A negative value `-N' indicates that the child was terminated by
     signal `N' (Unix only).


File: python.info,  Node: Windows Popen Helpers,  Next: Replacing Older Functions with the subprocess Module,  Prev: Popen Objects,  Up: subprocess --- Subprocess management

5.17.1.7 Windows Popen Helpers
..............................

The *note STARTUPINFO: 16c9. class and following constants are only
available on Windows.

 -- Class: subprocess.STARTUPINFO
     Partial support of the Windows STARTUPINFO(1) structure is used
     for *note Popen: 16b8. creation.

      -- Attribute: dwFlags
          A bit field that determines whether certain *note
          STARTUPINFO: 16c9.  attributes are used when the process
          creates a window.

              si = subprocess.STARTUPINFO()
              si.dwFlags = subprocess.STARTF_USESTDHANDLES | subprocess.STARTF_USESHOWWINDOW



      -- Attribute: hStdInput
          If *note dwFlags: 16d8. specifies *note STARTF_USESTDHANDLES:
          16da, this attribute is the standard input handle for the
          process. If *note STARTF_USESTDHANDLES: 16da. is not
          specified, the default for standard input is the keyboard
          buffer.

      -- Attribute: hStdOutput
          If *note dwFlags: 16d8. specifies *note STARTF_USESTDHANDLES:
          16da, this attribute is the standard output handle for the
          process. Otherwise, this attribute is ignored and the default
          for standard output is the console window's buffer.

      -- Attribute: hStdError
          If *note dwFlags: 16d8. specifies *note STARTF_USESTDHANDLES:
          16da, this attribute is the standard error handle for the
          process. Otherwise, this attribute is ignored and the default
          for standard error is the console window's buffer.

      -- Attribute: wShowWindow
          If *note dwFlags: 16d8. specifies *note STARTF_USESHOWWINDOW:
          16de, this attribute can be any of the values that can be
          specified in the `nCmdShow' parameter for the ShowWindow(2)
          function, except for `SW_SHOWDEFAULT'. Otherwise, this
          attribute is ignored.

          *note SW_HIDE: 16df. is provided for this attribute. It is
          used when *note Popen: 16b8. is called with `shell=True'.

* Menu:

* Constants: Constants<4>.

  ---------- Footnotes ----------

  (1) http://msdn.microsoft.com/en-us/library/ms686331(v=vs.85).aspx

  (2) http://msdn.microsoft.com/en-us/library/ms633548(v=vs.85).aspx


File: python.info,  Node: Constants<4>,  Up: Windows Popen Helpers

5.17.1.8 Constants
..................

The *note subprocess: 167. module exposes the following constants.

 -- Data: subprocess.STD_INPUT_HANDLE
     The standard input device. Initially, this is the console input
     buffer, `CONIN$'.

 -- Data: subprocess.STD_OUTPUT_HANDLE
     The standard output device. Initially, this is the active console
     screen buffer, `CONOUT$'.

 -- Data: subprocess.STD_ERROR_HANDLE
     The standard error device. Initially, this is the active console
     screen buffer, `CONOUT$'.

 -- Data: subprocess.SW_HIDE
     Hides the window. Another window will be activated.

 -- Data: subprocess.STARTF_USESTDHANDLES
     Specifies that the *note STARTUPINFO.hStdInput: 16d9, *note
     STARTUPINFO.hStdOutput: 16db, and *note STARTUPINFO.hStdError:
     16dc. attributes contain additional information.

 -- Data: subprocess.STARTF_USESHOWWINDOW
     Specifies that the *note STARTUPINFO.wShowWindow: 16dd. attribute
     contains additional information.

 -- Data: subprocess.CREATE_NEW_CONSOLE
     The new process has a new console, instead of inheriting its
     parent's console (the default).

     This flag is always set when *note Popen: 16b8. is created with
     `shell=True'.

 -- Data: subprocess.CREATE_NEW_PROCESS_GROUP
     A *note Popen: 16b8. `creationflags' parameter to specify that a
     new process group will be created. This flag is necessary for
     using *note os.kill(): 2ce.  on the subprocess.

     This flag is ignored if *note CREATE_NEW_CONSOLE: 16ca. is
     specified.


File: python.info,  Node: Replacing Older Functions with the subprocess Module,  Next: Notes,  Prev: Windows Popen Helpers,  Up: subprocess --- Subprocess management

5.17.1.9 Replacing Older Functions with the `subprocess' Module
...............................................................

In this section, "a becomes b" means that b can be used as a
replacement for a.

     Note: All "a" functions in this section fail (more or less)
     silently if the executed program cannot be found; the "b"
     replacements raise *note OSError: 22e.  instead.

     In addition, the replacements using *note check_output(): 25e.
     will fail with a *note CalledProcessError: 25f. if the requested
     operation produces a non-zero return code. The output is still
     available as the `output' attribute of the raised exception.

  In the following examples, we assume that the relevant functions have
already been imported from the *note subprocess: 167. module.

* Menu:

* Replacing /bin/sh shell backquote::
* Replacing shell pipeline::
* Replacing os.system(): Replacing os system.
* Replacing the os.spawn family: Replacing the os spawn family.
* Replacing os.popen(), os.popen2(), os.popen3(): Replacing os popen os popen2 os popen3.
* Replacing functions from the popen2 module::


File: python.info,  Node: Replacing /bin/sh shell backquote,  Next: Replacing shell pipeline,  Up: Replacing Older Functions with the subprocess Module

5.17.1.10 Replacing /bin/sh shell backquote
...........................................

    output=`mycmd myarg`
    # becomes
    output = check_output(["mycmd", "myarg"])



File: python.info,  Node: Replacing shell pipeline,  Next: Replacing os system,  Prev: Replacing /bin/sh shell backquote,  Up: Replacing Older Functions with the subprocess Module

5.17.1.11 Replacing shell pipeline
..................................

    output=`dmesg | grep hda`
    # becomes
    p1 = Popen(["dmesg"], stdout=PIPE)
    p2 = Popen(["grep", "hda"], stdin=p1.stdout, stdout=PIPE)
    p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.
    output = p2.communicate()[0]

The p1.stdout.close() call after starting the p2 is important in order
for p1 to receive a SIGPIPE if p2 exits before p1.

  Alternatively, for trusted input, the shell's own pipeline support
may still be used directly:

    output=`dmesg | grep hda`
    # becomes
    output=check_output("dmesg | grep hda", shell=True)



File: python.info,  Node: Replacing os system,  Next: Replacing the os spawn family,  Prev: Replacing shell pipeline,  Up: Replacing Older Functions with the subprocess Module

5.17.1.12 Replacing `os.system()'
.................................

    sts = os.system("mycmd" + " myarg")
    # becomes
    sts = call("mycmd" + " myarg", shell=True)

Notes:

   * Calling the program through the shell is usually not required.

  A more realistic example would look like this:

    try:
        retcode = call("mycmd" + " myarg", shell=True)
        if retcode < 0:
            print >>sys.stderr, "Child was terminated by signal", -retcode
        else:
            print >>sys.stderr, "Child returned", retcode
    except OSError as e:
        print >>sys.stderr, "Execution failed:", e



File: python.info,  Node: Replacing the os spawn family,  Next: Replacing os popen os popen2 os popen3,  Prev: Replacing os system,  Up: Replacing Older Functions with the subprocess Module

5.17.1.13 Replacing the `os.spawn' family
.........................................

P_NOWAIT example:

    pid = os.spawnlp(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg")
    ==>
    pid = Popen(["/bin/mycmd", "myarg"]).pid

P_WAIT example:

    retcode = os.spawnlp(os.P_WAIT, "/bin/mycmd", "mycmd", "myarg")
    ==>
    retcode = call(["/bin/mycmd", "myarg"])

Vector example:

    os.spawnvp(os.P_NOWAIT, path, args)
    ==>
    Popen([path] + args[1:])

Environment example:

    os.spawnlpe(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg", env)
    ==>
    Popen(["/bin/mycmd", "myarg"], env={"PATH": "/usr/bin"})



File: python.info,  Node: Replacing os popen os popen2 os popen3,  Next: Replacing functions from the popen2 module,  Prev: Replacing the os spawn family,  Up: Replacing Older Functions with the subprocess Module

5.17.1.14 Replacing `os.popen()', `os.popen2()', `os.popen3()'
..............................................................

    pipe = os.popen("cmd", 'r', bufsize)
    ==>
    pipe = Popen("cmd", shell=True, bufsize=bufsize, stdout=PIPE).stdout


    pipe = os.popen("cmd", 'w', bufsize)
    ==>
    pipe = Popen("cmd", shell=True, bufsize=bufsize, stdin=PIPE).stdin


    (child_stdin, child_stdout) = os.popen2("cmd", mode, bufsize)
    ==>
    p = Popen("cmd", shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, close_fds=True)
    (child_stdin, child_stdout) = (p.stdin, p.stdout)


    (child_stdin,
     child_stdout,
     child_stderr) = os.popen3("cmd", mode, bufsize)
    ==>
    p = Popen("cmd", shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
    (child_stdin,
     child_stdout,
     child_stderr) = (p.stdin, p.stdout, p.stderr)


    (child_stdin, child_stdout_and_stderr) = os.popen4("cmd", mode,
                                                       bufsize)
    ==>
    p = Popen("cmd", shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)
    (child_stdin, child_stdout_and_stderr) = (p.stdin, p.stdout)

On Unix, os.popen2, os.popen3 and os.popen4 also accept a sequence as
the command to execute, in which case arguments will be passed directly
to the program without shell intervention.  This usage can be replaced
as follows:

    (child_stdin, child_stdout) = os.popen2(["/bin/ls", "-l"], mode,
                                            bufsize)
    ==>
    p = Popen(["/bin/ls", "-l"], bufsize=bufsize, stdin=PIPE, stdout=PIPE)
    (child_stdin, child_stdout) = (p.stdin, p.stdout)

Return code handling translates as follows:

    pipe = os.popen("cmd", 'w')
    ...
    rc = pipe.close()
    if rc is not None and rc >> 8:
        print "There were some errors"
    ==>
    process = Popen("cmd", 'w', shell=True, stdin=PIPE)
    ...
    process.stdin.close()
    if process.wait() != 0:
        print "There were some errors"



File: python.info,  Node: Replacing functions from the popen2 module,  Prev: Replacing os popen os popen2 os popen3,  Up: Replacing Older Functions with the subprocess Module

5.17.1.15 Replacing functions from the `popen2' module
......................................................

    (child_stdout, child_stdin) = popen2.popen2("somestring", bufsize, mode)
    ==>
    p = Popen(["somestring"], shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, close_fds=True)
    (child_stdout, child_stdin) = (p.stdout, p.stdin)

On Unix, popen2 also accepts a sequence as the command to execute, in
which case arguments will be passed directly to the program without
shell intervention.  This usage can be replaced as follows:

    (child_stdout, child_stdin) = popen2.popen2(["mycmd", "myarg"], bufsize,
                                                mode)
    ==>
    p = Popen(["mycmd", "myarg"], bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, close_fds=True)
    (child_stdout, child_stdin) = (p.stdout, p.stdin)

*note popen2.Popen3: 16eb. and *note popen2.Popen4: 16ec. basically
work as *note subprocess.Popen: 16b8, except that:

   * *note Popen: 16b8. raises an exception if the execution fails.

   * the _capturestderr_ argument is replaced with the _stderr_
     argument.

   * `stdin=PIPE' and `stdout=PIPE' must be specified.

   * popen2 closes all file descriptors by default, but you have to
     specify `close_fds=True' with *note Popen: 16b8.


File: python.info,  Node: Notes,  Prev: Replacing Older Functions with the subprocess Module,  Up: subprocess --- Subprocess management

5.17.1.16 Notes
...............

* Menu:

* Converting an argument sequence to a string on Windows::


File: python.info,  Node: Converting an argument sequence to a string on Windows,  Up: Notes

5.17.1.17 Converting an argument sequence to a string on Windows
................................................................

On Windows, an _args_ sequence is converted to a string that can be
parsed using the following rules (which correspond to the rules used by
the MS C runtime):

  1. Arguments are delimited by white space, which is either a space or
     a tab.

  2. A string surrounded by double quotation marks is interpreted as a
     single argument, regardless of white space contained within.  A
     quoted string can be embedded in an argument.

  3. A double quotation mark preceded by a backslash is interpreted as
     a literal double quotation mark.

  4. Backslashes are interpreted literally, unless they immediately
     precede a double quotation mark.

  5. If backslashes immediately precede a double quotation mark, every
     pair of backslashes is interpreted as a literal backslash.  If the
     number of backslashes is odd, the last backslash escapes the next
     double quotation mark as described in rule 3.


File: python.info,  Node: socket --- Low-level networking interface,  Next: ssl --- TLS/SSL wrapper for socket objects,  Prev: subprocess --- Subprocess management,  Up: Interprocess Communication and Networking

5.17.2 `socket' -- Low-level networking interface
-------------------------------------------------

This module provides access to the BSD _socket_ interface. It is
available on all modern Unix systems, Windows, Mac OS X, BeOS, OS/2,
and probably additional platforms.

     Note: Some behavior may be platform dependent, since calls are
     made to the operating system socket APIs.

  For an introduction to socket programming (in C), see the following
papers: An Introductory 4.3BSD Interprocess Communication Tutorial, by
Stuart Sechrest and An Advanced 4.3BSD Interprocess Communication
Tutorial, by Samuel J.  Leffler et al, both in the UNIX Programmer's
Manual, Supplementary Documents 1 (sections PS1:7 and PS1:8).  The
platform-specific reference material for the various socket-related
system calls are also a valuable source of information on the details
of socket semantics.  For Unix, refer to the manual pages; for Windows,
see the WinSock (or Winsock 2) specification. For IPv6-ready APIs,
readers may want to refer to RFC 3493(1) titled Basic Socket Interface
Extensions for IPv6.

  The Python interface is a straightforward transliteration of the Unix
system call and library interface for sockets to Python's
object-oriented style: the *note socket(): 157a. function returns a
_socket object_ whose methods implement the various socket system
calls.  Parameter types are somewhat higher-level than in the C
interface: as with `read()' and `write()' operations on Python files,
buffer allocation on receive operations is automatic, and buffer length
is implicit on send operations.

  Socket addresses are represented as follows: A single string is used
for the *note AF_UNIX: 16f1. address family. A pair `(host, port)' is
used for the *note AF_INET: 16f2. address family, where _host_ is a
string representing either a hostname in Internet domain notation like
`'daring.cwi.nl'' or an IPv4 address like `'100.50.200.5'', and _port_
is an integer. For *note AF_INET6: 16f3. address family, a four-tuple
`(host, port, flowinfo, scopeid)' is used, where _flowinfo_ and
_scopeid_ represents `sin6_flowinfo' and `sin6_scope_id' member in
`struct sockaddr_in6' in C. For *note socket: 15c. module methods,
_flowinfo_ and _scopeid_ can be omitted just for backward
compatibility. Note, however, omission of _scopeid_ can cause problems
in manipulating scoped IPv6 addresses. Other address families are
currently not supported. The address format required by a particular
socket object is automatically selected based on the address family
specified when the socket object was created.

  For IPv4 addresses, two special forms are accepted instead of a host
address: the empty string represents `INADDR_ANY', and the string
`'<broadcast>'' represents `INADDR_BROADCAST'. The behavior is not
available for IPv6 for backward compatibility, therefore, you may want
to avoid these if you intend to support IPv6 with your Python programs.

  If you use a hostname in the _host_ portion of IPv4/v6 socket
address, the program may show a nondeterministic behavior, as Python
uses the first address returned from the DNS resolution.  The socket
address will be resolved differently into an actual IPv4/v6 address,
depending on the results from DNS resolution and/or the host
configuration.  For deterministic behavior use a numeric address in
_host_ portion.

  New in version 2.5: AF_NETLINK sockets are represented as  pairs
`pid, groups'.

  New in version 2.6: Linux-only support for TIPC is also available
using the `AF_TIPC' address family. TIPC is an open, non-IP based
networked protocol designed for use in clustered computer environments.
Addresses are represented by a tuple, and the fields depend on the
address type. The general tuple form is `(addr_type, v1, v2, v3 [,
scope])', where:

   - _addr_type_ is one of `TIPC_ADDR_NAMESEQ', `TIPC_ADDR_NAME', or
     `TIPC_ADDR_ID'.

   - _scope_ is one of `TIPC_ZONE_SCOPE', `TIPC_CLUSTER_SCOPE', and
     `TIPC_NODE_SCOPE'.

   - If _addr_type_ is `TIPC_ADDR_NAME', then _v1_ is the server type,
     _v2_ is the port identifier, and _v3_ should be 0.

     If _addr_type_ is `TIPC_ADDR_NAMESEQ', then _v1_ is the server
     type, _v2_ is the lower port number, and _v3_ is the upper port
     number.

     If _addr_type_ is `TIPC_ADDR_ID', then _v1_ is the node, _v2_ is
     the reference, and _v3_ should be set to 0.

  All errors raise exceptions.  The normal exceptions for invalid
argument types and out-of-memory conditions can be raised; errors
related to socket or address semantics raise the error *note
socket.error: 37a.

  Non-blocking mode is supported through *note setblocking(): 16f4.  A
generalization of this based on timeouts is supported through *note
settimeout(): 16f5.

  The module *note socket: 15c. exports the following constants and
functions:

 -- Exception: socket.error
     This exception is raised for socket-related errors. The
     accompanying value is either a string telling what went wrong or a
     pair `(errno, string)' representing an error returned by a system
     call, similar to the value accompanying *note os.error: de3. See
     the module *note errno: c8, which contains names for the error
     codes defined by the underlying operating system.

     Changed in version 2.6: *note socket.error: 37a. is now a child
     class of *note IOError: 1f7.

 -- Exception: socket.herror
     This exception is raised for address-related errors, i.e. for
     functions that use _h_errno_ in the C API, including *note
     gethostbyname_ex(): 16f7. and *note gethostbyaddr(): 16f8.

     The accompanying value is a pair `(h_errno, string)' representing
     an error returned by a library call. _string_ represents the
     description of _h_errno_, as returned by the `hstrerror()' C
     function.

 -- Exception: socket.gaierror
     This exception is raised for address-related errors, for *note
     getaddrinfo(): 16fa. and *note getnameinfo(): 16fb. The
     accompanying value is a pair `(error, string)' representing an
     error returned by a library call. _string_ represents the
     description of _error_, as returned by the `gai_strerror()' C
     function. The _error_ value will match one of the `EAI_*'
     constants defined in this module.

 -- Exception: socket.timeout
     This exception is raised when a timeout occurs on a socket which
     has had timeouts enabled via a prior call to `settimeout()'.  The
     accompanying value is a string whose value is currently always
     "timed out".

     New in version 2.3.

 -- Data: socket.AF_UNIX
 -- Data: socket.AF_INET
 -- Data: socket.AF_INET6
     These constants represent the address (and protocol) families,
     used for the first argument to *note socket(): 157a.  If the *note
     AF_UNIX: 16f1. constant is not defined then this protocol is
     unsupported.

 -- Data: socket.SOCK_STREAM
 -- Data: socket.SOCK_DGRAM
 -- Data: socket.SOCK_RAW
 -- Data: socket.SOCK_RDM
 -- Data: socket.SOCK_SEQPACKET
     These constants represent the socket types, used for the second
     argument to *note socket(): 15c. (Only *note SOCK_STREAM: 1d9. and
     *note SOCK_DGRAM: 1d8. appear to be generally useful.)

 -- Data: SO_*
 -- Data: socket.SOMAXCONN
 -- Data: MSG_*
 -- Data: SOL_*
 -- Data: IPPROTO_*
 -- Data: IPPORT_*
 -- Data: INADDR_*
 -- Data: IP_*
 -- Data: IPV6_*
 -- Data: EAI_*
 -- Data: AI_*
 -- Data: NI_*
 -- Data: TCP_*
     Many constants of these forms, documented in the Unix
     documentation on sockets and/or the IP protocol, are also defined
     in the socket module. They are generally used in arguments to the
     `setsockopt()' and `getsockopt()' methods of socket objects.  In
     most cases, only those symbols that are defined in the Unix header
     files are defined; for a few symbols, default values are provided.

 -- Data: SIO_*
 -- Data: RCVALL_*
     Constants for Windows' WSAIoctl(). The constants are used as
     arguments to the `ioctl()' method of socket objects.

     New in version 2.6.

 -- Data: TIPC_*
     TIPC related constants, matching the ones exported by the C socket
     API. See the TIPC documentation for more information.

     New in version 2.6.

 -- Data: socket.has_ipv6
     This constant contains a boolean value which indicates if IPv6 is
     supported on this platform.

     New in version 2.3.

 -- Function: socket.create_connection (address[, timeout[,
          source_address]])
     Connect to a TCP service listening on the Internet _address_ (a
     2-tuple `(host, port)'), and return the socket object.  This is a
     higher-level function than *note socket.connect(): 1701.: if
     _host_ is a non-numeric hostname, it will try to resolve it for
     both *note AF_INET: 16f2. and *note AF_INET6: 16f3, and then try
     to connect to all possible addresses in turn until a connection
     succeeds.  This makes it easy to write clients that are compatible
     to both IPv4 and IPv6.

     Passing the optional _timeout_ parameter will set the timeout on
     the socket instance before attempting to connect.  If no _timeout_
     is supplied, the global default timeout setting returned by *note
     getdefaulttimeout(): 1702. is used.

     If supplied, _source_address_ must be a 2-tuple `(host, port)' for
     the socket to bind to as its source address before connecting.  If
     host or port are '' or 0 respectively the OS default behavior will
     be used.

     New in version 2.6.

     Changed in version 2.7: _source_address_ was added.

 -- Function: socket.getaddrinfo (host, port[, family[, socktype[,
          proto[, flags]]]])
     Translate the _host_/_port_ argument into a sequence of 5-tuples
     that contain all the necessary arguments for creating a socket
     connected to that service.  _host_ is a domain name, a string
     representation of an IPv4/v6 address or `None'. _port_ is a string
     service name such as `'http'', a numeric port number or `None'.
     By passing `None' as the value of _host_ and _port_, you can pass
     `NULL' to the underlying C API.

     The _family_, _socktype_ and _proto_ arguments can be optionally
     specified in order to narrow the list of addresses returned.  By
     default, their value is `0', meaning that the full range of
     results is selected.  The _flags_ argument can be one or several
     of the `AI_*' constants, and will influence how results are
     computed and returned.  Its default value is `0'.  For example,
     `AI_NUMERICHOST' will disable domain name resolution and will
     raise an error if _host_ is a domain name.

     The function returns a list of 5-tuples with the following
     structure:

     `(family, socktype, proto, canonname, sockaddr)'

     In these tuples, _family_, _socktype_, _proto_ are all integers
     and are meant to be passed to the *note socket(): 157a. function.
     _canonname_ will be a string representing the canonical name of
     the _host_ if `AI_CANONNAME' is part of the _flags_ argument; else
     _canonname_ will be empty.  _sockaddr_ is a tuple describing a
     socket address, whose format depends on the returned _family_ (a
     `(address, port)' 2-tuple for *note AF_INET: 16f2, a `(address,
     port, flow info, scope id)' 4-tuple for *note AF_INET6: 16f3.),
     and is meant to be passed to the *note socket.connect(): 1701.
     method.

     The following example fetches address information for a
     hypothetical TCP connection to `www.python.org' on port 80
     (results may differ on your system if IPv6 isn't enabled):

         >>> socket.getaddrinfo("www.python.org", 80, 0, 0, socket.SOL_TCP)
         [(2, 1, 6, '', ('82.94.164.162', 80)),
          (10, 1, 6, '', ('2001:888:2000:d::a2', 80, 0, 0))]

     New in version 2.2.

 -- Function: socket.getfqdn ([name])
     Return a fully qualified domain name for _name_. If _name_ is
     omitted or empty, it is interpreted as the local host.  To find
     the fully qualified name, the hostname returned by *note
     gethostbyaddr(): 16f8. is checked, followed by aliases for the
     host, if available.  The first name which includes a period is
     selected.  In case no fully qualified domain name is available,
     the hostname as returned by *note gethostname(): 10dd. is returned.

     New in version 2.0.

 -- Function: socket.gethostbyname (hostname)
     Translate a host name to IPv4 address format.  The IPv4 address is
     returned as a string, such as  `'100.50.200.5''.  If the host name
     is an IPv4 address itself it is returned unchanged.  See *note
     gethostbyname_ex(): 16f7. for a more complete interface. *note
     gethostbyname(): 1704. does not support IPv6 name resolution, and
     *note getaddrinfo(): 16fa. should be used instead for IPv4/v6 dual
     stack support.

 -- Function: socket.gethostbyname_ex (hostname)
     Translate a host name to IPv4 address format, extended interface.
     Return a triple `(hostname, aliaslist, ipaddrlist)' where
     _hostname_ is the primary host name responding to the given
     _ip_address_, _aliaslist_ is a (possibly empty) list of
     alternative host names for the same address, and _ipaddrlist_ is a
     list of IPv4 addresses for the same interface on the same host
     (often but not always a single address). *note gethostbyname_ex():
     16f7. does not support IPv6 name resolution, and *note
     getaddrinfo(): 16fa. should be used instead for IPv4/v6 dual stack
     support.

 -- Function: socket.gethostname ()
     Return a string containing the hostname of the machine where  the
     Python interpreter is currently executing.

     If you want to know the current machine's IP address, you may want
     to use `gethostbyname(gethostname())'. This operation assumes that
     there is a valid address-to-host mapping for the host, and the
     assumption does not always hold.

     Note: *note gethostname(): 10dd. doesn't always return the fully
     qualified domain name; use `getfqdn()' (see above).

 -- Function: socket.gethostbyaddr (ip_address)
     Return a triple `(hostname, aliaslist, ipaddrlist)' where
     _hostname_ is the primary host name responding to the given
     _ip_address_, _aliaslist_ is a (possibly empty) list of
     alternative host names for the same address, and _ipaddrlist_ is a
     list of IPv4/v6 addresses for the same interface on the same host
     (most likely containing only a single address). To find the fully
     qualified domain name, use the function *note getfqdn(): 1703.
     *note gethostbyaddr(): 16f8. supports both IPv4 and IPv6.

 -- Function: socket.getnameinfo (sockaddr, flags)
     Translate a socket address _sockaddr_ into a 2-tuple `(host,
     port)'. Depending on the settings of _flags_, the result can
     contain a fully-qualified domain name or numeric address
     representation in _host_.  Similarly, _port_ can contain a string
     port name or a numeric port number.

     New in version 2.2.

 -- Function: socket.getprotobyname (protocolname)
     Translate an Internet protocol name (for example, `'icmp'') to a
     constant suitable for passing as the (optional) third argument to
     the *note socket(): 157a.  function.  This is usually only needed
     for sockets opened in "raw" mode (*note SOCK_RAW: 16fc.); for the
     normal socket modes, the correct protocol is chosen automatically
     if the protocol is omitted or zero.

 -- Function: socket.getservbyname (servicename[, protocolname])
     Translate an Internet service name and protocol name to a port
     number for that service.  The optional protocol name, if given,
     should be `'tcp'' or `'udp'', otherwise any protocol will match.

 -- Function: socket.getservbyport (port[, protocolname])
     Translate an Internet port number and protocol name to a service
     name for that service.  The optional protocol name, if given,
     should be `'tcp'' or `'udp'', otherwise any protocol will match.

 -- Function: socket.socket ([family[, type[, proto]]])
     Create a new socket using the given address family, socket type
     and protocol number.  The address family should be *note AF_INET:
     16f2. (the default), *note AF_INET6: 16f3. or *note AF_UNIX: 16f1.
     The socket type should be *note SOCK_STREAM: 1d9. (the default),
     *note SOCK_DGRAM: 1d8. or perhaps one of the other `SOCK_'
     constants.  The protocol number is usually zero and may be omitted
     in that case.

 -- Function: socket.socketpair ([family[, type[, proto]]])
     Build a pair of connected socket objects using the given address
     family, socket type, and protocol number.  Address family, socket
     type, and protocol number are as for the *note socket(): 157a.
     function above. The default family is *note AF_UNIX: 16f1.  if
     defined on the platform; otherwise, the default is *note AF_INET:
     16f2.  Availability: Unix.

     New in version 2.4.

 -- Function: socket.fromfd (fd, family, type[, proto])
     Duplicate the file descriptor _fd_ (an integer as returned by a
     file object's `fileno()' method) and build a socket object from
     the result.  Address family, socket type and protocol number are
     as for the *note socket(): 157a. function above. The file
     descriptor should refer to a socket, but this is not checked --
     subsequent operations on the object may fail if the file
     descriptor is invalid.  This function is rarely needed, but can be
     used to get or set socket options on a socket passed to a program
     as standard input or output (such as a server started by the Unix
     inet daemon).  The socket is assumed to be in blocking mode.
     Availability: Unix.

 -- Function: socket.ntohl (x)
     Convert 32-bit positive integers from network to host byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 4-byte swap
     operation.

 -- Function: socket.ntohs (x)
     Convert 16-bit positive integers from network to host byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 2-byte swap
     operation.

 -- Function: socket.htonl (x)
     Convert 32-bit positive integers from host to network byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 4-byte swap
     operation.

 -- Function: socket.htons (x)
     Convert 16-bit positive integers from host to network byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 2-byte swap
     operation.

 -- Function: socket.inet_aton (ip_string)
     Convert an IPv4 address from dotted-quad string format (for
     example, '123.45.67.89') to 32-bit packed binary format, as a
     string four characters in length.  This is useful when conversing
     with a program that uses the standard C library and needs objects
     of type `struct in_addr', which is the C type for the 32-bit
     packed binary this function returns.

     *note inet_aton(): 170e. also accepts strings with less than three
     dots; see the Unix manual page `inet(3)' for details.

     If the IPv4 address string passed to this function is invalid,
     *note socket.error: 37a. will be raised. Note that exactly what is
     valid depends on the underlying C implementation of `inet_aton()'.

     *note inet_aton(): 170e. does not support IPv6, and *note
     inet_pton(): 170f. should be used instead for IPv4/v6 dual stack
     support.

 -- Function: socket.inet_ntoa (packed_ip)
     Convert a 32-bit packed IPv4 address (a string four characters in
     length) to its standard dotted-quad string representation (for
     example, '123.45.67.89').  This is useful when conversing with a
     program that uses the standard C library and needs objects of type
     `struct in_addr', which is the C type for the 32-bit packed binary
     data this function takes as an argument.

     If the string passed to this function is not exactly 4 bytes in
     length, *note socket.error: 37a. will be raised. *note
     inet_ntoa(): 1710. does not support IPv6, and *note inet_ntop():
     1711. should be used instead for IPv4/v6 dual stack support.

 -- Function: socket.inet_pton (address_family, ip_string)
     Convert an IP address from its family-specific string format to a
     packed, binary format. *note inet_pton(): 170f. is useful when a
     library or network protocol calls for an object of type `struct
     in_addr' (similar to *note inet_aton(): 170e.) or `struct
     in6_addr'.

     Supported values for _address_family_ are currently *note AF_INET:
     16f2. and *note AF_INET6: 16f3. If the IP address string
     _ip_string_ is invalid, *note socket.error: 37a. will be raised.
     Note that exactly what is valid depends on both the value of
     _address_family_ and the underlying implementation of
     `inet_pton()'.

     Availability: Unix (maybe not all platforms).

     New in version 2.3.

 -- Function: socket.inet_ntop (address_family, packed_ip)
     Convert a packed IP address (a string of some number of
     characters) to its standard, family-specific string representation
     (for example, `'7.10.0.5'' or `'5aef:2b::8'') *note inet_ntop():
     1711. is useful when a library or network protocol returns an
     object of type `struct in_addr' (similar to *note inet_ntoa():
     1710.)  or `struct in6_addr'.

     Supported values for _address_family_ are currently *note AF_INET:
     16f2. and *note AF_INET6: 16f3. If the string _packed_ip_ is not
     the correct length for the specified address family, *note
     ValueError: 233. will be raised.  A *note socket.error: 37a. is
     raised for errors from the call to *note inet_ntop(): 1711.

     Availability: Unix (maybe not all platforms).

     New in version 2.3.

 -- Function: socket.getdefaulttimeout ()
     Return the default timeout in seconds (float) for new socket
     objects. A value of `None' indicates that new socket objects have
     no timeout. When the socket module is first imported, the default
     is `None'.

     New in version 2.3.

 -- Function: socket.setdefaulttimeout (timeout)
     Set the default timeout in seconds (float) for new socket objects.
     A value of `None' indicates that new socket objects have no
     timeout. When the socket module is first imported, the default is
     `None'.

     New in version 2.3.

 -- Data: socket.SocketType
     This is a Python type object that represents the socket object
     type. It is the same as `type(socket(...))'.

See also
........

Module *note SocketServer: 15d.
     Classes that simplify writing network servers.

Module *note ssl: 160.
     A TLS/SSL wrapper for socket objects.

* Menu:

* Socket Objects::
* Example: Example<8>.

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc3493.html


File: python.info,  Node: Socket Objects,  Next: Example<8>,  Up: socket --- Low-level networking interface

5.17.2.1 Socket Objects
.......................

Socket objects have the following methods.  Except for `makefile()'
these correspond to Unix system calls applicable to sockets.

 -- Method: socket.accept ()
     Accept a connection. The socket must be bound to an address and
     listening for connections. The return value is a pair `(conn,
     address)' where _conn_ is a _new_ socket object usable to send and
     receive data on the connection, and _address_ is the address bound
     to the socket on the other end of the connection.

 -- Method: socket.bind (address)
     Bind the socket to _address_.  The socket must not already be
     bound. (The format of _address_ depends on the address family --
     see above.)

          Note: This method has historically accepted a pair of
          parameters for *note AF_INET: 16f2.  addresses instead of
          only a tuple.  This was never intentional and is no longer
          available in Python 2.0 and later.

 -- Method: socket.close ()
     Close the socket.  All future operations on the socket object will
     fail. The remote end will receive no more data (after queued data
     is flushed). Sockets are automatically closed when they are
     garbage-collected.

          Note: *note close(): 1718. releases the resource associated
          with a connection but does not necessarily close the
          connection immediately.  If you want to close the connection
          in a timely fashion, call *note shutdown(): 1719.  before
          *note close(): 1718.

 -- Method: socket.connect (address)
     Connect to a remote socket at _address_. (The format of _address_
     depends on the address family -- see above.)

          Note: This method has historically accepted a pair of
          parameters for *note AF_INET: 16f2.  addresses instead of
          only a tuple.  This was never intentional and is no longer
          available in Python 2.0 and later.

 -- Method: socket.connect_ex (address)
     Like `connect(address)', but return an error indicator instead of
     raising an exception for errors returned by the C-level
     `connect()' call (other problems, such as "host not found," can
     still raise exceptions).  The error indicator is `0' if the
     operation succeeded, otherwise the value of the `errno' variable.
     This is useful to support, for example, asynchronous connects.

          Note: This method has historically accepted a pair of
          parameters for *note AF_INET: 16f2.  addresses instead of
          only a tuple. This was never intentional and is no longer
          available in Python 2.0 and later.

 -- Method: socket.fileno ()
     Return the socket's file descriptor (a small integer).  This is
     useful with *note select.select(): 1579.

     Under Windows the small integer returned by this method cannot be
     used where a file descriptor can be used (such as *note
     os.fdopen(): 6ee.).  Unix does not have this limitation.

 -- Method: socket.getpeername ()
     Return the remote address to which the socket is connected.  This
     is useful to find out the port number of a remote IPv4/v6 socket,
     for instance. (The format of the address returned depends on the
     address family -- see above.)  On some systems this function is
     not supported.

 -- Method: socket.getsockname ()
     Return the socket's own address.  This is useful to find out the
     port number of an IPv4/v6 socket, for instance. (The format of the
     address returned depends on the address family -- see above.)

 -- Method: socket.getsockopt (level, optname[, buflen])
     Return the value of the given socket option (see the Unix man page
     `getsockopt(2)').  The needed symbolic constants (`SO_*' etc.)
     are defined in this module.  If _buflen_ is absent, an integer
     option is assumed and its integer value is returned by the
     function.  If _buflen_ is present, it specifies the maximum length
     of the buffer used to receive the option in, and this buffer is
     returned as a string.  It is up to the caller to decode the
     contents of the buffer (see the optional built-in module *note
     struct: 166. for a way to decode C structures encoded as strings).

 -- Method: socket.ioctl (control, option)
          Platform : Windows

     The *note ioctl(): 171f. method is a limited interface to the
     WSAIoctl system interface.  Please refer to the Win32
     documentation(1) for more information.

     On other platforms, the generic *note fcntl.fcntl(): 1720. and
     *note fcntl.ioctl(): 41a.  functions may be used; they accept a
     socket object as their first argument.

     New in version 2.6.

 -- Method: socket.listen (backlog)
     Listen for connections made to the socket.  The _backlog_ argument
     specifies the maximum number of queued connections and should be
     at least 0; the maximum value is system-dependent (usually 5), the
     minimum value is forced to 0.

 -- Method: socket.makefile ([mode[, bufsize]])
     Return a _file object_ associated with the socket.  (File objects
     are described in *note File Objects: 630.) The file object
     references a `dup()'ped version of the socket file descriptor, so
     the file object and socket object may be closed or
     garbage-collected independently.  The socket must be in blocking
     mode (it can not have a timeout). The optional _mode_ and
     _bufsize_ arguments are interpreted the same way as by the built-in
     *note file(): 1f6. function.

          Note: On Windows, the file-like object created by *note
          makefile(): 1722. cannot be used where a file object with a
          file descriptor is expected, such as the stream arguments of
          *note subprocess.Popen(): 16b8.

 -- Method: socket.recv (bufsize[, flags])
     Receive data from the socket.  The return value is a string
     representing the data received.  The maximum amount of data to be
     received at once is specified by _bufsize_.  See the Unix manual
     page `recv(2)' for the meaning of the optional argument _flags_;
     it defaults to zero.

          Note: For best match with hardware and network realities, the
          value of  _bufsize_ should be a relatively small power of 2,
          for example, 4096.

 -- Method: socket.recvfrom (bufsize[, flags])
     Receive data from the socket.  The return value is a pair
     `(string, address)' where _string_ is a string representing the
     data received and _address_ is the address of the socket sending
     the data.  See the Unix manual page `recv(2)' for the meaning of
     the optional argument _flags_; it defaults to zero. (The format of
     _address_ depends on the address family -- see above.)

 -- Method: socket.recvfrom_into (buffer[, nbytes[, flags]])
     Receive data from the socket, writing it into _buffer_ instead of
     creating a new string.  The return value is a pair `(nbytes,
     address)' where _nbytes_ is the number of bytes received and
     _address_ is the address of the socket sending the data.  See the
     Unix manual page `recv(2)' for the meaning of the optional
     argument _flags_; it defaults to zero.  (The format of _address_
     depends on the address family -- see above.)

     New in version 2.5.

 -- Method: socket.recv_into (buffer[, nbytes[, flags]])
     Receive up to _nbytes_ bytes from the socket, storing the data
     into a buffer rather than creating a new string.  If _nbytes_ is
     not specified (or 0), receive up to the size available in the
     given buffer.  Returns the number of bytes received.  See the Unix
     manual page `recv(2)' for the meaning of the optional argument
     _flags_; it defaults to zero.

     New in version 2.5.

 -- Method: socket.send (string[, flags])
     Send data to the socket.  The socket must be connected to a remote
     socket.  The optional _flags_ argument has the same meaning as for
     *note recv(): 1723. above.  Returns the number of bytes sent.
     Applications are responsible for checking that all data has been
     sent; if only some of the data was transmitted, the application
     needs to attempt delivery of the remaining data. For further
     information on this concept, consult the *note Socket Programming
     HOWTO: 1726.

 -- Method: socket.sendall (string[, flags])
     Send data to the socket.  The socket must be connected to a remote
     socket.  The optional _flags_ argument has the same meaning as for
     *note recv(): 1723. above.  Unlike *note send(): 1725, this method
     continues to send data from _string_ until either all data has
     been sent or an error occurs.  `None' is returned on success.  On
     error, an exception is raised, and there is no way to determine how
     much data, if any, was successfully sent.

 -- Method: socket.sendto (string, address)
 -- Method: socket.sendto (string, flags, address)
     Send data to the socket.  The socket should not be connected to a
     remote socket, since the destination socket is specified by
     _address_.  The optional _flags_ argument has the same meaning as
     for *note recv(): 1723. above.  Return the number of bytes sent.
     (The format of _address_ depends on the address family -- see
     above.)

 -- Method: socket.setblocking (flag)
     Set blocking or non-blocking mode of the socket: if _flag_ is 0,
     the socket is set to non-blocking, else to blocking mode.
     Initially all sockets are in blocking mode.  In non-blocking mode,
     if a *note recv(): 1723. call doesn't find any data, or if a *note
     send(): 1725. call can't immediately dispose of the data, a *note
     error: 37a. exception is raised; in blocking mode, the calls block
     until they can proceed. `s.setblocking(0)' is equivalent to
     `s.settimeout(0.0)'; `s.setblocking(1)' is equivalent to
     `s.settimeout(None)'.

 -- Method: socket.settimeout (value)
     Set a timeout on blocking socket operations.  The _value_ argument
     can be a nonnegative float expressing seconds, or `None'. If a
     float is given, subsequent socket operations will raise a *note
     timeout: 45c. exception if the timeout period _value_ has elapsed
     before the operation has completed.  Setting a timeout of `None'
     disables timeouts on socket operations.  `s.settimeout(0.0)' is
     equivalent to `s.setblocking(0)'; `s.settimeout(None)' is
     equivalent to `s.setblocking(1)'.

     New in version 2.3.

 -- Method: socket.gettimeout ()
     Return the timeout in seconds (float) associated with socket
     operations, or `None' if no timeout is set.  This reflects the
     last call to *note setblocking(): 16f4. or *note settimeout():
     16f5.

     New in version 2.3.

  Some notes on socket blocking and timeouts: A socket object can be in
one of three modes: blocking, non-blocking, or timeout.  Sockets are
always created in blocking mode.  In blocking mode, operations block
until complete or the system returns an error (such as connection timed
out).  In non-blocking mode, operations fail (with an error that is
unfortunately system-dependent) if they cannot be completed
immediately.  In timeout mode, operations fail if they cannot be
completed within the timeout specified for the socket or if the system
returns an error.  The *note setblocking(): 16f4.  method is simply a
shorthand for certain *note settimeout(): 16f5. calls.

  Timeout mode internally sets the socket in non-blocking mode.  The
blocking and timeout modes are shared between file descriptors and
socket objects that refer to the same network endpoint.  A consequence
of this is that file objects returned by the *note makefile(): 1722.
method must only be used when the socket is in blocking mode; in
timeout or non-blocking mode file operations that cannot be completed
immediately will fail.

  Note that the *note connect(): 1701. operation is subject to the
timeout setting, and in general it is recommended to call *note
settimeout(): 16f5.  before calling *note connect(): 1701. or pass a
timeout parameter to *note create_connection(): 24f.  The system
network stack may return a connection timeout error of its own
regardless of any Python socket timeout setting.

 -- Method: socket.setsockopt (level, optname, value)
     Set the value of the given socket option (see the Unix manual page
     `setsockopt(2)').  The needed symbolic constants are defined in the
     *note socket: 15c. module (`SO_*' etc.).  The value can be an
     integer or a string representing a buffer.  In the latter case it
     is up to the caller to ensure that the string contains the proper
     bits (see the optional built-in module *note struct: 166. for a
     way to encode C structures as strings).

 -- Method: socket.shutdown (how)
     Shut down one or both halves of the connection.  If _how_ is
     `SHUT_RD', further receives are disallowed.  If _how_ is
     `SHUT_WR', further sends are disallowed.  If _how_ is `SHUT_RDWR',
     further sends and receives are disallowed.  Depending on the
     platform, shutting down one half of the connection can also close
     the opposite half (e.g. on Mac OS X, `shutdown(SHUT_WR)' does not
     allow further reads on the other end of the connection).

  Note that there are no methods `read()' or `write()'; use *note
recv(): 1723. and *note send(): 1725. without _flags_ argument instead.

  Socket objects also have these (read-only) attributes that correspond
to the values given to the *note socket: 15c. constructor.

 -- Attribute: socket.family
     The socket family.

     New in version 2.5.

 -- Attribute: socket.type
     The socket type.

     New in version 2.5.

 -- Attribute: socket.proto
     The socket protocol.

     New in version 2.5.

  ---------- Footnotes ----------

  (1) http://msdn.microsoft.com/en-us/library/ms741621%28VS.85%29.aspx


File: python.info,  Node: Example<8>,  Prev: Socket Objects,  Up: socket --- Low-level networking interface

5.17.2.2 Example
................

Here are four minimal example programs using the TCP/IP protocol: a
server that echoes all data that it receives back (servicing only one
client), and a client using it.  Note that a server must perform the
sequence *note socket(): 157a, *note bind(): 1717, *note listen():
1721, *note accept(): 1716. (possibly repeating the *note accept():
1716. to service more than one client), while a client only needs the
sequence *note socket(): 157a, *note connect(): 1701.  Also note that
the server does not *note sendall(): 1727./*note recv(): 1723. on the
socket it is listening on but on the new socket returned by *note
accept(): 1716.

  The first two examples support IPv4 only.

    # Echo server program
    import socket

    HOST = ''                 # Symbolic name meaning all available interfaces
    PORT = 50007              # Arbitrary non-privileged port
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.bind((HOST, PORT))
    s.listen(1)
    conn, addr = s.accept()
    print 'Connected by', addr
    while 1:
        data = conn.recv(1024)
        if not data: break
        conn.sendall(data)
    conn.close()


    # Echo client program
    import socket

    HOST = 'daring.cwi.nl'    # The remote host
    PORT = 50007              # The same port as used by the server
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.connect((HOST, PORT))
    s.sendall('Hello, world')
    data = s.recv(1024)
    s.close()
    print 'Received', repr(data)

The next two examples are identical to the above two, but support both
IPv4 and IPv6. The server side will listen to the first address family
available (it should listen to both instead). On most of IPv6-ready
systems, IPv6 will take precedence and the server may not accept IPv4
traffic. The client side will try to connect to the all addresses
returned as a result of the name resolution, and sends traffic to the
first one connected successfully.

    # Echo server program
    import socket
    import sys

    HOST = None               # Symbolic name meaning all available interfaces
    PORT = 50007              # Arbitrary non-privileged port
    s = None
    for res in socket.getaddrinfo(HOST, PORT, socket.AF_UNSPEC,
                                  socket.SOCK_STREAM, 0, socket.AI_PASSIVE):
        af, socktype, proto, canonname, sa = res
        try:
            s = socket.socket(af, socktype, proto)
        except socket.error as msg:
            s = None
            continue
        try:
            s.bind(sa)
            s.listen(1)
        except socket.error as msg:
            s.close()
            s = None
            continue
        break
    if s is None:
        print 'could not open socket'
        sys.exit(1)
    conn, addr = s.accept()
    print 'Connected by', addr
    while 1:
        data = conn.recv(1024)
        if not data: break
        conn.send(data)
    conn.close()


    # Echo client program
    import socket
    import sys

    HOST = 'daring.cwi.nl'    # The remote host
    PORT = 50007              # The same port as used by the server
    s = None
    for res in socket.getaddrinfo(HOST, PORT, socket.AF_UNSPEC, socket.SOCK_STREAM):
        af, socktype, proto, canonname, sa = res
        try:
            s = socket.socket(af, socktype, proto)
        except socket.error as msg:
            s = None
            continue
        try:
            s.connect(sa)
        except socket.error as msg:
            s.close()
            s = None
            continue
        break
    if s is None:
        print 'could not open socket'
        sys.exit(1)
    s.sendall('Hello, world')
    data = s.recv(1024)
    s.close()
    print 'Received', repr(data)

The last example shows how to write a very simple network sniffer with
raw sockets on Windows. The example requires administrator privileges
to modify the interface:

    import socket

    # the public network interface
    HOST = socket.gethostbyname(socket.gethostname())

    # create a raw socket and bind it to the public interface
    s = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_IP)
    s.bind((HOST, 0))

    # Include IP headers
    s.setsockopt(socket.IPPROTO_IP, socket.IP_HDRINCL, 1)

    # receive all packages
    s.ioctl(socket.SIO_RCVALL, socket.RCVALL_ON)

    # receive a package
    print s.recvfrom(65565)

    # disabled promiscuous mode
    s.ioctl(socket.SIO_RCVALL, socket.RCVALL_OFF)

Running an example several times with too small delay between
executions, could lead to this error:

    socket.error: [Errno 98] Address already in use

This is because the previous execution has left the socket in a
`TIME_WAIT' state, and can't be immediately reused.

  There is a *note socket: 15c. flag to set, in order to prevent this,
`socket.SO_REUSEADDR':

    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.bind((HOST, PORT))

the `SO_REUSEADDR' flag tells the kernel to reuse a local socket in
`TIME_WAIT' state, without waiting for its natural timeout to expire.


File: python.info,  Node: ssl --- TLS/SSL wrapper for socket objects,  Next: signal --- Set handlers for asynchronous events,  Prev: socket --- Low-level networking interface,  Up: Interprocess Communication and Networking

5.17.3 `ssl' -- TLS/SSL wrapper for socket objects
--------------------------------------------------

New in version 2.6.

  *Source code:* Lib/ssl.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  This module provides access to Transport Layer Security (often known
as "Secure Sockets Layer") encryption and peer authentication
facilities for network sockets, both client-side and server-side.  This
module uses the OpenSSL library. It is available on all modern Unix
systems, Windows, Mac OS X, and probably additional platforms, as long
as OpenSSL is installed on that platform.

     Note: Some behavior may be platform dependent, since calls are
     made to the operating system socket APIs.  The installed version
     of OpenSSL may also cause variations in behavior.

  This section documents the objects and functions in the `ssl' module;
for more general information about TLS, SSL, and certificates, the
reader is referred to the documents in the "See Also" section at the
bottom.

  This module provides a class, `ssl.SSLSocket', which is derived from
the *note socket.socket: 157a. type, and provides a socket-like wrapper
that also encrypts and decrypts the data going over the socket with
SSL.  It supports additional `read()' and `write()' methods, along with
a method, `getpeercert()', to retrieve the certificate of the other
side of the connection, and a method, `cipher()', to retrieve the
cipher being used for the secure connection.

* Menu:

* Functions, Constants, and Exceptions: Functions Constants and Exceptions.
* SSLSocket Objects::
* Certificates::
* Examples: Examples<8>.

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/ssl.py


File: python.info,  Node: Functions Constants and Exceptions,  Next: SSLSocket Objects,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.1 Functions, Constants, and Exceptions
.............................................

 -- Exception: ssl.SSLError
     Raised to signal an error from the underlying SSL implementation.
     This signifies some problem in the higher-level encryption and
     authentication layer that's superimposed on the underlying network
     connection.  This error is a subtype of *note socket.error: 37a,
     which in turn is a subtype of *note IOError: 1f7.

 -- Function: ssl.wrap_socket (sock, keyfile=None, certfile=None,
          server_side=False, cert_reqs=CERT_NONE, ssl_version={see
          docs}, ca_certs=None, do_handshake_on_connect=True,
          suppress_ragged_eofs=True, ciphers=None)
     Takes an instance `sock' of *note socket.socket: 157a, and returns
     an instance of `ssl.SSLSocket', a subtype of *note socket.socket:
     157a, which wraps the underlying socket in an SSL context.  For
     client-side sockets, the context construction is lazy; if the
     underlying socket isn't connected yet, the context construction
     will be performed after `connect()' is called on the socket.  For
     server-side sockets, if the socket has no remote peer, it is
     assumed to be a listening socket, and the server-side SSL wrapping
     is automatically performed on client connections accepted via the
     `accept()' method.  *note wrap_socket(): 256. may raise *note
     SSLError: 1733.

     The `keyfile' and `certfile' parameters specify optional files
     which contain a certificate to be used to identify the local side
     of the connection.  See the discussion of *note Certificates:
     1734. for more information on how the certificate is stored in the
     `certfile'.

     Often the private key is stored in the same file as the
     certificate; in this case, only the `certfile' parameter need be
     passed.  If the private key is stored in a separate file, both
     parameters must be used.  If the private key is stored in the
     `certfile', it should come before the first certificate in the
     certificate chain:

         -----BEGIN RSA PRIVATE KEY-----
         ... (private key in base64 encoding) ...
         -----END RSA PRIVATE KEY-----
         -----BEGIN CERTIFICATE-----
         ... (certificate in base64 PEM encoding) ...
         -----END CERTIFICATE-----

     The parameter `server_side' is a boolean which identifies whether
     server-side or client-side behavior is desired from this socket.

     The parameter `cert_reqs' specifies whether a certificate is
     required from the other side of the connection, and whether it
     will be validated if provided.  It must be one of the three values
     *note CERT_NONE: 1735.  (certificates ignored), *note
     CERT_OPTIONAL: 1736. (not required, but validated if provided), or
     *note CERT_REQUIRED: 1737. (required and validated).  If the value
     of this parameter is not *note CERT_NONE: 1735, then the `ca_certs'
     parameter must point to a file of CA certificates.

     The `ca_certs' file contains a set of concatenated "certification
     authority" certificates, which are used to validate certificates
     passed from the other end of the connection.  See the discussion of
     *note Certificates: 1734. for more information about how to
     arrange the certificates in this file.

     The parameter `ssl_version' specifies which version of the SSL
     protocol to use.  Typically, the server chooses a particular
     protocol version, and the client must adapt to the server's
     choice.  Most of the versions are not interoperable with the other
     versions.  If not specified, the default is *note PROTOCOL_SSLv23:
     1738.; it provides the most compatibility with other versions.

     Here's a table showing which versions in a client (down the side)
     can connect to which versions in a server (along the top):

           _client_ / *server*          *SSLv2*       *SSLv3*       *SSLv23*       *TLSv1*
          _SSLv2_                      yes           no            yes            no
          _SSLv3_                      no            yes           yes            no
          _SSLv23_                     yes           no            yes            no
          _TLSv1_                      no            no            yes            yes


          Note: Which connections succeed will vary depending on the
          version of OpenSSL.  For instance, in some older versions of
          OpenSSL (such as 0.9.7l on OS X 10.4), an SSLv2 client could
          not connect to an SSLv23 server.  Another example: beginning
          with OpenSSL 1.0.0, an SSLv23 client will not actually
          attempt SSLv2 connections unless you explicitly enable SSLv2
          ciphers; for example, you might specify `"ALL"' or `"SSLv2"'
          as the _ciphers_ parameter to enable them.

     The _ciphers_ parameter sets the available ciphers for this SSL
     object.  It should be a string in the OpenSSL cipher list
     format(1).

     The parameter `do_handshake_on_connect' specifies whether to do
     the SSL handshake automatically after doing a `socket.connect()',
     or whether the application program will call it explicitly, by
     invoking the *note SSLSocket.do_handshake(): 1739. method.  Calling
     *note SSLSocket.do_handshake(): 1739. explicitly gives the program
     control over the blocking behavior of the socket I/O involved in
     the handshake.

     The parameter `suppress_ragged_eofs' specifies how the
     `SSLSocket.read()' method should signal unexpected EOF from the
     other end of the connection.  If specified as *note True: 3a9.
     (the default), it returns a normal EOF in response to unexpected
     EOF errors raised from the underlying socket; if *note False: 3aa,
     it will raise the exceptions back to the caller.

     Changed in version 2.7: New optional argument _ciphers_.

 -- Function: ssl.RAND_status ()
     Returns True if the SSL pseudo-random number generator has been
     seeded with 'enough' randomness, and False otherwise.  You can use
     *note ssl.RAND_egd(): 173b.  and *note ssl.RAND_add(): 173c. to
     increase the randomness of the pseudo-random number generator.

 -- Function: ssl.RAND_egd (path)
     If you are running an entropy-gathering daemon (EGD) somewhere,
     and `path' is the pathname of a socket connection open to it, this
     will read 256 bytes of randomness from the socket, and add it to
     the SSL pseudo-random number generator to increase the security of
     generated secret keys.  This is typically only necessary on
     systems without better sources of randomness.

     See <http://egd.sourceforge.net/> or
     <http://prngd.sourceforge.net/> for sources of entropy-gathering
     daemons.

 -- Function: ssl.RAND_add (bytes, entropy)
     Mixes the given `bytes' into the SSL pseudo-random number
     generator.  The parameter `entropy' (a float) is a lower bound on
     the entropy contained in string (so you can always use `0.0').  See RFC
     1750(2) for more information on sources of entropy.

 -- Function: ssl.cert_time_to_seconds (timestring)
     Returns a floating-point value containing a normal
     seconds-after-the-epoch time value, given the time-string
     representing the "notBefore" or "notAfter" date from a certificate.

     Here's an example:

         >>> import ssl
         >>> ssl.cert_time_to_seconds("May  9 00:00:00 2007 GMT")
         1178694000.0
         >>> import time
         >>> time.ctime(ssl.cert_time_to_seconds("May  9 00:00:00 2007 GMT"))
         'Wed May  9 00:00:00 2007'
         >>>



 -- Function: ssl.get_server_certificate (addr,
          ssl_version=PROTOCOL_SSLv3, ca_certs=None)
     Given the address `addr' of an SSL-protected server, as a
     (_hostname_, _port-number_) pair, fetches the server's
     certificate, and returns it as a PEM-encoded string.  If
     `ssl_version' is specified, uses that version of the SSL protocol
     to attempt to connect to the server.  If `ca_certs' is specified,
     it should be a file containing a list of root certificates, the
     same format as used for the same parameter in *note wrap_socket():
     256.  The call will attempt to validate the server certificate
     against that set of root certificates, and will fail if the
     validation attempt fails.

 -- Function: ssl.DER_cert_to_PEM_cert (DER_cert_bytes)
     Given a certificate as a DER-encoded blob of bytes, returns a
     PEM-encoded string version of the same certificate.

 -- Function: ssl.PEM_cert_to_DER_cert (PEM_cert_string)
     Given a certificate as an ASCII PEM string, returns a DER-encoded
     sequence of bytes for that same certificate.

 -- Data: ssl.CERT_NONE
     Value to pass to the `cert_reqs' parameter to `sslobject()' when no
     certificates will be required or validated from the other side of
     the socket connection.

 -- Data: ssl.CERT_OPTIONAL
     Value to pass to the `cert_reqs' parameter to `sslobject()' when no
     certificates will be required from the other side of the socket
     connection, but if they are provided, will be validated.  Note
     that use of this setting requires a valid certificate validation
     file also be passed as a value of the `ca_certs' parameter.

 -- Data: ssl.CERT_REQUIRED
     Value to pass to the `cert_reqs' parameter to `sslobject()' when
     certificates will be required from the other side of the socket
     connection.  Note that use of this setting requires a valid
     certificate validation file also be passed as a value of the
     `ca_certs' parameter.

 -- Data: ssl.PROTOCOL_SSLv2
     Selects SSL version 2 as the channel encryption protocol.

     This protocol is not available if OpenSSL is compiled with
     OPENSSL_NO_SSL2 flag.

          Warning: SSL version 2 is insecure.  Its use is highly
          discouraged.

 -- Data: ssl.PROTOCOL_SSLv23
     Selects SSL version 2 or 3 as the channel encryption protocol.
     This is a setting to use with servers for maximum compatibility
     with the other end of an SSL connection, but it may cause the
     specific ciphers chosen for the encryption to be of fairly low
     quality.

 -- Data: ssl.PROTOCOL_SSLv3
     Selects SSL version 3 as the channel encryption protocol.  For
     clients, this is the maximally compatible SSL variant.

 -- Data: ssl.PROTOCOL_TLSv1
     Selects TLS version 1 as the channel encryption protocol.  This is
     the most modern version, and probably the best choice for maximum
     protection, if both sides can speak it.

 -- Data: ssl.OPENSSL_VERSION
     The version string of the OpenSSL library loaded by the
     interpreter:

         >>> ssl.OPENSSL_VERSION
         'OpenSSL 0.9.8k 25 Mar 2009'

     New in version 2.7.

 -- Data: ssl.OPENSSL_VERSION_INFO
     A tuple of five integers representing version information about the
     OpenSSL library:

         >>> ssl.OPENSSL_VERSION_INFO
         (0, 9, 8, 11, 15)

     New in version 2.7.

 -- Data: ssl.OPENSSL_VERSION_NUMBER
     The raw version number of the OpenSSL library, as a single integer:

         >>> ssl.OPENSSL_VERSION_NUMBER
         9470143L
         >>> hex(ssl.OPENSSL_VERSION_NUMBER)
         '0x9080bfL'

     New in version 2.7.

  ---------- Footnotes ----------

  (1) http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT

  (2) http://tools.ietf.org/html/rfc1750.html


File: python.info,  Node: SSLSocket Objects,  Next: Certificates,  Prev: Functions Constants and Exceptions,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.2 SSLSocket Objects
..........................

SSL sockets provide the following methods of *note Socket Objects:
1714.:

   - *note accept(): 1716.

   - *note bind(): 1717.

   - *note close(): 1718.

   - *note connect(): 1701.

   - *note fileno(): 171b.

   - *note getpeername(): 171c, *note getsockname(): 171d.

   - *note getsockopt(): 171e, *note setsockopt(): 172a.

   - *note gettimeout(): 1729, *note settimeout(): 16f5, *note
     setblocking(): 16f4.

   - *note listen(): 1721.

   - *note makefile(): 1722.

   - *note recv(): 1723, *note recv_into(): 250.  (but passing a
     non-zero `flags' argument is not allowed)

   - *note send(): 1725, *note sendall(): 1727. (with the same
     limitation)

   - *note shutdown(): 1719.

  However, since the SSL (and TLS) protocol has its own framing atop of
TCP, the SSL sockets abstraction can, in certain respects, diverge from
the specification of normal, OS-level sockets.

  SSL sockets also have the following additional methods and attributes:

 -- Method: SSLSocket.getpeercert (binary_form=False)
     If there is no certificate for the peer on the other end of the
     connection, returns `None'.

     If the `binary_form' parameter is *note False: 3aa, and a
     certificate was received from the peer, this method returns a
     *note dict: 2fe. instance.  If the certificate was not validated,
     the dict is empty.  If the certificate was validated, it returns a
     dict with the keys `subject' (the principal for which the
     certificate was issued), and `notAfter' (the time after which the
     certificate should not be trusted).  The certificate was already
     validated, so the `notBefore' and `issuer' fields are not
     returned.  If a certificate contains an instance of the _Subject
     Alternative Name_ extension (see RFC 3280(1)), there will also be
     a `subjectAltName' key in the dictionary.

     The "subject" field is a tuple containing the sequence of relative
     distinguished names (RDNs) given in the certificate's data
     structure for the principal, and each RDN is a sequence of
     name-value pairs:

         {'notAfter': 'Feb 16 16:54:50 2013 GMT',
          'subject': ((('countryName', u'US'),),
                      (('stateOrProvinceName', u'Delaware'),),
                      (('localityName', u'Wilmington'),),
                      (('organizationName', u'Python Software Foundation'),),
                      (('organizationalUnitName', u'SSL'),),
                      (('commonName', u'somemachine.python.org'),))}

     If the `binary_form' parameter is *note True: 3a9, and a
     certificate was provided, this method returns the DER-encoded form
     of the entire certificate as a sequence of bytes, or *note None:
     393. if the peer did not provide a certificate.  Whether the peer
     provides a certificate depends on the SSL socket's role:

        * for a client SSL socket, the server will always provide a
          certificate, regardless of whether validation was required;

        * for a server SSL socket, the client will only provide a
          certificate when requested by the server; therefore *note
          getpeercert(): 1745. will return *note None: 393. if you used
          *note CERT_NONE: 1735. (rather than *note CERT_OPTIONAL:
          1736. or *note CERT_REQUIRED: 1737.).

 -- Method: SSLSocket.cipher ()
     Returns a three-value tuple containing the name of the cipher
     being used, the version of the SSL protocol that defines its use,
     and the number of secret bits being used.  If no connection has
     been established, returns `None'.

 -- Method: SSLSocket.do_handshake ()
     Perform a TLS/SSL handshake.  If this is used with a non-blocking
     socket, it may raise *note SSLError: 1733. with an `arg[0]' of
     `SSL_ERROR_WANT_READ' or `SSL_ERROR_WANT_WRITE', in which case it
     must be called again until it completes successfully.  For
     example, to simulate the behavior of a blocking socket, one might
     write:

         while True:
             try:
                 s.do_handshake()
                 break
             except ssl.SSLError as err:
                 if err.args[0] == ssl.SSL_ERROR_WANT_READ:
                     select.select([s], [], [])
                 elif err.args[0] == ssl.SSL_ERROR_WANT_WRITE:
                     select.select([], [s], [])
                 else:
                     raise



 -- Method: SSLSocket.unwrap ()
     Performs the SSL shutdown handshake, which removes the TLS layer
     from the underlying socket, and returns the underlying socket
     object.  This can be used to go from encrypted operation over a
     connection to unencrypted.  The socket instance returned should
     always be used for further communication with the other side of
     the connection, rather than the original socket instance (which
     may not function properly after the unwrap).

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc3280.html


File: python.info,  Node: Certificates,  Next: Examples<8>,  Prev: SSLSocket Objects,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.3 Certificates
.....................

Certificates in general are part of a public-key / private-key system.
In this system, each _principal_, (which may be a machine, or a person,
or an organization) is assigned a unique two-part encryption key.  One
part of the key is public, and is called the _public key_; the other
part is kept secret, and is called the _private key_.  The two parts
are related, in that if you encrypt a message with one of the parts,
you can decrypt it with the other part, and *only* with the other part.

  A certificate contains information about two principals.  It contains
the name of a _subject_, and the subject's public key.  It also
contains a statement by a second principal, the _issuer_, that the
subject is who he claims to be, and that this is indeed the subject's
public key.  The issuer's statement is signed with the issuer's private
key, which only the issuer knows.  However, anyone can verify the
issuer's statement by finding the issuer's public key, decrypting the
statement with it, and comparing it to the other information in the
certificate.  The certificate also contains information about the time
period over which it is valid.  This is expressed as two fields, called
"notBefore" and "notAfter".

  In the Python use of certificates, a client or server can use a
certificate to prove who they are.  The other side of a network
connection can also be required to produce a certificate, and that
certificate can be validated to the satisfaction of the client or
server that requires such validation.  The connection attempt can be
set to raise an exception if the validation fails.  Validation is done
automatically, by the underlying OpenSSL framework; the application
need not concern itself with its mechanics.  But the application does
usually need to provide sets of certificates to allow this process to
take place.

  Python uses files to contain certificates.  They should be formatted
as "PEM" (see RFC 1422(1)), which is a base-64 encoded form wrapped
with a header line and a footer line:

    -----BEGIN CERTIFICATE-----
    ... (certificate in base64 PEM encoding) ...
    -----END CERTIFICATE-----

The Python files which contain certificates can contain a sequence of
certificates, sometimes called a _certificate chain_.  This chain
should start with the specific certificate for the principal who "is"
the client or server, and then the certificate for the issuer of that
certificate, and then the certificate for the issuer of _that_
certificate, and so on up the chain till you get to a certificate which
is _self-signed_, that is, a certificate which has the same subject and
issuer, sometimes called a _root certificate_.  The certificates should
just be concatenated together in the certificate file.  For example,
suppose we had a three certificate chain, from our server certificate
to the certificate of the certification authority that signed our server
certificate, to the root certificate of the agency which issued the
certification authority's certificate:

    -----BEGIN CERTIFICATE-----
    ... (certificate for your server)...
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    ... (the certificate for the CA)...
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    ... (the root certificate for the CA's issuer)...
    -----END CERTIFICATE-----

If you are going to require validation of the other side of the
connection's certificate, you need to provide a "CA certs" file, filled
with the certificate chains for each issuer you are willing to trust.
Again, this file just contains these chains concatenated together.  For
validation, Python will use the first chain it finds in the file which
matches.

  Some "standard" root certificates are available from various
certification authorities: CACert.org(2), Thawte(3), Verisign(4),
Positive SSL(5) (used by python.org), Equifax and GeoTrust(6).

  In general, if you are using SSL3 or TLS1, you don't need to put the
full chain in your "CA certs" file; you only need the root
certificates, and the remote peer is supposed to furnish the other
certificates necessary to chain from its certificate to a root
certificate.  See RFC 4158(7) for more discussion of the way in which
certification chains can be built.

  If you are going to create a server that provides SSL-encrypted
connection services, you will need to acquire a certificate for that
service.  There are many ways of acquiring appropriate certificates,
such as buying one from a certification authority.  Another common
practice is to generate a self-signed certificate.  The simplest way to
do this is with the OpenSSL package, using something like the following:

    % openssl req -new -x509 -days 365 -nodes -out cert.pem -keyout cert.pem
    Generating a 1024 bit RSA private key
    .......++++++
    .............................++++++
    writing new private key to 'cert.pem'
    -----
    You are about to be asked to enter information that will be incorporated
    into your certificate request.
    What you are about to enter is what is called a Distinguished Name or a DN.
    There are quite a few fields but you can leave some blank
    For some fields there will be a default value,
    If you enter '.', the field will be left blank.
    -----
    Country Name (2 letter code) [AU]:US
    State or Province Name (full name) [Some-State]:MyState
    Locality Name (eg, city) []:Some City
    Organization Name (eg, company) [Internet Widgits Pty Ltd]:My Organization, Inc.
    Organizational Unit Name (eg, section) []:My Group
    Common Name (eg, YOUR name) []:myserver.mygroup.myorganization.com
    Email Address []:ops@myserver.mygroup.myorganization.com
    %

The disadvantage of a self-signed certificate is that it is its own root
certificate, and no one else will have it in their cache of known (and
trusted) root certificates.

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc1422.html

  (2) http://www.cacert.org/index.php?id=3

  (3) http://www.thawte.com/roots/

  (4) http://www.verisign.com/support/roots.html

  (5)
http://www.PositiveSSL.com/ssl-certificate-support/cert_installation/UTN-USERFirst-Hardware.crt

  (6) http://www.geotrust.com/resources/root_certificates/index.asp

  (7) http://tools.ietf.org/html/rfc4158.html


File: python.info,  Node: Examples<8>,  Prev: Certificates,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.4 Examples
.................

* Menu:

* Testing for SSL support::
* Client-side operation::
* Server-side operation::


File: python.info,  Node: Testing for SSL support,  Next: Client-side operation,  Up: Examples<8>

5.17.3.5 Testing for SSL support
................................

To test for the presence of SSL support in a Python installation, user
code should use the following idiom:

    try:
        import ssl
    except ImportError:
        pass
    else:
        ... # do something that requires SSL support



File: python.info,  Node: Client-side operation,  Next: Server-side operation,  Prev: Testing for SSL support,  Up: Examples<8>

5.17.3.6 Client-side operation
..............................

This example connects to an SSL server, prints the server's address and
certificate, sends some bytes, and reads part of the response:

    import socket, ssl, pprint

    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    # require a certificate from the server
    ssl_sock = ssl.wrap_socket(s,
                               ca_certs="/etc/ca_certs_file",
                               cert_reqs=ssl.CERT_REQUIRED)

    ssl_sock.connect(('www.verisign.com', 443))

    print repr(ssl_sock.getpeername())
    print ssl_sock.cipher()
    print pprint.pformat(ssl_sock.getpeercert())

    # Set a simple HTTP request -- use httplib in actual code.
    ssl_sock.write("""GET / HTTP/1.0\r
    Host: www.verisign.com\r\n\r\n""")

    # Read a chunk of data.  Will not necessarily
    # read all the data returned by the server.
    data = ssl_sock.read()

    # note that closing the SSLSocket will also close the underlying socket
    ssl_sock.close()

As of September 6, 2007, the certificate printed by this program looked
like this:

    {'notAfter': 'May  8 23:59:59 2009 GMT',
     'subject': ((('serialNumber', u'2497886'),),
                 (('1.3.6.1.4.1.311.60.2.1.3', u'US'),),
                 (('1.3.6.1.4.1.311.60.2.1.2', u'Delaware'),),
                 (('countryName', u'US'),),
                 (('postalCode', u'94043'),),
                 (('stateOrProvinceName', u'California'),),
                 (('localityName', u'Mountain View'),),
                 (('streetAddress', u'487 East Middlefield Road'),),
                 (('organizationName', u'VeriSign, Inc.'),),
                 (('organizationalUnitName',
                   u'Production Security Services'),),
                 (('organizationalUnitName',
                   u'Terms of use at www.verisign.com/rpa (c)06'),),
                 (('commonName', u'www.verisign.com'),))}

which is a fairly poorly-formed `subject' field.


File: python.info,  Node: Server-side operation,  Prev: Client-side operation,  Up: Examples<8>

5.17.3.7 Server-side operation
..............................

For server operation, typically you'd need to have a server
certificate, and private key, each in a file.  You'd open a socket,
bind it to a port, call `listen()' on it, then start waiting for
clients to connect:

    import socket, ssl

    bindsocket = socket.socket()
    bindsocket.bind(('myaddr.mydomain.com', 10023))
    bindsocket.listen(5)

When one did, you'd call `accept()' on the socket to get the new socket
from the other end, and use *note wrap_socket(): 256. to create a
server-side SSL context for it:

    while True:
        newsocket, fromaddr = bindsocket.accept()
        connstream = ssl.wrap_socket(newsocket,
                                     server_side=True,
                                     certfile="mycertfile",
                                     keyfile="mykeyfile",
                                     ssl_version=ssl.PROTOCOL_TLSv1)
        try:
            deal_with_client(connstream)
        finally:
            connstream.shutdown(socket.SHUT_RDWR)
            connstream.close()

Then you'd read data from the `connstream' and do something with it
till you are finished with the client (or the client is finished with
you):

    def deal_with_client(connstream):
        data = connstream.read()
        # null data means the client is finished with us
        while data:
            if not do_something(connstream, data):
                # we'll assume do_something returns False
                # when we're finished with client
                break
            data = connstream.read()
        # finished with client

And go back to listening for new client connections.

See also
........

Class *note socket.socket: 157a.
     Documentation of underlying *note socket: 15c. class

TLS (Transport Layer Security) and SSL (Secure Socket Layer)(1)
     Debby Koren

RFC 1422: Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-Based Key Management(2)
     Steve Kent

RFC 1750: Randomness Recommendations for Security(3)
     D. Eastlake et. al.

RFC 3280: Internet X.509 Public Key Infrastructure Certificate and CRL Profile(4)
     Housley et. al.

  ---------- Footnotes ----------

  (1) http://www3.rad.com/networks/applications/secure/tls.htm

  (2) http://www.ietf.org/rfc/rfc1422

  (3) http://www.ietf.org/rfc/rfc1750

  (4) http://www.ietf.org/rfc/rfc3280


File: python.info,  Node: signal --- Set handlers for asynchronous events,  Next: popen2 --- Subprocesses with accessible I/O streams,  Prev: ssl --- TLS/SSL wrapper for socket objects,  Up: Interprocess Communication and Networking

5.17.4 `signal' -- Set handlers for asynchronous events
-------------------------------------------------------

This module provides mechanisms to use signal handlers in Python. Some
general rules for working with signals and their handlers:

   * A handler for a particular signal, once set, remains installed
     until it is explicitly reset (Python emulates the BSD style
     interface regardless of the underlying implementation), with the
     exception of the handler for `SIGCHLD', which follows the
     underlying implementation.

   * There is no way to "block" signals temporarily from critical
     sections (since this is not supported by all Unix flavors).

   * Although Python signal handlers are called asynchronously as far
     as the Python user is concerned, they can only occur between the
     "atomic" instructions of the Python interpreter.  This means that
     signals arriving during long calculations implemented purely in C
     (such as regular expression matches on large bodies of text) may
     be delayed for an arbitrary amount of time.

   * When a signal arrives during an I/O operation, it is possible that
     the I/O operation raises an exception after the signal handler
     returns. This is dependent on the underlying Unix system's
     semantics regarding interrupted system calls.

   * Because the C signal handler always returns, it makes little sense
     to catch synchronous errors like `SIGFPE' or `SIGSEGV'.

   * Python installs a small number of signal handlers by default:
     `SIGPIPE' is ignored (so write errors on pipes and sockets can be
     reported as ordinary Python exceptions) and `SIGINT' is translated
     into a *note KeyboardInterrupt: 24e. exception.  All of these can
     be overridden.

   * Some care must be taken if both signals and threads are used in
     the same program.  The fundamental thing to remember in using
     signals and threads simultaneously is: always perform *note
     signal(): 155. operations in the main thread of execution.  Any
     thread can perform an *note alarm(): 174f, *note getsignal(): 1750,
     *note pause(): 1751, *note setitimer(): 1752. or *note
     getitimer(): 1753.; only the main thread can set a new signal
     handler, and the main thread will be the only one to receive
     signals (this is enforced by the Python *note signal: 155. module,
     even if the underlying thread implementation supports sending
     signals to individual threads).  This means that signals can't be
     used as a means of inter-thread communication.  Use locks instead.

  The variables defined in the *note signal: 155. module are:

 -- Data: signal.SIG_DFL
     This is one of two standard signal handling options; it will
     simply perform the default function for the signal.  For example,
     on most systems the default action for `SIGQUIT' is to dump core
     and exit, while the default action for `SIGCHLD' is to simply
     ignore it.

 -- Data: signal.SIG_IGN
     This is another standard signal handler, which will simply ignore
     the given signal.

 -- Data: SIG*
     All the signal numbers are defined symbolically.  For example, the
     hangup signal is defined as `signal.SIGHUP'; the variable names
     are identical to the names used in C programs, as found in
     `<signal.h>'. The Unix man page for '`signal()'' lists the
     existing signals (on some systems this is `signal(2)', on others
     the list is in `signal(7)'). Note that not all systems define the
     same set of signal names; only those names defined by the system
     are defined by this module.

 -- Data: signal.CTRL_C_EVENT
     The signal corresponding to the CTRL+C keystroke event. This
     signal can only be used with *note os.kill(): 2ce.

     Availability: Windows.

     New in version 2.7.

 -- Data: signal.CTRL_BREAK_EVENT
     The signal corresponding to the CTRL+BREAK keystroke event. This
     signal can only be used with *note os.kill(): 2ce.

     Availability: Windows.

     New in version 2.7.

 -- Data: signal.NSIG
     One more than the number of the highest signal number.

 -- Data: signal.ITIMER_REAL
     Decrements interval timer in real time, and delivers `SIGALRM'
     upon expiration.

 -- Data: signal.ITIMER_VIRTUAL
     Decrements interval timer only when the process is executing, and
     delivers SIGVTALRM upon expiration.

 -- Data: signal.ITIMER_PROF
     Decrements interval timer both when the process executes and when
     the system is executing on behalf of the process. Coupled with
     ITIMER_VIRTUAL, this timer is usually used to profile the time
     spent by the application in user and kernel space. SIGPROF is
     delivered upon expiration.

  The *note signal: 155. module defines one exception:

 -- Exception: signal.ItimerError
     Raised to signal an error from the underlying *note setitimer():
     1752. or *note getitimer(): 1753. implementation. Expect this
     error if an invalid interval timer or a negative time is passed to
     *note setitimer(): 1752.  This error is a subtype of *note
     IOError: 1f7.

  The *note signal: 155. module defines the following functions:

 -- Function: signal.alarm (time)
     If _time_ is non-zero, this function requests that a `SIGALRM'
     signal be sent to the process in _time_ seconds. Any previously
     scheduled alarm is canceled (only one alarm can be scheduled at
     any time).  The returned value is then the number of seconds
     before any previously set alarm was to have been delivered. If
     _time_ is zero, no alarm is scheduled, and any scheduled alarm is
     canceled.  If the return value is zero, no alarm is currently
     scheduled.  (See the Unix man page `alarm(2)'.) Availability: Unix.

 -- Function: signal.getsignal (signalnum)
     Return the current signal handler for the signal _signalnum_. The
     returned value may be a callable Python object, or one of the
     special values *note signal.SIG_IGN: 1755, *note signal.SIG_DFL:
     1754. or *note None: 393.  Here, *note signal.SIG_IGN: 1755. means
     that the signal was previously ignored, *note signal.SIG_DFL:
     1754. means that the default way of handling the signal was
     previously in use, and `None' means that the previous signal
     handler was not installed from Python.

 -- Function: signal.pause ()
     Cause the process to sleep until a signal is received; the
     appropriate handler will then be called.  Returns nothing.  Not on
     Windows. (See the Unix man page `signal(2)'.)

 -- Function: signal.setitimer (which, seconds[, interval])
     Sets given interval timer (one of *note signal.ITIMER_REAL: 1757,
     *note signal.ITIMER_VIRTUAL: 1758. or *note signal.ITIMER_PROF:
     1759.) specified by _which_ to fire after _seconds_ (float is
     accepted, different from *note alarm(): 174f.) and after that
     every _interval_ seconds. The interval timer specified by _which_
     can be cleared by setting seconds to zero.

     When an interval timer fires, a signal is sent to the process.
     The signal sent is dependent on the timer being used; *note
     signal.ITIMER_REAL: 1757. will deliver `SIGALRM', *note
     signal.ITIMER_VIRTUAL: 1758. sends `SIGVTALRM', and *note
     signal.ITIMER_PROF: 1759. will deliver `SIGPROF'.

     The old values are returned as a tuple: (delay, interval).

     Attempting to pass an invalid interval timer will cause an *note
     ItimerError: 175a.  Availability: Unix.

     New in version 2.6.

 -- Function: signal.getitimer (which)
     Returns current value of a given interval timer specified by
     _which_.  Availability: Unix.

     New in version 2.6.

 -- Function: signal.set_wakeup_fd (fd)
     Set the wakeup fd to _fd_.  When a signal is received, a `'\0''
     byte is written to the fd.  This can be used by a library to
     wakeup a poll or select call, allowing the signal to be fully
     processed.

     The old wakeup fd is returned.  _fd_ must be non-blocking.  It is
     up to the library to remove any bytes before calling poll or
     select again.

     When threads are enabled, this function can only be called from
     the main thread; attempting to call it from other threads will
     cause a *note ValueError: 233.  exception to be raised.

     New in version 2.6.

 -- Function: signal.siginterrupt (signalnum, flag)
     Change system call restart behaviour: if _flag_ is *note False:
     3aa, system calls will be restarted when interrupted by signal
     _signalnum_, otherwise system calls will be interrupted.  Returns
     nothing.  Availability: Unix (see the man page `siginterrupt(3)'
     for further information).

     Note that installing a signal handler with *note signal(): 155.
     will reset the restart behaviour to interruptible by implicitly
     calling `siginterrupt()' with a true _flag_ value for the given
     signal.

     New in version 2.6.

 -- Function: signal.signal (signalnum, handler)
     Set the handler for signal _signalnum_ to the function _handler_.
     _handler_ can be a callable Python object taking two arguments
     (see below), or one of the special values *note signal.SIG_IGN:
     1755. or *note signal.SIG_DFL: 1754.  The previous signal handler
     will be returned (see the description of *note getsignal(): 1750.
     above).  (See the Unix man page `signal(2)'.)

     When threads are enabled, this function can only be called from
     the main thread; attempting to call it from other threads will
     cause a *note ValueError: 233.  exception to be raised.

     The _handler_ is called with two arguments: the signal number and
     the current stack frame (`None' or a frame object; for a
     description of frame objects, see the *note description in the
     type hierarchy: 6ef. or see the attribute descriptions in the
     *note inspect: f8. module).

     On Windows, *note signal(): 155. can only be called with `SIGABRT',
     `SIGFPE', `SIGILL', `SIGINT', `SIGSEGV', or `SIGTERM'. A *note
     ValueError: 233. will be raised in any other case.

* Menu:

* Example: Example<9>.


File: python.info,  Node: Example<9>,  Up: signal --- Set handlers for asynchronous events

5.17.4.1 Example
................

Here is a minimal example program. It uses the *note alarm(): 174f.
function to limit the time spent waiting to open a file; this is useful
if the file is for a serial device that may not be turned on, which
would normally cause the *note os.open(): 5d5. to hang indefinitely.
The solution is to set a 5-second alarm before opening the file; if the
operation takes too long, the alarm signal will be sent, and the
handler raises an exception.

    import signal, os

    def handler(signum, frame):
        print 'Signal handler called with signal', signum
        raise IOError("Couldn't open device!")

    # Set the signal handler and a 5-second alarm
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(5)

    # This open() may hang indefinitely
    fd = os.open('/dev/ttyS0', os.O_RDWR)

    signal.alarm(0)          # Disable the alarm



File: python.info,  Node: popen2 --- Subprocesses with accessible I/O streams,  Next: asyncore --- Asynchronous socket handler,  Prev: signal --- Set handlers for asynchronous events,  Up: Interprocess Communication and Networking

5.17.5 `popen2' -- Subprocesses with accessible I/O streams
-----------------------------------------------------------

Deprecated since version 2.6: This module is obsolete.  Use the *note
subprocess: 167. module.  Check especially the *note Replacing Older
Functions with the subprocess Module: 10e1. section.

  This module allows you to spawn processes and connect to their
input/output/error pipes and obtain their return codes under Unix and
Windows.

  The *note subprocess: 167. module provides more powerful facilities
for spawning new processes and retrieving their results.  Using the
*note subprocess: 167. module is preferable to using the *note popen2:
134. module.

  The primary interface offered by this module is a trio of factory
functions.  For each of these, if _bufsize_ is specified,  it specifies
the buffer size for the I/O pipes.  _mode_, if provided, should be the
string `'b'' or `'t''; on Windows this is needed to determine whether
the file objects should be opened in binary or text mode.  The default
value for _mode_ is `'t''.

  On Unix, _cmd_ may be a sequence, in which case arguments will be
passed directly to the program without shell intervention (as with
*note os.spawnv(): 10e3.).  If _cmd_ is a string it will be passed to
the shell (as with *note os.system(): 3f3.).

  The only way to retrieve the return codes for the child processes is
by using the `poll()' or `wait()' methods on the *note Popen3: 16eb. and
*note Popen4: 16ec. classes; these are only available on Unix.  This
information is not available when using the *note popen2(): 134, *note
popen3(): 1761, and *note popen4(): 1762.  functions, or the equivalent
functions in the *note os: 128. module. (Note that the tuples returned
by the *note os: 128. module's functions are in a different order from
the ones returned by the *note popen2: 134. module.)

 -- Function: popen2.popen2 (cmd[, bufsize[, mode]])
     Executes _cmd_ as a sub-process.  Returns the file objects
     `(child_stdout, child_stdin)'.

 -- Function: popen2.popen3 (cmd[, bufsize[, mode]])
     Executes _cmd_ as a sub-process.  Returns the file objects
     `(child_stdout, child_stdin, child_stderr)'.

 -- Function: popen2.popen4 (cmd[, bufsize[, mode]])
     Executes _cmd_ as a sub-process.  Returns the file objects
     `(child_stdout_and_stderr, child_stdin)'.

     New in version 2.0.

  On Unix, a class defining the objects returned by the factory
functions is also available.  These are not used for the Windows
implementation, and are not available on that platform.

 -- Class: popen2.Popen3 (cmd[, capturestderr[, bufsize]])
     This class represents a child process.  Normally, *note Popen3:
     16eb. instances are created using the *note popen2(): 134. and
     *note popen3(): 1761. factory functions described above.

     If not using one of the helper functions to create *note Popen3:
     16eb. objects, the parameter _cmd_ is the shell command to execute
     in a sub-process.  The _capturestderr_ flag, if true, specifies
     that the object should capture standard error output of the child
     process. The default is false.  If the _bufsize_ parameter is
     specified, it specifies the size of the I/O buffers to/from the
     child process.

 -- Class: popen2.Popen4 (cmd[, bufsize])
     Similar to *note Popen3: 16eb, but always captures standard error
     into the same file object as standard output.  These are typically
     created using *note popen4(): 1762.

     New in version 2.0.

* Menu:

* Popen3 and Popen4 Objects::
* Flow Control Issues::


File: python.info,  Node: Popen3 and Popen4 Objects,  Next: Flow Control Issues,  Up: popen2 --- Subprocesses with accessible I/O streams

5.17.5.1 Popen3 and Popen4 Objects
..................................

Instances of the *note Popen3: 16eb. and *note Popen4: 16ec. classes
have the following methods:

 -- Method: Popen3.poll ()
     Returns `-1' if child process hasn't completed yet, or its status
     code (see *note wait(): 1767.) otherwise.

 -- Method: Popen3.wait ()
     Waits for and returns the status code of the child process.  The
     status code encodes both the return code of the process and
     information about whether it exited using the `exit()' system call
     or died due to a signal.  Functions to help interpret the status
     code are defined in the *note os: 128. module; see section *note
     Process Management: 113a. for the `W*()' family of functions.

  The following attributes are also available:

 -- Attribute: Popen3.fromchild
     A file object that provides output from the child process.  For
     *note Popen4: 16ec.  instances, this will provide both the
     standard output and standard error streams.

 -- Attribute: Popen3.tochild
     A file object that provides input to the child process.

 -- Attribute: Popen3.childerr
     A file object that provides error output from the child process, if
     _capturestderr_ was true for the constructor, otherwise `None'.
     This will always be `None' for *note Popen4: 16ec. instances.

 -- Attribute: Popen3.pid
     The process ID of the child process.


File: python.info,  Node: Flow Control Issues,  Prev: Popen3 and Popen4 Objects,  Up: popen2 --- Subprocesses with accessible I/O streams

5.17.5.2 Flow Control Issues
............................

Any time you are working with any form of inter-process communication,
control flow needs to be carefully thought out.  This remains the case
with the file objects provided by this module (or the *note os: 128.
module equivalents).

  When reading output from a child process that writes a lot of data to
standard error while the parent is reading from the child's standard
output, a deadlock can occur.  A similar situation can occur with other
combinations of reads and writes.  The essential factors are that more
than `_PC_PIPE_BUF' bytes are being written by one process in a
blocking fashion, while the other process is reading from the first
process, also in a blocking fashion.

  There are several ways to deal with this situation.

  The simplest application change, in many cases, will be to follow
this model in the parent process:

    import popen2

    r, w, e = popen2.popen3('python slave.py')
    e.readlines()
    r.readlines()
    r.close()
    e.close()
    w.close()

with code like this in the child:

    import os
    import sys

    # note that each of these print statements
    # writes a single long string

    print >>sys.stderr, 400 * 'this is a test\n'
    os.close(sys.stderr.fileno())
    print >>sys.stdout, 400 * 'this is another test\n'

In particular, note that `sys.stderr' must be closed after writing all
data, or `readlines()' won't return.  Also note that *note os.close():
10ea. must be used, as `sys.stderr.close()' won't close `stderr'
(otherwise assigning to `sys.stderr' will silently close it, so no
further errors can be printed).

  Applications which need to support a more general approach should
integrate I/O over pipes with their *note select(): 14e. loops, or use
separate threads to read each of the individual files provided by
whichever `popen*()' function or `Popen*' class was used.

See also
........

Module *note subprocess: 167.
     Module for spawning and managing subprocesses.


File: python.info,  Node: asyncore --- Asynchronous socket handler,  Next: asynchat --- Asynchronous socket command/response handler,  Prev: popen2 --- Subprocesses with accessible I/O streams,  Up: Interprocess Communication and Networking

5.17.6 `asyncore' -- Asynchronous socket handler
------------------------------------------------

*Source code:* Lib/asyncore.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  This module provides the basic infrastructure for writing
asynchronous  socket service clients and servers.

  There are only two ways to have a program on a single processor do
"more than one thing at a time." Multi-threaded programming is the
simplest and most popular way to do it, but there is another very
different technique, that lets you have nearly all the advantages of
multi-threading, without actually using multiple threads.  It's really
only practical if your program is largely I/O bound.  If your program
is processor bound, then pre-emptive scheduled threads are probably
what you really need.  Network servers are rarely processor bound,
however.

  If your operating system supports the `select()' system call in its
I/O library (and nearly all do), then you can use it to juggle multiple
communication channels at once; doing other work while your I/O is
taking place in the "background."  Although this strategy can seem
strange and complex, especially at first, it is in many ways easier to
understand and control than multi-threaded programming.  The *note
asyncore: 11. module solves many of the difficult problems for you,
making the task of building sophisticated high-performance network
servers and clients a snap.  For "conversational" applications and
protocols the companion *note asynchat: 10.  module is invaluable.

  The basic idea behind both modules is to create one or more network
_channels_, instances of class *note asyncore.dispatcher: 176f. and
*note asynchat.async_chat: 1770.  Creating the channels adds them to a
global map, used by the *note loop(): 1771. function if you do not
provide it with your own _map_.

  Once the initial channel(s) is(are) created, calling the *note
loop(): 1771. function activates channel service, which continues until
the last channel (including any that have been added to the map during
asynchronous service) is closed.

 -- Function: asyncore.loop ([timeout[, use_poll[, map[, count]]]])
     Enter a polling loop that terminates after count passes or all open
     channels have been closed.  All arguments are optional.  The
     _count_ parameter defaults to None, resulting in the loop
     terminating only when all channels have been closed.  The
     _timeout_ argument sets the timeout parameter for the appropriate
     *note select(): 14e. or `poll()' call, measured in seconds; the
     default is 30 seconds.  The _use_poll_ parameter, if true,
     indicates that `poll()' should be used in preference to *note
     select(): 14e.  (the default is `False').

     The _map_ parameter is a dictionary whose items are the channels
     to watch.  As channels are closed they are deleted from their map.
     If _map_ is omitted, a global map is used. Channels (instances of
     *note asyncore.dispatcher: 176f, *note asynchat.async_chat: 1770.
     and subclasses thereof) can freely be mixed in the map.

 -- Class: asyncore.dispatcher
     The *note dispatcher: 176f. class is a thin wrapper around a
     low-level socket object. To make it more useful, it has a few
     methods for event-handling which are called from the asynchronous
     loop.   Otherwise, it can be treated as a normal non-blocking
     socket object.

     The firing of low-level events at certain times or in certain
     connection states tells the asynchronous loop that certain
     higher-level events have taken place.  For example, if we have
     asked for a socket to connect to another host, we know that the
     connection has been made when the socket becomes writable for the
     first time (at this point you know that you may write to it with
     the expectation of success).  The implied higher-level events are:

     Event                      Description
     ------------------------------------------------------------------------ 
     `handle_connect()'         Implied by the first read or write event
     `handle_close()'           Implied by a read event with no data
                                available
     `handle_accept()'          Implied by a read event on a listening
                                socket

     During asynchronous processing, each mapped channel's *note
     readable(): 1772. and *note writable(): 1773. methods are used to
     determine whether the channel's socket should be added to the list
     of channels `select()'ed or `poll()'ed for read and write events.

     Thus, the set of channel events is larger than the basic socket
     events.  The full set of methods that can be overridden in your
     subclass follows:

      -- Method: handle_read ()
          Called when the asynchronous loop detects that a `read()'
          call on the channel's socket will succeed.

      -- Method: handle_write ()
          Called when the asynchronous loop detects that a writable
          socket can be written.  Often this method will implement the
          necessary buffering for performance.  For example:

              def handle_write(self):
                  sent = self.send(self.buffer)
                  self.buffer = self.buffer[sent:]



      -- Method: handle_expt ()
          Called when there is out of band (OOB) data for a socket
          connection.  This will almost never happen, as OOB is
          tenuously supported and rarely used.

      -- Method: handle_connect ()
          Called when the active opener's socket actually makes a
          connection.  Might send a "welcome" banner, or initiate a
          protocol negotiation with the remote endpoint, for example.

      -- Method: handle_close ()
          Called when the socket is closed.

      -- Method: handle_error ()
          Called when an exception is raised and not otherwise handled.
          The default version prints a condensed traceback.

      -- Method: handle_accept ()
          Called on listening channels (passive openers) when a
          connection can be established with a new remote endpoint that
          has issued a *note connect(): 177b.  call for the local
          endpoint.

      -- Method: readable ()
          Called each time around the asynchronous loop to determine
          whether a channel's socket should be added to the list on
          which read events can occur.  The default method simply
          returns `True', indicating that by default, all channels will
          be interested in read events.

      -- Method: writable ()
          Called each time around the asynchronous loop to determine
          whether a channel's socket should be added to the list on
          which write events can occur.  The default method simply
          returns `True', indicating that by default, all channels will
          be interested in write events.

     In addition, each channel delegates or extends many of the socket
     methods.  Most of these are nearly identical to their socket
     partners.

      -- Method: create_socket (family, type)
          This is identical to the creation of a normal socket, and
          will use the same options for creation.  Refer to the *note
          socket: 15c. documentation for information on creating
          sockets.

      -- Method: connect (address)
          As with the normal socket object, _address_ is a tuple with
          the first element the host to connect to, and the second the
          port number.

      -- Method: send (data)
          Send _data_ to the remote end-point of the socket.

      -- Method: recv (buffer_size)
          Read at most _buffer_size_ bytes from the socket's remote
          end-point.  An empty string implies that the channel has been
          closed from the other end.

      -- Method: listen (backlog)
          Listen for connections made to the socket.  The _backlog_
          argument specifies the maximum number of queued connections
          and should be at least 1; the maximum value is
          system-dependent (usually 5).

      -- Method: bind (address)
          Bind the socket to _address_.  The socket must not already be
          bound.  (The format of _address_ depends on the address
          family -- refer to the *note socket: 15c. documentation for
          more information.)  To mark the socket as re-usable (setting
          the `SO_REUSEADDR' option), call the *note dispatcher: 176f.
          object's `set_reuse_addr()' method.

      -- Method: accept ()
          Accept a connection.  The socket must be bound to an address
          and listening for connections.  The return value can be
          either `None' or a pair `(conn, address)' where _conn_ is a
          _new_ socket object usable to send and receive data on the
          connection, and _address_ is the address bound to the socket
          on the other end of the connection.  When `None' is returned
          it means the connection didn't take place, in which case the
          server should just ignore this event and keep listening for
          further incoming connections.

      -- Method: close ()
          Close the socket.  All future operations on the socket object
          will fail.  The remote end-point will receive no more data
          (after queued data is flushed).  Sockets are automatically
          closed when they are garbage-collected.

 -- Class: asyncore.dispatcher_with_send
     A *note dispatcher: 176f. subclass which adds simple buffered
     output capability, useful for simple clients. For more
     sophisticated usage use *note asynchat.async_chat: 1770.

 -- Class: asyncore.file_dispatcher
     A file_dispatcher takes a file descriptor or file object along
     with an optional map argument and wraps it for use with the
     `poll()' or `loop()' functions.  If provided a file object or
     anything with a `fileno()' method, that method will be called and
     passed to the *note file_wrapper: 1785. constructor.
     Availability: UNIX.

 -- Class: asyncore.file_wrapper
     A file_wrapper takes an integer file descriptor and calls *note
     os.dup(): 10ed. to duplicate the handle so that the original
     handle may be closed independently of the file_wrapper.  This
     class implements sufficient methods to emulate a socket for use by
     the *note file_dispatcher: 1784. class.  Availability: UNIX.

* Menu:

* asyncore Example basic HTTP client::
* asyncore Example basic echo server::

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/asyncore.py


File: python.info,  Node: asyncore Example basic HTTP client,  Next: asyncore Example basic echo server,  Up: asyncore --- Asynchronous socket handler

5.17.6.1 asyncore Example basic HTTP client
...........................................

Here is a very basic HTTP client that uses the *note dispatcher: 176f.
class to implement its socket handling:

    import asyncore, socket

    class HTTPClient(asyncore.dispatcher):

        def __init__(self, host, path):
            asyncore.dispatcher.__init__(self)
            self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
            self.connect( (host, 80) )
            self.buffer = 'GET %s HTTP/1.0\r\n\r\n' % path

        def handle_connect(self):
            pass

        def handle_close(self):
            self.close()

        def handle_read(self):
            print self.recv(8192)

        def writable(self):
            return (len(self.buffer) > 0)

        def handle_write(self):
            sent = self.send(self.buffer)
            self.buffer = self.buffer[sent:]


    client = HTTPClient('www.python.org', '/')
    asyncore.loop()



File: python.info,  Node: asyncore Example basic echo server,  Prev: asyncore Example basic HTTP client,  Up: asyncore --- Asynchronous socket handler

5.17.6.2 asyncore Example basic echo server
...........................................

Here is a basic echo server that uses the *note dispatcher: 176f. class
to accept connections and dispatches the incoming connections to a
handler:

    import asyncore
    import socket

    class EchoHandler(asyncore.dispatcher_with_send):

        def handle_read(self):
            data = self.recv(8192)
            if data:
                self.send(data)

    class EchoServer(asyncore.dispatcher):

        def __init__(self, host, port):
            asyncore.dispatcher.__init__(self)
            self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
            self.set_reuse_addr()
            self.bind((host, port))
            self.listen(5)

        def handle_accept(self):
            pair = self.accept()
            if pair is not None:
                sock, addr = pair
                print 'Incoming connection from %s' % repr(addr)
                handler = EchoHandler(sock)

    server = EchoServer('localhost', 8080)
    asyncore.loop()



File: python.info,  Node: asynchat --- Asynchronous socket command/response handler,  Prev: asyncore --- Asynchronous socket handler,  Up: Interprocess Communication and Networking

5.17.7 `asynchat' -- Asynchronous socket command/response handler
-----------------------------------------------------------------

*Source code:* Lib/asynchat.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  This module builds on the *note asyncore: 11. infrastructure,
simplifying asynchronous clients and servers and making it easier to
handle protocols whose elements are terminated by arbitrary strings, or
are of variable length.  *note asynchat: 10. defines the abstract class
*note async_chat: 1770. that you subclass, providing implementations of
the `collect_incoming_data()' and `found_terminator()' methods. It uses
the same asynchronous loop as *note asyncore: 11, and the two types of
channel, *note asyncore.dispatcher: 176f.  and *note
asynchat.async_chat: 1770, can freely be mixed in the channel map.
Typically an *note asyncore.dispatcher: 176f. server channel generates
new *note asynchat.async_chat: 1770. channel objects as it receives
incoming connection requests.

 -- Class: asynchat.async_chat
     This class is an abstract subclass of *note asyncore.dispatcher:
     176f. To make practical use of the code you must subclass *note
     async_chat: 1770, providing meaningful *note
     collect_incoming_data(): 178c. and *note found_terminator(): 178d.
     methods.  The *note asyncore.dispatcher: 176f. methods can be
     used, although not all make sense in a message/response context.

     Like *note asyncore.dispatcher: 176f, *note async_chat: 1770.
     defines a set of events that are generated by an analysis of
     socket conditions after a `select()' call. Once the polling loop
     has been started the *note async_chat: 1770. object's methods are
     called by the event-processing framework with no action on the
     part of the programmer.

     Two class attributes can be modified, to improve performance, or
     possibly even to conserve memory.

      -- Data: ac_in_buffer_size
          The asynchronous input buffer size (default `4096').

      -- Data: ac_out_buffer_size
          The asynchronous output buffer size (default `4096').

     Unlike *note asyncore.dispatcher: 176f, *note async_chat: 1770.
     allows you to define a first-in-first-out queue (fifo) of
     _producers_. A producer need have only one method, `more()', which
     should return data to be transmitted on the channel.  The producer
     indicates exhaustion (_i.e._ that it contains no more data) by
     having its `more()' method return the empty string. At this point
     the *note async_chat: 1770. object removes the producer from the
     fifo and starts using the next producer, if any. When the producer
     fifo is empty the `handle_write()' method does nothing. You use
     the channel object's *note set_terminator(): 1790. method to
     describe how to recognize the end of, or an important breakpoint
     in, an incoming transmission from the remote endpoint.

     To build a functioning *note async_chat: 1770. subclass your
     input methods *note collect_incoming_data(): 178c. and *note
     found_terminator(): 178d. must handle the data that the channel
     receives asynchronously. The methods are described below.

 -- Method: async_chat.close_when_done ()
     Pushes a `None' on to the producer fifo. When this producer is
     popped off the fifo it causes the channel to be closed.

 -- Method: async_chat.collect_incoming_data (data)
     Called with _data_ holding an arbitrary amount of received data.
     The default method, which must be overridden, raises a *note
     NotImplementedError: 93b. exception.

 -- Method: async_chat.discard_buffers ()
     In emergencies this method will discard any data held in the input
     and/or output buffers and the producer fifo.

 -- Method: async_chat.found_terminator ()
     Called when the incoming data stream  matches the termination
     condition set by *note set_terminator(): 1790. The default method,
     which must be overridden, raises a *note NotImplementedError: 93b.
     exception. The buffered input data should be available via an
     instance attribute.

 -- Method: async_chat.get_terminator ()
     Returns the current terminator for the channel.

 -- Method: async_chat.push (data)
     Pushes data on to the channel's fifo to ensure its transmission.
     This is all you need to do to have the channel write the data out
     to the network, although it is possible to use your own producers
     in more complex schemes to implement encryption and chunking, for
     example.

 -- Method: async_chat.push_with_producer (producer)
     Takes a producer object and adds it to the producer fifo
     associated with the channel.  When all currently-pushed producers
     have been exhausted the channel will consume this producer's data
     by calling its `more()' method and send the data to the remote
     endpoint.

 -- Method: async_chat.set_terminator (term)
     Sets the terminating condition to be recognized on the channel.
     `term' may be any of three types of value, corresponding to three
     different ways to handle incoming protocol data.

     term            Description
     ------------------------------------------------------------------ 
     _string_        Will call *note found_terminator(): 178d. when
                     the string is found in the input stream
     _integer_       Will call *note found_terminator(): 178d. when
                     the indicated number of characters have been
                     received
     `None'          The channel continues to collect data forever

     Note that any data following the terminator will be available for
     reading by the channel after *note found_terminator(): 178d. is
     called.

* Menu:

* asynchat - Auxiliary Classes::
* asynchat Example::

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/asynchat.py



Local Variables:
coding: utf-8
End:
