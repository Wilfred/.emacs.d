This is
/home/melpa/melpa/working/python-info-20130916.1420/python.info,
produced by makeinfo version 4.13 from
/home/melpa/melpa/working/python-info/python.texi.

Generated by Sphinx 1.1.3.
INFO-DIR-SECTION Programming
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY

     Python 2.7.5, September 16, 2013

     Georg Brandl

     Copyright (C) 1990-2013, Python Software Foundation


File: python.info,  Node: Demo scripts,  Prev: Help and configuration,  Up: turtle --- Turtle graphics for Tk

5.24.5.31 Demo scripts
......................

There is a set of demo scripts in the turtledemo directory located in
the `Demo/turtle' directory in the source distribution.

  It contains:

   - a set of 15 demo scripts demonstrating different features of the
     new module *note turtle: 184.

   - a demo viewer `turtleDemo.py' which can be used to view the
     sourcecode of the scripts and run them at the same time. 14 of the
     examples can be accessed via the Examples menu; all of them can
     also be run standalone.

   - The example `turtledemo_two_canvases.py' demonstrates the
     simultaneous use of two canvases with the turtle module.
     Therefore it only can be run standalone.

   - There is a `turtle.cfg' file in this directory, which also serves
     as an example for how to write and use such files.

  The demoscripts are:

Name                 Description                        Features
------------------------------------------------------------------------------------ 
bytedesign           complex classical turtlegraphics   *note tracer(): 2167,
                     pattern                            delay, *note update():
                                                        2172.
chaos                graphs Verhulst dynamics, shows    world coordinates
                     that computer's computations can   
                     generate results sometimes         
                     against the common sense           
                     expectations                       
clock                analog clock showing time of your  turtles as clock's hands,
                     computer                           ontimer
colormixer           experiment with r, g, b            *note ondrag(): 215b.
fractalcurves        Hilbert & Koch curves              recursion
lindenmayer          ethnomathematics (indian kolams)   L-System
minimal_hanoi        Towers of Hanoi                    Rectangular Turtles as
                                                        Hanoi discs (shape,
                                                        shapesize)
paint                super minimalistic drawing program *note onclick(): 2159.
peace                elementary                         turtle: appearance and
                                                        animation
penrose              aperiodic tiling with kites and    *note stamp(): 212c.
                     darts                              
planet_and_moon      simulation of gravitational system compound shapes, *note
                                                        Vec2D: 2184.
tree                 a (graphical) breadth first tree   *note clone(): 2161.
                     (using generators)                 
wikipedia            a pattern from the wikipedia       *note clone(): 2161, *note
                     article on turtle graphics         undo(): 212f.
yingyang             another elementary example         *note circle(): 212a.

  Have fun!


File: python.info,  Node: IDLE,  Next: Other Graphical User Interface Packages,  Prev: turtle --- Turtle graphics for Tk,  Up: Graphical User Interfaces with Tk

5.24.6 IDLE
-----------

IDLE is the Python IDE built with the `tkinter' GUI toolkit.

  IDLE has the following features:

   * coded in 100% pure Python, using the `tkinter' GUI toolkit

   * cross-platform: works on Windows and Unix

   * multi-window text editor with multiple undo, Python colorizing and
     many other features, e.g. smart indent and call tips

   * Python shell window (a.k.a. interactive interpreter)

   * debugger (not complete, but you can set breakpoints, view  and
     step)

* Menu:

* Menus::
* Basic editing and navigation::
* Syntax colors::
* Startup::

Menus

* File menu::
* Edit menu::
* Windows menu::
* Debug menu::
* Edit context menu::
* Shell context menu::

Basic editing and navigation

* Automatic indentation::
* Python Shell window::

Startup

* Command line usage::


File: python.info,  Node: Menus,  Next: Basic editing and navigation,  Up: IDLE

5.24.6.1 Menus
..............

* Menu:

* File menu::
* Edit menu::
* Windows menu::
* Debug menu::
* Edit context menu::
* Shell context menu::


File: python.info,  Node: File menu,  Next: Edit menu,  Up: Menus

5.24.6.2 File menu
..................

New window
     create a new editing window

Open...
     open an existing file

Open module...
     open an existing module (searches sys.path)

Class browser
     show classes and methods in current file

Path browser
     show sys.path directories, modules, classes and methods

Save
     save current window to the associated file (unsaved windows have a
     * before and after the window title)

Save As...
     save current window to new file, which becomes the associated file

Save Copy As...
     save current window to different file without changing the
     associated file

Close
     close current window (asks to save if unsaved)

Exit
     close all windows and quit IDLE (asks to save if unsaved)


File: python.info,  Node: Edit menu,  Next: Windows menu,  Prev: File menu,  Up: Menus

5.24.6.3 Edit menu
..................

Undo
     Undo last change to current window (max 1000 changes)

Redo
     Redo last undone change to current window

Cut
     Copy selection into system-wide clipboard; then delete selection

Copy
     Copy selection into system-wide clipboard

Paste
     Insert system-wide clipboard into window

Select All
     Select the entire contents of the edit buffer

Find...
     Open a search dialog box with many options

Find again
     Repeat last search

Find selection
     Search for the string in the selection

Find in Files...
     Open a search dialog box for searching files

Replace...
     Open a search-and-replace dialog box

Go to line
     Ask for a line number and show that line

Indent region
     Shift selected lines right 4 spaces

Dedent region
     Shift selected lines left 4 spaces

Comment out region
     Insert ## in front of selected lines

Uncomment region
     Remove leading # or ## from selected lines

Tabify region
     Turns _leading_ stretches of spaces into tabs

Untabify region
     Turn _all_ tabs into the right number of spaces

Expand word
     Expand the word you have typed to match another word in the same
     buffer; repeat to get a different expansion

Format Paragraph
     Reformat the current blank-line-separated paragraph

Import module
     Import or reload the current module

Run script
     Execute the current file in the __main__ namespace


File: python.info,  Node: Windows menu,  Next: Debug menu,  Prev: Edit menu,  Up: Menus

5.24.6.4 Windows menu
.....................

Zoom Height
     toggles the window between normal size (24x80) and maximum height.

  The rest of this menu lists the names of all open windows; select one
to bring it to the foreground (deiconifying it if necessary).


File: python.info,  Node: Debug menu,  Next: Edit context menu,  Prev: Windows menu,  Up: Menus

5.24.6.5 Debug menu
...................

   * in the Python Shell window only

Go to file/line
     Look around the insert point for a filename and line number, open
     the file, and show the line.  Useful to view the source lines
     referenced in an exception traceback.

Debugger
     Run commands in the shell under the debugger.

Stack viewer
     Show the stack traceback of the last exception.

Auto-open Stack Viewer
     Open stack viewer on traceback.


File: python.info,  Node: Edit context menu,  Next: Shell context menu,  Prev: Debug menu,  Up: Menus

5.24.6.6 Edit context menu
..........................

   * Right-click in Edit window (Control-click on OS X)

Cut
     Copy selection into system-wide clipboard; then delete selection

Copy
     Copy selection into system-wide clipboard

Paste
     Insert system-wide clipboard into window

Set Breakpoint
     Sets a breakpoint.  Breakpoints are only enabled when the debugger
     is open.

Clear Breakpoint
     Clears the breakpoint on that line.


File: python.info,  Node: Shell context menu,  Prev: Edit context menu,  Up: Menus

5.24.6.7 Shell context menu
...........................

   * Right-click in Python Shell window (Control-click on OS X)

Cut
     Copy selection into system-wide clipboard; then delete selection

Copy
     Copy selection into system-wide clipboard

Paste
     Insert system-wide clipboard into window

Go to file/line
     Same as in Debug menu.


File: python.info,  Node: Basic editing and navigation,  Next: Syntax colors,  Prev: Menus,  Up: IDLE

5.24.6.8 Basic editing and navigation
.....................................

   * `Backspace' deletes to the left; `Del' deletes to the right

   * Arrow keys and `Page Up'/`Page Down' to move around

   * `Home'/`End' go to begin/end of line

   * `C-Home'/`C-End' go to begin/end of file

   * Some *Emacs* bindings may also work, including `C-B', `C-P',
     `C-A', `C-E', `C-D', `C-L'

* Menu:

* Automatic indentation::
* Python Shell window::


File: python.info,  Node: Automatic indentation,  Next: Python Shell window,  Up: Basic editing and navigation

5.24.6.9 Automatic indentation
..............................

After a block-opening statement, the next line is indented by 4 spaces
(in the Python Shell window by one tab).  After certain keywords
(break, return etc.)  the next line is dedented.  In leading
indentation, `Backspace' deletes up to 4 spaces if they are there.
`Tab' inserts 1-4 spaces (in the Python Shell window one tab). See also
the indent/dedent region commands in the edit menu.


File: python.info,  Node: Python Shell window,  Prev: Automatic indentation,  Up: Basic editing and navigation

5.24.6.10 Python Shell window
.............................

   * `C-C' interrupts executing command

   * `C-D' sends end-of-file; closes window if typed at a `>>>' prompt

   * `Alt-p' retrieves previous command matching what you have typed

   * `Alt-n' retrieves next

   * `Return' while on any previous command retrieves that command

   * `Alt-/' (Expand word) is also useful here


File: python.info,  Node: Syntax colors,  Next: Startup,  Prev: Basic editing and navigation,  Up: IDLE

5.24.6.11 Syntax colors
.......................

The coloring is applied in a background "thread," so you may
occasionally see uncolorized text.  To change the color scheme, edit
the `[Colors]' section in `config.txt'.

Python syntax colors:

    Keywords
          orange

    Strings
          green

    Comments
          red

    Definitions
          blue

Shell colors:

    Console output
          brown

    stdout
          blue

    stderr
          dark green

    stdin
          black


File: python.info,  Node: Startup,  Prev: Syntax colors,  Up: IDLE

5.24.6.12 Startup
.................

Upon startup with the `-s' option, IDLE will execute the file
referenced by the environment variables `IDLESTARTUP' or *note
PYTHONSTARTUP: 50b.  Idle first checks for `IDLESTARTUP'; if
`IDLESTARTUP' is present the file referenced is run.  If `IDLESTARTUP'
is not present, Idle checks for `PYTHONSTARTUP'.  Files referenced by
these environment variables are convenient places to store functions
that are used frequently from the Idle shell, or for executing import
statements to import common modules.

  In addition, `Tk' also loads a startup file if it is present.  Note
that the Tk file is loaded unconditionally.  This additional file is
`.Idle.py' and is looked for in the user's home directory.  Statements
in this file will be executed in the Tk namespace, so this file is not
useful for importing functions to be used from Idle's Python shell.

* Menu:

* Command line usage::


File: python.info,  Node: Command line usage,  Up: Startup

5.24.6.13 Command line usage
............................

    idle.py [-c command] [-d] [-e] [-s] [-t title] [arg] ...

    -c command  run this command
    -d          enable debugger
    -e          edit mode; arguments are files to be edited
    -s          run $IDLESTARTUP or $PYTHONSTARTUP first
    -t title    set title of shell window

If there are arguments:

  1. If `-e' is used, arguments are files opened for editing and
     `sys.argv' reflects the arguments passed to IDLE itself.

  2. Otherwise, if `-c' is used, all arguments are placed in
     `sys.argv[1:...]', with `sys.argv[0]' set to `'-c''.

  3. Otherwise, if neither `-e' nor `-c' is used, the first argument is
     a script which is executed with the remaining arguments in
     `sys.argv[1:...]'  and `sys.argv[0]' set to the script name.  If
     the script name is '-', no script is executed but an interactive
     Python session is started; the arguments are still available in
     `sys.argv'.


File: python.info,  Node: Other Graphical User Interface Packages,  Prev: IDLE,  Up: Graphical User Interfaces with Tk

5.24.7 Other Graphical User Interface Packages
----------------------------------------------

Major cross-platform (Windows, Mac OS X, Unix-like) GUI toolkits are
available for Python:

See also
........

PyGTK(1)
     is a set of bindings for the GTK(2) widget set. It provides an
     object oriented interface that is slightly higher level than the C
     one. It comes with many more widgets than Tkinter provides, and has
     good Python-specific reference documentation. There are also
     bindings to GNOME(3).  One well known PyGTK application is
     PythonCAD(4). An online tutorial(5) is available.

PyQt(6)
     PyQt is a *sip*-wrapped binding to the Qt toolkit.  Qt is an
     extensive C++ GUI application development framework that is
     available for Unix, Windows and Mac OS X. *sip* is a tool for
     generating bindings for C++ libraries as Python classes, and is
     specifically designed for Python. The _PyQt3_ bindings have a
     book, GUI Programming with Python: QT Edition(7) by Boudewijn
     Rempt. The _PyQt4_ bindings also have a book, Rapid GUI
     Programming with Python and Qt(8), by Mark Summerfield.

wxPython(9)
     wxPython is a cross-platform GUI toolkit for Python that is built
     around the popular wxWidgets(10) (formerly wxWindows) C++ toolkit.
     It provides a native look and feel for applications on Windows,
     Mac OS X, and Unix systems by using each platform's native widgets
     where ever possible, (GTK+ on Unix-like systems).  In addition to
     an extensive set of widgets, wxPython provides classes for online
     documentation and context sensitive help, printing, HTML viewing,
     low-level device context drawing, drag and drop, system clipboard
     access, an XML-based resource format and more, including an ever
     growing library of user-contributed modules.  wxPython has a book,
     wxPython in Action(11), by Noel Rappin and Robin Dunn.

  PyGTK, PyQt, and wxPython, all have a modern look and feel and more
widgets than Tkinter. In addition, there are many other GUI toolkits for
Python, both cross-platform, and platform-specific. See the GUI
Programming(12) page in the Python Wiki for a much more complete list,
and also for links to documents where the different GUI toolkits are
compared.

  ---------- Footnotes ----------

  (1) http://www.pygtk.org/

  (2) http://www.gtk.org/

  (3) http://www.gnome.org

  (4) http://www.pythoncad.org/

  (5) http://www.pygtk.org/pygtk2tutorial/index.html

  (6) http://www.riverbankcomputing.co.uk/software/pyqt/

  (7) http://www.commandprompt.com/community/pyqt/

  (8) http://www.qtrac.eu/pyqtbook.html

  (9) http://www.wxpython.org

  (10) http://www.wxwidgets.org/

  (11) http://www.amazon.com/exec/obidos/ASIN/1932394621

  (12) http://wiki.python.org/moin/GuiProgramming


File: python.info,  Node: Development Tools,  Next: Debugging and Profiling,  Prev: Graphical User Interfaces with Tk,  Up: The Python Standard Library

5.25 Development Tools
======================

The modules described in this chapter help you write software.  For
example, the *note pydoc: 13f. module takes a module and generates
documentation based on the module's contents.  The *note doctest: b5.
and *note unittest: 187. modules contains frameworks for writing unit
tests that automatically exercise code and verify that the expected
output is produced.  *2to3* can translate Python 2.x source code into
valid Python 3.x code.

  The list of modules described in this chapter is:

* Menu:

* pydoc: pydoc --- Documentation generator and online help system. Documentation generator and online help system
* doctest: doctest --- Test interactive Python examples. Test interactive Python examples
* unittest: unittest --- Unit testing framework. Unit testing framework
* 2to3 - Automated Python 2 to 3 code translation::
* test: test --- Regression tests package for Python. Regression tests package for Python
* test.test_support: test test_support --- Utility functions for tests. Utility functions for tests


File: python.info,  Node: pydoc --- Documentation generator and online help system,  Next: doctest --- Test interactive Python examples,  Up: Development Tools

5.25.1 `pydoc' -- Documentation generator and online help system
----------------------------------------------------------------

New in version 2.1.

  *Source code:* Lib/pydoc.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  The *note pydoc: 13f. module automatically generates documentation
from Python modules.  The documentation can be presented as pages of
text on the console, served to a Web browser, or saved to HTML files.

  The built-in function *note help(): 491. invokes the online help
system in the interactive interpreter, which uses *note pydoc: 13f. to
generate its documentation as text on the console.  The same text
documentation can also be viewed from outside the Python interpreter by
running *pydoc* as a script at the operating system's command prompt.
For example, running

    pydoc sys

at a shell prompt will display documentation on the *note sys: 16d.
module, in a style similar to the manual pages shown by the Unix *man*
command.  The argument to *pydoc* can be the name of a function,
module, or package, or a dotted reference to a class, method, or
function within a module or module in a package.  If the argument to
*pydoc* looks like a path (that is, it contains the path separator for
your operating system, such as a slash in Unix), and refers to an
existing Python source file, then documentation is produced for that
file.

     Note: In order to find objects and their documentation, *note
     pydoc: 13f. imports the module(s) to be documented.  Therefore,
     any code on module level will be executed on that occasion.  Use
     an `if __name__ == '__main__':' guard to only execute code when a
     file is invoked as a script and not just imported.

  Specifying a `-w' flag before the argument will cause HTML
documentation to be written out to a file in the current directory,
instead of displaying text on the console.

  Specifying a `-k' flag before the argument will search the synopsis
lines of all available modules for the keyword given as the argument,
again in a manner similar to the Unix *man* command.  The synopsis line
of a module is the first line of its documentation string.

  You can also use *pydoc* to start an HTTP server on the local machine
that will serve documentation to visiting Web browsers. *pydoc -p 1234*
will start a HTTP server on port 1234, allowing you to browse the
documentation at `http://localhost:1234/' in your preferred Web browser.
*pydoc -g* will start the server and additionally bring up a small
*note Tkinter: 17d.-based graphical interface to help you search for
documentation pages.

  When *pydoc* generates documentation, it uses the current environment
and path to locate modules.  Thus, invoking *pydoc spam* documents
precisely the version of the module you would get if you started the
Python interpreter and typed `import spam'.

  Module docs for core modules are assumed to reside in
<http://docs.python.org/library/>.  This can be overridden by setting
the `PYTHONDOCS' environment variable to a different URL or to a local
directory containing the Library Reference Manual pages.

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/pydoc.py


File: python.info,  Node: doctest --- Test interactive Python examples,  Next: unittest --- Unit testing framework,  Prev: pydoc --- Documentation generator and online help system,  Up: Development Tools

5.25.2 `doctest' -- Test interactive Python examples
----------------------------------------------------

The *note doctest: b5. module searches for pieces of text that look
like interactive Python sessions, and then executes those sessions to
verify that they work exactly as shown.  There are several common ways
to use doctest:

   * To check that a module's docstrings are up-to-date by verifying
     that all interactive examples still work as documented.

   * To perform regression testing by verifying that interactive
     examples from a test file or a test object work as expected.

   * To write tutorial documentation for a package, liberally
     illustrated with input-output examples.  Depending on whether the
     examples or the expository text are emphasized, this has the
     flavor of "literate testing" or "executable documentation".

  Here's a complete but small example module:

    """
    This is the "example" module.

    The example module supplies one function, factorial().  For example,

    >>> factorial(5)
    120
    """

    def factorial(n):
        """Return the factorial of n, an exact integer >= 0.

        If the result is small enough to fit in an int, return an int.
        Else return a long.

        >>> [factorial(n) for n in range(6)]
        [1, 1, 2, 6, 24, 120]
        >>> [factorial(long(n)) for n in range(6)]
        [1, 1, 2, 6, 24, 120]
        >>> factorial(30)
        265252859812191058636308480000000L
        >>> factorial(30L)
        265252859812191058636308480000000L
        >>> factorial(-1)
        Traceback (most recent call last):
            ...
        ValueError: n must be >= 0

        Factorials of floats are OK, but the float must be an exact integer:
        >>> factorial(30.1)
        Traceback (most recent call last):
            ...
        ValueError: n must be exact integer
        >>> factorial(30.0)
        265252859812191058636308480000000L

        It must also not be ridiculously large:
        >>> factorial(1e100)
        Traceback (most recent call last):
            ...
        OverflowError: n too large
        """

        import math
        if not n >= 0:
            raise ValueError("n must be >= 0")
        if math.floor(n) != n:
            raise ValueError("n must be exact integer")
        if n+1 == n:  # catch a value like 1e300
            raise OverflowError("n too large")
        result = 1
        factor = 2
        while factor <= n:
            result *= factor
            factor += 1
        return result


    if __name__ == "__main__":
        import doctest
        doctest.testmod()

If you run `example.py' directly from the command line, *note doctest:
b5.  works its magic:

    $ python example.py
    $

There's no output!  That's normal, and it means all the examples
worked.  Pass `-v' to the script, and *note doctest: b5. prints a
detailed log of what it's trying, and prints a summary at the end:

    $ python example.py -v
    Trying:
        factorial(5)
    Expecting:
        120
    ok
    Trying:
        [factorial(n) for n in range(6)]
    Expecting:
        [1, 1, 2, 6, 24, 120]
    ok
    Trying:
        [factorial(long(n)) for n in range(6)]
    Expecting:
        [1, 1, 2, 6, 24, 120]
    ok

And so on, eventually ending with:

    Trying:
        factorial(1e100)
    Expecting:
        Traceback (most recent call last):
            ...
        OverflowError: n too large
    ok
    2 items passed all tests:
       1 tests in __main__
       8 tests in __main__.factorial
    9 tests in 2 items.
    9 passed and 0 failed.
    Test passed.
    $

That's all you need to know to start making productive use of *note
doctest: b5.!  Jump in.  The following sections provide full details.
Note that there are many examples of doctests in the standard Python
test suite and libraries.  Especially useful examples can be found in
the standard test file `Lib/test/test_doctest.py'.

* Menu:

* Simple Usage; Checking Examples in Docstrings: Simple Usage Checking Examples in Docstrings.
* Simple Usage; Checking Examples in a Text File: Simple Usage Checking Examples in a Text File.
* How It Works::
* Basic API::
* Unittest API::
* Advanced API::
* Debugging::
* Soapbox::


File: python.info,  Node: Simple Usage Checking Examples in Docstrings,  Next: Simple Usage Checking Examples in a Text File,  Up: doctest --- Test interactive Python examples

5.25.2.1 Simple Usage: Checking Examples in Docstrings
......................................................

The simplest way to start using doctest (but not necessarily the way
you'll continue to do it) is to end each module `M' with:

    if __name__ == "__main__":
        import doctest
        doctest.testmod()

*note doctest: b5. then examines docstrings in module `M'.

  Running the module as a script causes the examples in the docstrings
to get executed and verified:

    python M.py

This won't display anything unless an example fails, in which case the
failing example(s) and the cause(s) of the failure(s) are printed to
stdout, and the final line of output is `***Test Failed*** N
failures.', where _N_ is the number of examples that failed.

  Run it with the `-v' switch instead:

    python M.py -v

and a detailed report of all examples tried is printed to standard
output, along with assorted summaries at the end.

  You can force verbose mode by passing `verbose=True' to *note
testmod(): 40a, or prohibit it by passing `verbose=False'.  In either
of those cases, `sys.argv' is not examined by *note testmod(): 40a. (so
passing `-v' or not has no effect).

  Since Python 2.6, there is also a command line shortcut for running
*note testmod(): 40a.  You can instruct the Python interpreter to run
the doctest module directly from the standard library and pass the
module name(s) on the command line:

    python -m doctest -v example.py

This will import `example.py' as a standalone module and run *note
testmod(): 40a. on it.  Note that this may not work correctly if the
file is part of a package and imports other submodules from that
package.

  For more information on *note testmod(): 40a, see section *note Basic
API: 21be.


File: python.info,  Node: Simple Usage Checking Examples in a Text File,  Next: How It Works,  Prev: Simple Usage Checking Examples in Docstrings,  Up: doctest --- Test interactive Python examples

5.25.2.2 Simple Usage: Checking Examples in a Text File
.......................................................

Another simple application of doctest is testing interactive examples
in a text file.  This can be done with the *note testfile(): 21c1.
function:

    import doctest
    doctest.testfile("example.txt")

That short script executes and verifies any interactive Python examples
contained in the file `example.txt'.  The file content is treated as if
it were a single giant docstring; the file doesn't need to contain a
Python program!   For example, perhaps `example.txt' contains this:

    The ``example`` module
    ======================

    Using ``factorial``
    -------------------

    This is an example text file in reStructuredText format.  First import
    ``factorial`` from the ``example`` module:

        >>> from example import factorial

    Now use it:

        >>> factorial(6)
        120

Running `doctest.testfile("example.txt")' then finds the error in this
documentation:

    File "./example.txt", line 14, in example.txt
    Failed example:
        factorial(6)
    Expected:
        120
    Got:
        720

As with *note testmod(): 40a, *note testfile(): 21c1. won't display
anything unless an example fails.  If an example does fail, then the
failing example(s) and the cause(s) of the failure(s) are printed to
stdout, using the same format as *note testmod(): 40a.

  By default, *note testfile(): 21c1. looks for files in the calling
module's directory.  See section *note Basic API: 21be. for a
description of the optional arguments that can be used to tell it to
look for files in other locations.

  Like *note testmod(): 40a, *note testfile(): 21c1.'s verbosity can be
set with the `-v' command-line switch or with the optional keyword
argument _verbose_.

  Since Python 2.6, there is also a command line shortcut for running
*note testfile(): 21c1.  You can instruct the Python interpreter to run
the doctest module directly from the standard library and pass the file
name(s) on the command line:

    python -m doctest -v example.txt

Because the file name does not end with `.py', *note doctest: b5.
infers that it must be run with *note testfile(): 21c1, not *note
testmod(): 40a.

  For more information on *note testfile(): 21c1, see section *note
Basic API: 21be.


File: python.info,  Node: How It Works,  Next: Basic API,  Prev: Simple Usage Checking Examples in a Text File,  Up: doctest --- Test interactive Python examples

5.25.2.3 How It Works
.....................

This section examines in detail how doctest works: which docstrings it
looks at, how it finds interactive examples, what execution context it
uses, how it handles exceptions, and how option flags can be used to
control its behavior.  This is the information that you need to know to
write doctest examples; for information about actually running doctest
on these examples, see the following sections.

* Menu:

* Which Docstrings Are Examined?::
* How are Docstring Examples Recognized?::
* What's the Execution Context?::
* What About Exceptions?::
* Option Flags::
* Directives::
* Warnings::


File: python.info,  Node: Which Docstrings Are Examined?,  Next: How are Docstring Examples Recognized?,  Up: How It Works

5.25.2.4 Which Docstrings Are Examined?
.......................................

The module docstring, and all function, class and method docstrings are
searched.  Objects imported into the module are not searched.

  In addition, if `M.__test__' exists and "is true", it must be a dict,
and each entry maps a (string) name to a function object, class object,
or string.  Function and class object docstrings found from
`M.__test__' are searched, and strings are treated as if they were
docstrings.  In output, a key `K' in `M.__test__' appears with name

    <name of M>.__test__.K

Any classes found are recursively searched similarly, to test
docstrings in their contained methods and nested classes.

  Changed in version 2.4: A "private name" concept is deprecated and no
longer documented.


File: python.info,  Node: How are Docstring Examples Recognized?,  Next: What's the Execution Context?,  Prev: Which Docstrings Are Examined?,  Up: How It Works

5.25.2.5 How are Docstring Examples Recognized?
...............................................

In most cases a copy-and-paste of an interactive console session works
fine, but doctest isn't trying to do an exact emulation of any specific
Python shell.

    >>> # comments are ignored
    >>> x = 12
    >>> x
    12
    >>> if x == 13:
    ...     print "yes"
    ... else:
    ...     print "no"
    ...     print "NO"
    ...     print "NO!!!"
    ...
    no
    NO
    NO!!!
    >>>

Any expected output must immediately follow the final `'>>> '' or `'...
'' line containing the code, and the expected output (if any) extends
to the next `'>>> '' or all-whitespace line.

  The fine print:

   * Expected output cannot contain an all-whitespace line, since such
     a line is taken to signal the end of expected output.  If expected
     output does contain a blank line, put `<BLANKLINE>' in your
     doctest example each place a blank line is expected.

     New in version 2.4: `<BLANKLINE>' was added; there was no way to
     use expected output containing empty lines in previous versions.

   * All hard tab characters are expanded to spaces, using 8-column tab
     stops.  Tabs in output generated by the tested code are not
     modified.  Because any hard tabs in the sample output _are_
     expanded, this means that if the code output includes hard tabs,
     the only way the doctest can pass is if the *note
     NORMALIZE_WHITESPACE: 21c8. option or *note directive: 21c9.  is
     in effect.  Alternatively, the test can be rewritten to capture
     the output and compare it to an expected value as part of the
     test.  This handling of tabs in the source was arrived at through
     trial and error, and has proven to be the least error prone way of
     handling them.  It is possible to use a different algorithm for
     handling tabs by writing a custom *note DocTestParser: 21ca. class.

     Changed in version 2.4: Expanding tabs to spaces is new; previous
     versions tried to preserve hard tabs, with confusing results.

   * Output to stdout is captured, but not output to stderr (exception
     tracebacks are captured via a different means).

   * If you continue a line via backslashing in an interactive session,
     or for any other reason use a backslash, you should use a raw
     docstring, which will preserve your backslashes exactly as you
     type them:

         >>> def f(x):
         ...     r'''Backslashes in a raw docstring: m\n'''
         >>> print f.__doc__
         Backslashes in a raw docstring: m\n

     Otherwise, the backslash will be interpreted as part of the
     string. For example, the `\n' above would be interpreted as a
     newline character.  Alternatively, you can double each backslash
     in the doctest version (and not use a raw string):

         >>> def f(x):
         ...     '''Backslashes in a raw docstring: m\\n'''
         >>> print f.__doc__
         Backslashes in a raw docstring: m\n


   * The starting column doesn't matter:

         >>> assert "Easy!"
               >>> import math
                   >>> math.floor(1.9)
                   1.0

     and as many leading whitespace characters are stripped from the
     expected output as appeared in the initial `'>>> '' line that
     started the example.


File: python.info,  Node: What's the Execution Context?,  Next: What About Exceptions?,  Prev: How are Docstring Examples Recognized?,  Up: How It Works

5.25.2.6 What's the Execution Context?
......................................

By default, each time *note doctest: b5. finds a docstring to test, it
uses a _shallow copy_ of `M''s globals, so that running tests doesn't
change the module's real globals, and so that one test in `M' can't
leave behind crumbs that accidentally allow another test to work.  This
means examples can freely use any names defined at top-level in `M',
and names defined earlier in the docstring being run. Examples cannot
see names defined in other docstrings.

  You can force use of your own dict as the execution context by passing
`globs=your_dict' to *note testmod(): 40a. or *note testfile(): 21c1.
instead.


File: python.info,  Node: What About Exceptions?,  Next: Option Flags,  Prev: What's the Execution Context?,  Up: How It Works

5.25.2.7 What About Exceptions?
...............................

No problem, provided that the traceback is the only output produced by
the example:  just paste in the traceback. (1) Since tracebacks contain
details that are likely to change rapidly (for example, exact file
paths and line numbers), this is one case where doctest works hard to
be flexible in what it accepts.

  Simple example:

    >>> [1, 2, 3].remove(42)
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    ValueError: list.remove(x): x not in list

That doctest succeeds if *note ValueError: 233. is raised, with the
`list.remove(x): x not in list' detail as shown.

  The expected output for an exception must start with a traceback
header, which may be either of the following two lines, indented the
same as the first line of the example:

    Traceback (most recent call last):
    Traceback (innermost last):

The traceback header is followed by an optional traceback stack, whose
contents are ignored by doctest.  The traceback stack is typically
omitted, or copied verbatim from an interactive session.

  The traceback stack is followed by the most interesting part: the
line(s) containing the exception type and detail.  This is usually the
last line of a traceback, but can extend across multiple lines if the
exception has a multi-line detail:

    >>> raise ValueError('multi\n    line\ndetail')
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    ValueError: multi
        line
    detail

The last three lines (starting with *note ValueError: 233.) are
compared against the exception's type and detail, and the rest are
ignored.

  Changed in version 2.4: Previous versions were unable to handle
multi-line exception details.

  Best practice is to omit the traceback stack, unless it adds
significant documentation value to the example.  So the last example is
probably better as:

    >>> raise ValueError('multi\n    line\ndetail')
    Traceback (most recent call last):
        ...
    ValueError: multi
        line
    detail

Note that tracebacks are treated very specially.  In particular, in the
rewritten example, the use of `...' is independent of doctest's *note
ELLIPSIS: 40b. option.  The ellipsis in that example could be left out,
or could just as well be three (or three hundred) commas or digits, or
an indented transcript of a Monty Python skit.

  Some details you should read once, but won't need to remember:

   * Doctest can't guess whether your expected output came from an
     exception traceback or from ordinary printing.  So, e.g., an
     example that expects `ValueError: 42 is prime' will pass whether
     *note ValueError: 233. is actually raised or if the example merely
     prints that traceback text.  In practice, ordinary output rarely
     begins with a traceback header line, so this doesn't create real
     problems.

   * Each line of the traceback stack (if present) must be indented
     further than the first line of the example, _or_ start with a
     non-alphanumeric character.  The first line following the
     traceback header indented the same and starting with an
     alphanumeric is taken to be the start of the exception detail.  Of
     course this does the right thing for genuine tracebacks.

   * When the *note IGNORE_EXCEPTION_DETAIL: 21cf. doctest option is
     specified, everything following the leftmost colon and any module
     information in the exception name is ignored.

   * The interactive shell omits the traceback header line for some
     *note SyntaxError: 48f.s.  But doctest uses the traceback header
     line to distinguish exceptions from non-exceptions.  So in the
     rare case where you need to test a *note SyntaxError: 48f. that
     omits the traceback header, you will need to manually add the
     traceback header line to your test example.

   * For some *note SyntaxError: 48f.s, Python displays the character
     position of the syntax error, using a `^' marker:

         >>> 1 1
           File "<stdin>", line 1
             1 1
               ^
         SyntaxError: invalid syntax

     Since the lines showing the position of the error come before the
     exception type and detail, they are not checked by doctest.  For
     example, the following test would pass, even though it puts the
     `^' marker in the wrong location:

         >>> 1 1
         Traceback (most recent call last):
           File "<stdin>", line 1
             1 1
             ^
         SyntaxError: invalid syntax



  ---------- Footnotes ----------

  (1) Examples containing both expected output and an exception are not
supported.  Trying to guess where one ends and the other begins is too
error-prone, and that also makes for a confusing test.


File: python.info,  Node: Option Flags,  Next: Directives,  Prev: What About Exceptions?,  Up: How It Works

5.25.2.8 Option Flags
.....................

A number of option flags control various aspects of doctest's behavior.
Symbolic names for the flags are supplied as module constants, which
can be or'ed together and passed to various functions.  The names can
also be used in *note doctest directives: 21c9.

  The first group of options define test semantics, controlling aspects
of how doctest decides whether actual output matches an example's
expected output:

 -- Data: doctest.DONT_ACCEPT_TRUE_FOR_1
     By default, if an expected output block contains just `1', an
     actual output block containing just `1' or just `True' is
     considered to be a match, and similarly for `0' versus `False'.
     When *note DONT_ACCEPT_TRUE_FOR_1: 21d3. is specified, neither
     substitution is allowed.  The default behavior caters to that
     Python changed the return type of many functions from integer to
     boolean; doctests expecting "little integer" output still work in
     these cases.  This option will probably go away, but not for
     several years.

 -- Data: doctest.DONT_ACCEPT_BLANKLINE
     By default, if an expected output block contains a line containing
     only the string `<BLANKLINE>', then that line will match a blank
     line in the actual output.  Because a genuinely blank line
     delimits the expected output, this is the only way to communicate
     that a blank line is expected.  When *note DONT_ACCEPT_BLANKLINE:
     21d4. is specified, this substitution is not allowed.

 -- Data: doctest.NORMALIZE_WHITESPACE
     When specified, all sequences of whitespace (blanks and newlines)
     are treated as equal.  Any sequence of whitespace within the
     expected output will match any sequence of whitespace within the
     actual output. By default, whitespace must match exactly. *note
     NORMALIZE_WHITESPACE: 21c8. is especially useful when a line of
     expected output is very long, and you want to wrap it across
     multiple lines in your source.

 -- Data: doctest.ELLIPSIS
     When specified, an ellipsis marker (`...') in the expected output
     can match any substring in the actual output.  This includes
     substrings that span line boundaries, and empty substrings, so
     it's best to keep usage of this simple.  Complicated uses can lead
     to the same kinds of "oops, it matched too much!"  surprises that
     `.*' is prone to in regular expressions.

 -- Data: doctest.IGNORE_EXCEPTION_DETAIL
     When specified, an example that expects an exception passes if an
     exception of the expected type is raised, even if the exception
     detail does not match.  For example, an example expecting
     `ValueError: 42' will pass if the actual exception raised is
     `ValueError: 3*14', but will fail, e.g., if *note TypeError: 215.
     is raised.

     It will also ignore the module name used in Python 3 doctest
     reports. Hence both of these variations will work with the flag
     specified, regardless of whether the test is run under Python 2.7
     or Python 3.2 (or later versions):

         >>> raise CustomError('message')
         Traceback (most recent call last):
         CustomError: message

         >>> raise CustomError('message')
         Traceback (most recent call last):
         my_module.CustomError: message

     Note that *note ELLIPSIS: 40b. can also be used to ignore the
     details of the exception message, but such a test may still fail
     based on whether or not the module details are printed as part of
     the exception name. Using *note IGNORE_EXCEPTION_DETAIL: 21cf. and
     the details from Python 2.3 is also the only clear way to write a
     doctest that doesn't care about the exception detail yet continues
     to pass under Python 2.3 or earlier (those releases do not support
     *note doctest directives: 21c9. and ignore them as irrelevant
     comments). For example:

         >>> (1, 2)[3] = 'moo'
         Traceback (most recent call last):
           File "<stdin>", line 1, in ?
         TypeError: object doesn't support item assignment

     passes under Python 2.3 and later Python versions with the flag
     specified, even though the detail changed in Python 2.4 to say
     "does not" instead of "doesn't".

     Changed in version 2.7: *note IGNORE_EXCEPTION_DETAIL: 21cf. now
     also ignores any information relating to the module containing the
     exception under test

 -- Data: doctest.SKIP
     When specified, do not run the example at all.  This can be useful
     in contexts where doctest examples serve as both documentation and
     test cases, and an example should be included for documentation
     purposes, but should not be checked.  E.g., the example's output
     might be random; or the example might depend on resources which
     would be unavailable to the test driver.

     The SKIP flag can also be used for temporarily "commenting out"
     examples.

  New in version 2.5.

 -- Data: doctest.COMPARISON_FLAGS
     A bitmask or'ing together all the comparison flags above.

  The second group of options controls how test failures are reported:

 -- Data: doctest.REPORT_UDIFF
     When specified, failures that involve multi-line expected and
     actual outputs are displayed using a unified diff.

 -- Data: doctest.REPORT_CDIFF
     When specified, failures that involve multi-line expected and
     actual outputs will be displayed using a context diff.

 -- Data: doctest.REPORT_NDIFF
     When specified, differences are computed by `difflib.Differ',
     using the same algorithm as the popular `ndiff.py' utility. This
     is the only method that marks differences within lines as well as
     across lines.  For example, if a line of expected output contains
     digit `1' where actual output contains letter `l', a line is
     inserted with a caret marking the mismatching column positions.

 -- Data: doctest.REPORT_ONLY_FIRST_FAILURE
     When specified, display the first failing example in each doctest,
     but suppress output for all remaining examples.  This will prevent
     doctest from reporting correct examples that break because of
     earlier failures; but it might also hide incorrect examples that
     fail independently of the first failure.  When *note
     REPORT_ONLY_FIRST_FAILURE: 21d7. is specified, the remaining
     examples are still run, and still count towards the total number
     of failures reported; only the output is suppressed.

 -- Data: doctest.REPORTING_FLAGS
     A bitmask or'ing together all the reporting flags above.

  New in version 2.4: The constants *note DONT_ACCEPT_BLANKLINE: 21d4,
*note NORMALIZE_WHITESPACE: 21c8, *note ELLIPSIS: 40b, *note
IGNORE_EXCEPTION_DETAIL: 21cf, *note REPORT_UDIFF: 40c, *note
REPORT_CDIFF: 40d, *note REPORT_NDIFF: 40e, *note
REPORT_ONLY_FIRST_FAILURE: 21d7, *note COMPARISON_FLAGS: 21d6. and
*note REPORTING_FLAGS: 21d8. were added.

  There's also a way to register new option flag names, although this
isn't useful unless you intend to extend *note doctest: b5. internals
via subclassing:

 -- Function: doctest.register_optionflag (name)
     Create a new option flag with a given name, and return the new
     flag's integer value.  *note register_optionflag(): 21d9. can be
     used when subclassing *note OutputChecker: 21da. or *note
     DocTestRunner: 21db. to create new options that are supported by
     your subclasses.  *note register_optionflag(): 21d9. should always
     be called using the following idiom:

         MY_FLAG = register_optionflag('MY_FLAG')

     New in version 2.4.


File: python.info,  Node: Directives,  Next: Warnings,  Prev: Option Flags,  Up: How It Works

5.25.2.9 Directives
...................

Doctest directives may be used to modify the *note option flags: 21d2.
for an individual example.  Doctest directives are special Python
comments following an example's source code:

    directive             ::= "#" "doctest:" directive_options
    directive_options     ::= directive_option ("," directive_option)\*
    directive_option      ::= on_or_off directive_option_name
    on_or_off             ::= "+" \| "-"
    directive_option_name ::= "DONT_ACCEPT_BLANKLINE" \| "NORMALIZE_WHITESPACE" \| ...

Whitespace is not allowed between the `+' or `-' and the directive
option name.  The directive option name can be any of the option flag
names explained above.

  An example's doctest directives modify doctest's behavior for that
single example.  Use `+' to enable the named behavior, or `-' to
disable it.

  For example, this test passes:

    >>> print range(20) # doctest: +NORMALIZE_WHITESPACE
    [0,   1,  2,  3,  4,  5,  6,  7,  8,  9,
    10,  11, 12, 13, 14, 15, 16, 17, 18, 19]

Without the directive it would fail, both because the actual output
doesn't have two blanks before the single-digit list elements, and
because the actual output is on a single line.  This test also passes,
and also requires a directive to do so:

    >>> print range(20) # doctest: +ELLIPSIS
    [0, 1, ..., 18, 19]

Multiple directives can be used on a single physical line, separated by
commas:

    >>> print range(20) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
    [0,    1, ...,   18,    19]

If multiple directive comments are used for a single example, then they
are combined:

    >>> print range(20) # doctest: +ELLIPSIS
    ...                 # doctest: +NORMALIZE_WHITESPACE
    [0,    1, ...,   18,    19]

As the previous example shows, you can add `...' lines to your example
containing only directives.  This can be useful when an example is too
long for a directive to comfortably fit on the same line:

    >>> print range(5) + range(10,20) + range(30,40) + range(50,60)
    ... # doctest: +ELLIPSIS
    [0, ..., 4, 10, ..., 19, 30, ..., 39, 50, ..., 59]

Note that since all options are disabled by default, and directives
apply only to the example they appear in, enabling options (via `+' in
a directive) is usually the only meaningful choice.  However, option
flags can also be passed to functions that run doctests, establishing
different defaults.  In such cases, disabling an option via `-' in a
directive can be useful.

  New in version 2.4: Support for doctest directives was added.


File: python.info,  Node: Warnings,  Prev: Directives,  Up: How It Works

5.25.2.10 Warnings
..................

*note doctest: b5. is serious about requiring exact matches in expected
output.  If even a single character doesn't match, the test fails.
This will probably surprise you a few times, as you learn exactly what
Python does and doesn't guarantee about output.  For example, when
printing a dict, Python doesn't guarantee that the key-value pairs will
be printed in any particular order, so a test like

    >>> foo()
    {"Hermione": "hippogryph", "Harry": "broomstick"}

is vulnerable!  One workaround is to do

    >>> foo() == {"Hermione": "hippogryph", "Harry": "broomstick"}
    True

instead.  Another is to do

    >>> d = foo().items()
    >>> d.sort()
    >>> d
    [('Harry', 'broomstick'), ('Hermione', 'hippogryph')]

There are others, but you get the idea.

  Another bad idea is to print things that embed an object address, like

    >>> id(1.0) # certain to fail some of the time
    7948648
    >>> class C: pass
    >>> C()   # the default repr() for instances embeds an address
    <__main__.C instance at 0x00AC18F0>

The *note ELLIPSIS: 40b. directive gives a nice approach for the last
example:

    >>> C() #doctest: +ELLIPSIS
    <__main__.C instance at 0x...>

Floating-point numbers are also subject to small output variations
across platforms, because Python defers to the platform C library for
float formatting, and C libraries vary widely in quality here.

    >>> 1./7  # risky
    0.14285714285714285
    >>> print 1./7 # safer
    0.142857142857
    >>> print round(1./7, 6) # much safer
    0.142857

Numbers of the form `I/2.**J' are safe across all platforms, and I often
contrive doctest examples to produce numbers of that form:

    >>> 3./4  # utterly safe
    0.75

Simple fractions are also easier for people to understand, and that
makes for better documentation.


File: python.info,  Node: Basic API,  Next: Unittest API,  Prev: How It Works,  Up: doctest --- Test interactive Python examples

5.25.2.11 Basic API
...................

The functions *note testmod(): 40a. and *note testfile(): 21c1. provide
a simple interface to doctest that should be sufficient for most basic
uses.  For a less formal introduction to these two functions, see
sections *note Simple Usage; Checking Examples in Docstrings: 21bd.
and *note Simple Usage; Checking Examples in a Text File: 21c0.

 -- Function: doctest.testfile (filename[, module_relative][, name][,
          package][, globs][, verbose][, report][, optionflags][,
          extraglobs][, raise_on_error][, parser][, encoding])
     All arguments except _filename_ are optional, and should be
     specified in keyword form.

     Test examples in the file named _filename_.  Return
     `(failure_count, test_count)'.

     Optional argument _module_relative_ specifies how the filename
     should be interpreted:

        * If _module_relative_ is `True' (the default), then _filename_
          specifies an OS-independent module-relative path.  By
          default, this path is relative to the calling module's
          directory; but if the _package_ argument is specified, then it
          is relative to that package.  To ensure OS-independence,
          _filename_ should use `/' characters to separate path
          segments, and may not be an absolute path (i.e., it may not
          begin with `/').

        * If _module_relative_ is `False', then _filename_ specifies an
          OS-specific path.  The path may be absolute or relative;
          relative paths are resolved with respect to the current
          working directory.

     Optional argument _name_ gives the name of the test; by default,
     or if `None', `os.path.basename(filename)' is used.

     Optional argument _package_ is a Python package or the name of a
     Python package whose directory should be used as the base
     directory for a module-relative filename.  If no package is
     specified, then the calling module's directory is used as the base
     directory for module-relative filenames.  It is an error to
     specify _package_ if _module_relative_ is `False'.

     Optional argument _globs_ gives a dict to be used as the globals
     when executing examples.  A new shallow copy of this dict is
     created for the doctest, so its examples start with a clean slate.
     By default, or if `None', a new empty dict is used.

     Optional argument _extraglobs_ gives a dict merged into the
     globals used to execute examples.  This works like *note
     dict.update(): 3fb.:  if _globs_ and _extraglobs_ have a common
     key, the associated value in _extraglobs_ appears in the combined
     dict.  By default, or if `None', no extra globals are used.  This
     is an advanced feature that allows parameterization of doctests.
     For example, a doctest can be written for a base class, using a
     generic name for the class, then reused to test any number of
     subclasses by passing an _extraglobs_ dict mapping the generic
     name to the subclass to be tested.

     Optional argument _verbose_ prints lots of stuff if true, and
     prints only failures if false; by default, or if `None', it's true
     if and only if `'-v'' is in `sys.argv'.

     Optional argument _report_ prints a summary at the end when true,
     else prints nothing at the end.  In verbose mode, the summary is
     detailed, else the summary is very brief (in fact, empty if all
     tests passed).

     Optional argument _optionflags_ or's together option flags.  See
     section *note Option Flags: 21d2.

     Optional argument _raise_on_error_ defaults to false.  If true, an
     exception is raised upon the first failure or unexpected exception
     in an example.  This allows failures to be post-mortem debugged.
     Default behavior is to continue running examples.

     Optional argument _parser_ specifies a *note DocTestParser: 21ca.
     (or subclass) that should be used to extract tests from the files.
     It defaults to a normal parser (i.e., `DocTestParser()').

     Optional argument _encoding_ specifies an encoding that should be
     used to convert the file to unicode.

     New in version 2.4.

     Changed in version 2.5: The parameter _encoding_ was added.

 -- Function: doctest.testmod ([m][, name][, globs][, verbose][,
          report][, optionflags][, extraglobs][, raise_on_error][,
          exclude_empty])
     All arguments are optional, and all except for _m_ should be
     specified in keyword form.

     Test examples in docstrings in functions and classes reachable
     from module _m_ (or module *note __main__: 2. if _m_ is not
     supplied or is `None'), starting with `m.__doc__'.

     Also test examples reachable from dict `m.__test__', if it exists
     and is not `None'.  `m.__test__' maps names (strings) to
     functions, classes and strings; function and class docstrings are
     searched for examples; strings are searched directly, as if they
     were docstrings.

     Only docstrings attached to objects belonging to module _m_ are
     searched.

     Return `(failure_count, test_count)'.

     Optional argument _name_ gives the name of the module; by default,
     or if `None', `m.__name__' is used.

     Optional argument _exclude_empty_ defaults to false.  If true,
     objects for which no doctests are found are excluded from
     consideration. The default is a backward compatibility hack, so
     that code still using `doctest.master.summarize()' in conjunction
     with *note testmod(): 40a. continues to get output for objects
     with no tests. The _exclude_empty_ argument to the newer *note
     DocTestFinder: 21e5.  constructor defaults to true.

     Optional arguments _extraglobs_, _verbose_, _report_,
     _optionflags_, _raise_on_error_, and _globs_ are the same as for
     function *note testfile(): 21c1.  above, except that _globs_
     defaults to `m.__dict__'.

     Changed in version 2.3: The parameter _optionflags_ was added.

     Changed in version 2.4: The parameters _extraglobs_,
     _raise_on_error_ and _exclude_empty_ were added.

     Changed in version 2.5: The optional argument _isprivate_,
     deprecated in 2.4, was removed.

  There's also a function to run the doctests associated with a single
object.  This function is provided for backward compatibility.  There
are no plans to deprecate it, but it's rarely useful:

 -- Function: doctest.run_docstring_examples (f, globs[, verbose][,
          name][, compileflags][, optionflags])
     Test examples associated with object _f_; for example, _f_ may be
     a module, function, or class object.

     A shallow copy of dictionary argument _globs_ is used for the
     execution context.

     Optional argument _name_ is used in failure messages, and defaults
     to `"NoName"'.

     If optional argument _verbose_ is true, output is generated even
     if there are no failures.  By default, output is generated only in
     case of an example failure.

     Optional argument _compileflags_ gives the set of flags that
     should be used by the Python compiler when running the examples.
     By default, or if `None', flags are deduced corresponding to the
     set of future features found in _globs_.

     Optional argument _optionflags_ works as for function *note
     testfile(): 21c1. above.


File: python.info,  Node: Unittest API,  Next: Advanced API,  Prev: Basic API,  Up: doctest --- Test interactive Python examples

5.25.2.12 Unittest API
......................

As your collection of doctest'ed modules grows, you'll want a way to
run all their doctests systematically.  Prior to Python 2.4, *note
doctest: b5. had a barely documented `Tester' class that supplied a
rudimentary way to combine doctests from multiple modules. `Tester' was
feeble, and in practice most serious Python testing frameworks build on
the *note unittest: 187. module, which supplies many flexible ways to
combine tests from multiple sources.  So, in Python 2.4, *note doctest:
b5.'s `Tester' class is deprecated, and *note doctest: b5. provides two
functions that can be used to create *note unittest: 187.  test suites
from modules and text files containing doctests.  To integrate with
*note unittest: 187. test discovery, include a `load_tests()' function
in your test module:

    import unittest
    import doctest
    import my_module_with_doctests

    def load_tests(loader, tests, ignore):
        tests.addTests(doctest.DocTestSuite(my_module_with_doctests))
        return tests

There are two main functions for creating *note unittest.TestSuite:
456. instances from text files and modules with doctests:

 -- Function: doctest.DocFileSuite (*paths, [module_relative][,
          package][, setUp][, tearDown][, globs][, optionflags][,
          parser][, encoding])
     Convert doctest tests from one or more text files to a *note
     unittest.TestSuite: 456.

     The returned *note unittest.TestSuite: 456. is to be run by the
     unittest framework and runs the interactive examples in each file.
     If an example in any file fails, then the synthesized unit test
     fails, and a `failureException' exception is raised showing the
     name of the file containing the test and a (sometimes approximate)
     line number.

     Pass one or more paths (as strings) to text files to be examined.

     Options may be provided as keyword arguments:

     Optional argument _module_relative_ specifies how the filenames in
     _paths_ should be interpreted:

        * If _module_relative_ is `True' (the default), then each
          filename in _paths_ specifies an OS-independent
          module-relative path.  By default, this path is relative to
          the calling module's directory; but if the _package_ argument
          is specified, then it is relative to that package.  To ensure
          OS-independence, each filename should use `/' characters to
          separate path segments, and may not be an absolute path
          (i.e., it may not begin with `/').

        * If _module_relative_ is `False', then each filename in
          _paths_ specifies an OS-specific path.  The path may be
          absolute or relative; relative paths are resolved with
          respect to the current working directory.

     Optional argument _package_ is a Python package or the name of a
     Python package whose directory should be used as the base
     directory for module-relative filenames in _paths_.  If no package
     is specified, then the calling module's directory is used as the
     base directory for module-relative filenames.  It is an error to
     specify _package_ if _module_relative_ is `False'.

     Optional argument _setUp_ specifies a set-up function for the test
     suite.  This is called before running the tests in each file.  The
     _setUp_ function will be passed a *note DocTest: 21ea. object.
     The setUp function can access the test globals as the _globs_
     attribute of the test passed.

     Optional argument _tearDown_ specifies a tear-down function for
     the test suite.  This is called after running the tests in each
     file.  The _tearDown_ function will be passed a *note DocTest:
     21ea. object.  The setUp function can access the test globals as
     the _globs_ attribute of the test passed.

     Optional argument _globs_ is a dictionary containing the initial
     global variables for the tests.  A new copy of this dictionary is
     created for each test.  By default, _globs_ is a new empty
     dictionary.

     Optional argument _optionflags_ specifies the default doctest
     options for the tests, created by or-ing together individual
     option flags.  See section *note Option Flags: 21d2. See function
     *note set_unittest_reportflags(): 21eb. below for a better way to
     set reporting options.

     Optional argument _parser_ specifies a *note DocTestParser: 21ca.
     (or subclass) that should be used to extract tests from the files.
     It defaults to a normal parser (i.e., `DocTestParser()').

     Optional argument _encoding_ specifies an encoding that should be
     used to convert the file to unicode.

     New in version 2.4.

     Changed in version 2.5: The global `__file__' was added to the
     globals provided to doctests loaded from a text file using *note
     DocFileSuite(): 21e9.

     Changed in version 2.5: The parameter _encoding_ was added.

          Note: Unlike *note testmod(): 40a. and *note DocTestFinder:
          21e5, this function raises a *note ValueError: 233. if
          _module_ contains no docstrings.  You can prevent this error
          by passing a *note DocTestFinder: 21e5. instance as the
          _test_finder_ argument with its _exclude_empty_ keyword
          argument set to `False':

              >>> finder = doctest.DocTestFinder(exclude_empty=False)
              >>> suite = doctest.DocTestSuite(test_finder=finder)



 -- Function: doctest.DocTestSuite ([module][, globs][, extraglobs][,
          test_finder][, setUp][, tearDown][, checker])
     Convert doctest tests for a module to a *note unittest.TestSuite:
     456.

     The returned *note unittest.TestSuite: 456. is to be run by the
     unittest framework and runs each doctest in the module.  If any of
     the doctests fail, then the synthesized unit test fails, and a
     `failureException' exception is raised showing the name of the
     file containing the test and a (sometimes approximate) line number.

     Optional argument _module_ provides the module to be tested.  It
     can be a module object or a (possibly dotted) module name.  If not
     specified, the module calling this function is used.

     Optional argument _globs_ is a dictionary containing the initial
     global variables for the tests.  A new copy of this dictionary is
     created for each test.  By default, _globs_ is a new empty
     dictionary.

     Optional argument _extraglobs_ specifies an extra set of global
     variables, which is merged into _globs_.  By default, no extra
     globals are used.

     Optional argument _test_finder_ is the *note DocTestFinder: 21e5.
     object (or a drop-in replacement) that is used to extract doctests
     from the module.

     Optional arguments _setUp_, _tearDown_, and _optionflags_ are the
     same as for function *note DocFileSuite(): 21e9. above.

     New in version 2.3.

     Changed in version 2.4: The parameters _globs_, _extraglobs_,
     _test_finder_, _setUp_, _tearDown_, and _optionflags_ were added;
     this function now uses the same search technique as *note
     testmod(): 40a.

  Under the covers, *note DocTestSuite(): 21ec. creates a *note
unittest.TestSuite: 456. out of `doctest.DocTestCase' instances, and
`DocTestCase' is a subclass of *note unittest.TestCase: 27f.
`DocTestCase' isn't documented here (it's an internal detail), but
studying its code can answer questions about the exact details of *note
unittest: 187. integration.

  Similarly, *note DocFileSuite(): 21e9. creates a *note
unittest.TestSuite: 456. out of `doctest.DocFileCase' instances, and
`DocFileCase' is a subclass of `DocTestCase'.

  So both ways of creating a *note unittest.TestSuite: 456. run
instances of `DocTestCase'.  This is important for a subtle reason:
when you run *note doctest: b5. functions yourself, you can control the
*note doctest: b5. options in use directly, by passing option flags to
*note doctest: b5. functions.  However, if you're writing a *note
unittest: 187. framework, *note unittest: 187. ultimately controls when
and how tests get run.  The framework author typically wants to control
*note doctest: b5. reporting options (perhaps, e.g., specified by
command line options), but there's no way to pass options through *note
unittest: 187. to *note doctest: b5. test runners.

  For this reason, *note doctest: b5. also supports a notion of *note
doctest: b5.  reporting flags specific to *note unittest: 187. support,
via this function:

 -- Function: doctest.set_unittest_reportflags (flags)
     Set the *note doctest: b5. reporting flags to use.

     Argument _flags_ or's together option flags.  See section *note
     Option Flags: 21d2.  Only "reporting flags" can be used.

     This is a module-global setting, and affects all future doctests
     run by module *note unittest: 187.:  the `runTest()' method of
     `DocTestCase' looks at the option flags specified for the test
     case when the `DocTestCase' instance was constructed.  If no
     reporting flags were specified (which is the typical and expected
     case), *note doctest: b5.'s *note unittest: 187. reporting flags
     are or'ed into the option flags, and the option flags so augmented
     are passed to the *note DocTestRunner: 21db. instance created to
     run the doctest.  If any reporting flags were specified when the
     `DocTestCase' instance was constructed, *note doctest: b5.'s *note
     unittest: 187. reporting flags are ignored.

     The value of the *note unittest: 187. reporting flags in effect
     before the function was called is returned by the function.

     New in version 2.4.


File: python.info,  Node: Advanced API,  Next: Debugging,  Prev: Unittest API,  Up: doctest --- Test interactive Python examples

5.25.2.13 Advanced API
......................

The basic API is a simple wrapper that's intended to make doctest easy
to use.  It is fairly flexible, and should meet most users' needs;
however, if you require more fine-grained control over testing, or wish
to extend doctest's capabilities, then you should use the advanced API.

  The advanced API revolves around two container classes, which are
used to store the interactive examples extracted from doctest cases:

   * *note Example: 21ef.: A single Python *note statement: dac, paired
     with its expected output.

   * *note DocTest: 21ea.: A collection of *note Example: 21ef.s,
     typically extracted from a single docstring or text file.

  Additional processing classes are defined to find, parse, and run,
and check doctest examples:

   * *note DocTestFinder: 21e5.: Finds all docstrings in a given
     module, and uses a *note DocTestParser: 21ca. to create a *note
     DocTest: 21ea. from every docstring that contains interactive
     examples.

   * *note DocTestParser: 21ca.: Creates a *note DocTest: 21ea. object
     from a string (such as an object's docstring).

   * *note DocTestRunner: 21db.: Executes the examples in a *note
     DocTest: 21ea, and uses an *note OutputChecker: 21da. to verify
     their output.

   * *note OutputChecker: 21da.: Compares the actual output from a
     doctest example with the expected output, and decides whether they
     match.

  The relationships among these processing classes are summarized in
the following diagram:

                                list of:
    +------+                   +---------+
    |module| --DocTestFinder-> | DocTest | --DocTestRunner-> results
    +------+    |        ^     +---------+     |       ^    (printed)
                |        |     | Example |     |       |
                v        |     |   ...   |     v       |
               DocTestParser   | Example |   OutputChecker
                               +---------+


* Menu:

* DocTest Objects::
* Example Objects::
* DocTestFinder objects::
* DocTestParser objects::
* DocTestRunner objects::
* OutputChecker objects::


File: python.info,  Node: DocTest Objects,  Next: Example Objects,  Up: Advanced API

5.25.2.14 DocTest Objects
.........................

 -- Class: doctest.DocTest (examples, globs, name, filename, lineno,
          docstring)
     A collection of doctest examples that should be run in a single
     namespace.  The constructor arguments are used to initialize the
     attributes of the same names.

     New in version 2.4.

     *note DocTest: 21ea. defines the following attributes.  They are
     initialized by the constructor, and should not be modified
     directly.

      -- Attribute: examples
          A list of *note Example: 21ef. objects encoding the
          individual interactive Python examples that should be run by
          this test.

      -- Attribute: globs
          The namespace (aka globals) that the examples should be run
          in. This is a dictionary mapping names to values.  Any
          changes to the namespace made by the examples (such as
          binding new variables) will be reflected in *note globs: 21f3.
          after the test is run.

      -- Attribute: name
          A string name identifying the *note DocTest: 21ea.
          Typically, this is the name of the object or file that the
          test was extracted from.

      -- Attribute: filename
          The name of the file that this *note DocTest: 21ea. was
          extracted from; or `None' if the filename is unknown, or if
          the *note DocTest: 21ea. was not extracted from a file.

      -- Attribute: lineno
          The line number within *note filename: 21f5. where this *note
          DocTest: 21ea. begins, or `None' if the line number is
          unavailable.  This line number is zero-based with respect to
          the beginning of the file.

      -- Attribute: docstring
          The string that the test was extracted from, or 'None' if the
          string is unavailable, or if the test was not extracted from
          a string.


File: python.info,  Node: Example Objects,  Next: DocTestFinder objects,  Prev: DocTest Objects,  Up: Advanced API

5.25.2.15 Example Objects
.........................

 -- Class: doctest.Example (source, want[, exc_msg][, lineno][,
          indent][, options])
     A single interactive example, consisting of a Python statement and
     its expected output.  The constructor arguments are used to
     initialize the attributes of the same names.

     New in version 2.4.

     *note Example: 21ef. defines the following attributes.  They are
     initialized by the constructor, and should not be modified
     directly.

      -- Attribute: source
          A string containing the example's source code.  This source
          code consists of a single Python statement, and always ends
          with a newline; the constructor adds a newline when necessary.

      -- Attribute: want
          The expected output from running the example's source code
          (either from stdout, or a traceback in case of exception).
          *note want: 21fb. ends with a newline unless no output is
          expected, in which case it's an empty string.  The
          constructor adds a newline when necessary.

      -- Attribute: exc_msg
          The exception message generated by the example, if the
          example is expected to generate an exception; or `None' if it
          is not expected to generate an exception.  This exception
          message is compared against the return value of *note
          traceback.format_exception_only(): 21fd.  *note exc_msg:
          21fc. ends with a newline unless it's `None'.  The
          constructor adds a newline if needed.

      -- Attribute: lineno
          The line number within the string containing this example
          where the example begins.  This line number is zero-based
          with respect to the beginning of the containing string.

      -- Attribute: indent
          The example's indentation in the containing string, i.e., the
          number of space characters that precede the example's first
          prompt.

      -- Attribute: options
          A dictionary mapping from option flags to `True' or `False',
          which is used to override default options for this example.
          Any option flags not contained in this dictionary are left at
          their default value (as specified by the *note DocTestRunner:
          21db.'s `optionflags'). By default, no options are set.


File: python.info,  Node: DocTestFinder objects,  Next: DocTestParser objects,  Prev: Example Objects,  Up: Advanced API

5.25.2.16 DocTestFinder objects
...............................

 -- Class: doctest.DocTestFinder ([verbose][, parser][, recurse][,
          exclude_empty])
     A processing class used to extract the *note DocTest: 21ea.s that
     are relevant to a given object, from its docstring and the
     docstrings of its contained objects.  *note DocTest: 21ea.s can
     currently be extracted from the following object types: modules,
     functions, classes, methods, staticmethods, classmethods, and
     properties.

     The optional argument _verbose_ can be used to display the objects
     searched by the finder.  It defaults to `False' (no output).

     The optional argument _parser_ specifies the *note DocTestParser:
     21ca. object (or a drop-in replacement) that is used to extract
     doctests from docstrings.

     If the optional argument _recurse_ is false, then *note
     DocTestFinder.find(): 2203.  will only examine the given object,
     and not any contained objects.

     If the optional argument _exclude_empty_ is false, then *note
     DocTestFinder.find(): 2203. will include tests for objects with
     empty docstrings.

     New in version 2.4.

     *note DocTestFinder: 21e5. defines the following method:

      -- Method: find (obj[, name][, module][, globs][, extraglobs])
          Return a list of the *note DocTest: 21ea.s that are defined
          by _obj_'s docstring, or by any of its contained objects'
          docstrings.

          The optional argument _name_ specifies the object's name;
          this name will be used to construct names for the returned
          *note DocTest: 21ea.s.  If _name_ is not specified, then
          `obj.__name__' is used.

          The optional parameter _module_ is the module that contains
          the given object.  If the module is not specified or is None,
          then the test finder will attempt to automatically determine
          the correct module.  The object's module is used:

             * As a default namespace, if _globs_ is not specified.

             * To prevent the DocTestFinder from extracting DocTests
               from objects that are imported from other modules.
               (Contained objects with modules other than _module_ are
               ignored.)

             * To find the name of the file containing the object.

             * To help find the line number of the object within its
               file.

          If _module_ is `False', no attempt to find the module will be
          made.  This is obscure, of use mostly in testing doctest
          itself: if _module_ is `False', or is `None' but cannot be
          found automatically, then all objects are considered to
          belong to the (non-existent) module, so all contained objects
          will (recursively) be searched for doctests.

          The globals for each *note DocTest: 21ea. is formed by
          combining _globs_ and _extraglobs_ (bindings in _extraglobs_
          override bindings in _globs_).  A new shallow copy of the
          globals dictionary is created for each *note DocTest: 21ea.
          If _globs_ is not specified, then it defaults to the module's
          ___dict___, if specified, or `{}' otherwise.  If _extraglobs_
          is not specified, then it defaults to `{}'.


File: python.info,  Node: DocTestParser objects,  Next: DocTestRunner objects,  Prev: DocTestFinder objects,  Up: Advanced API

5.25.2.17 DocTestParser objects
...............................

 -- Class: doctest.DocTestParser
     A processing class used to extract interactive examples from a
     string, and use them to create a *note DocTest: 21ea. object.

     New in version 2.4.

     *note DocTestParser: 21ca. defines the following methods:

      -- Method: get_doctest (string, globs, name, filename, lineno)
          Extract all doctest examples from the given string, and
          collect them into a *note DocTest: 21ea. object.

          _globs_, _name_, _filename_, and _lineno_ are attributes for
          the new *note DocTest: 21ea. object.  See the documentation
          for *note DocTest: 21ea. for more information.

      -- Method: get_examples (string[, name])
          Extract all doctest examples from the given string, and
          return them as a list of *note Example: 21ef. objects.  Line
          numbers are 0-based.  The optional argument _name_ is a name
          identifying this string, and is only used for error messages.

      -- Method: parse (string[, name])
          Divide the given string into examples and intervening text,
          and return them as a list of alternating *note Example:
          21ef.s and strings. Line numbers for the *note Example:
          21ef.s are 0-based.  The optional argument _name_ is a name
          identifying this string, and is only used for error messages.


File: python.info,  Node: DocTestRunner objects,  Next: OutputChecker objects,  Prev: DocTestParser objects,  Up: Advanced API

5.25.2.18 DocTestRunner objects
...............................

 -- Class: doctest.DocTestRunner ([checker][, verbose][, optionflags])
     A processing class used to execute and verify the interactive
     examples in a *note DocTest: 21ea.

     The comparison between expected outputs and actual outputs is done
     by an *note OutputChecker: 21da.  This comparison may be
     customized with a number of option flags; see section *note Option
     Flags: 21d2. for more information.  If the option flags are
     insufficient, then the comparison may also be customized by
     passing a subclass of *note OutputChecker: 21da. to the
     constructor.

     The test runner's display output can be controlled in two ways.
     First, an output function can be passed to `TestRunner.run()';
     this function will be called with strings that should be
     displayed.  It defaults to `sys.stdout.write'.  If capturing the
     output is not sufficient, then the display output can be also
     customized by subclassing DocTestRunner, and overriding the methods
     *note report_start(): 220b, *note report_success(): 220c, *note
     report_unexpected_exception(): 220d, and *note report_failure():
     220e.

     The optional keyword argument _checker_ specifies the *note
     OutputChecker: 21da.  object (or drop-in replacement) that should
     be used to compare the expected outputs to the actual outputs of
     doctest examples.

     The optional keyword argument _verbose_ controls the *note
     DocTestRunner: 21db.'s verbosity.  If _verbose_ is `True', then
     information is printed about each example, as it is run.  If
     _verbose_ is `False', then only failures are printed.  If
     _verbose_ is unspecified, or `None', then verbose output is used
     iff the command-line switch `-v' is used.

     The optional keyword argument _optionflags_ can be used to control
     how the test runner compares expected output to actual output, and
     how it displays failures.  For more information, see section *note
     Option Flags: 21d2.

     New in version 2.4.

     *note DocTestParser: 21ca. defines the following methods:

      -- Method: report_start (out, test, example)
          Report that the test runner is about to process the given
          example. This method is provided to allow subclasses of *note
          DocTestRunner: 21db. to customize their output; it should not
          be called directly.

          _example_ is the example about to be processed.  _test_ is
          the test _containing example_.  _out_ is the output function
          that was passed to *note DocTestRunner.run(): 220f.

      -- Method: report_success (out, test, example, got)
          Report that the given example ran successfully.  This method
          is provided to allow subclasses of *note DocTestRunner: 21db.
          to customize their output; it should not be called directly.

          _example_ is the example about to be processed.  _got_ is the
          actual output from the example.  _test_ is the test
          containing _example_.  _out_ is the output function that was
          passed to *note DocTestRunner.run(): 220f.

      -- Method: report_failure (out, test, example, got)
          Report that the given example failed.  This method is
          provided to allow subclasses of *note DocTestRunner: 21db. to
          customize their output; it should not be called directly.

          _example_ is the example about to be processed.  _got_ is the
          actual output from the example.  _test_ is the test
          containing _example_.  _out_ is the output function that was
          passed to *note DocTestRunner.run(): 220f.

      -- Method: report_unexpected_exception (out, test, example,
               exc_info)
          Report that the given example raised an unexpected exception.
          This method is provided to allow subclasses of *note
          DocTestRunner: 21db. to customize their output; it should not
          be called directly.

          _example_ is the example about to be processed. _exc_info_ is
          a tuple containing information about the unexpected exception
          (as returned by *note sys.exc_info(): 2ec.). _test_ is the
          test containing _example_.  _out_ is the output function that
          was passed to *note DocTestRunner.run(): 220f.

      -- Method: run (test[, compileflags][, out][, clear_globs])
          Run the examples in _test_ (a *note DocTest: 21ea. object),
          and display the results using the writer function _out_.

          The examples are run in the namespace `test.globs'.  If
          _clear_globs_ is true (the default), then this namespace will
          be cleared after the test runs, to help with garbage
          collection. If you would like to examine the namespace after
          the test completes, then use _clear_globs=False_.

          _compileflags_ gives the set of flags that should be used by
          the Python compiler when running the examples.  If not
          specified, then it will default to the set of future-import
          flags that apply to _globs_.

          The output of each example is checked using the *note
          DocTestRunner: 21db.'s output checker, and the results are
          formatted by the `DocTestRunner.report_*()' methods.

      -- Method: summarize ([verbose])
          Print a summary of all the test cases that have been run by
          this DocTestRunner, and return a *note named tuple: a0f.
          `TestResults(failed, attempted)'.

          The optional _verbose_ argument controls how detailed the
          summary is.  If the verbosity is not specified, then the
          *note DocTestRunner: 21db.'s verbosity is used.

          Changed in version 2.6: Use a named tuple.


File: python.info,  Node: OutputChecker objects,  Prev: DocTestRunner objects,  Up: Advanced API

5.25.2.19 OutputChecker objects
...............................

 -- Class: doctest.OutputChecker
     A class used to check the whether the actual output from a doctest
     example matches the expected output.  *note OutputChecker: 21da.
     defines two methods: *note check_output(): 2213, which compares a
     given pair of outputs, and returns true if they match; and *note
     output_difference(): 2214, which returns a string describing the
     differences between two outputs.

     New in version 2.4.

     *note OutputChecker: 21da. defines the following methods:

      -- Method: check_output (want, got, optionflags)
          Return `True' iff the actual output from an example (_got_)
          matches the expected output (_want_).  These strings are
          always considered to match if they are identical; but
          depending on what option flags the test runner is using,
          several non-exact match types are also possible.  See section
          *note Option Flags: 21d2. for more information about option
          flags.

      -- Method: output_difference (example, got, optionflags)
          Return a string describing the differences between the
          expected output for a given example (_example_) and the
          actual output (_got_).  _optionflags_ is the set of option
          flags used to compare _want_ and _got_.


File: python.info,  Node: Debugging,  Next: Soapbox,  Prev: Advanced API,  Up: doctest --- Test interactive Python examples

5.25.2.20 Debugging
...................

Doctest provides several mechanisms for debugging doctest examples:

   * Several functions convert doctests to executable Python programs,
     which can be run under the Python debugger, *note pdb: 12c.

   * The *note DebugRunner: 2217. class is a subclass of *note
     DocTestRunner: 21db. that raises an exception for the first
     failing example, containing information about that example. This
     information can be used to perform post-mortem debugging on the
     example.

   * The *note unittest: 187. cases generated by *note DocTestSuite():
     21ec. support the *note debug(): 2218. method defined by *note
     unittest.TestCase: 27f.

   * You can add a call to *note pdb.set_trace(): 2219. in a doctest
     example, and you'll drop into the Python debugger when that line
     is executed.  Then you can inspect current values of variables,
     and so on.  For example, suppose `a.py' contains just this module
     docstring:

         """
         >>> def f(x):
         ...     g(x*2)
         >>> def g(x):
         ...     print x+3
         ...     import pdb; pdb.set_trace()
         >>> f(3)
         9
         """

     Then an interactive Python session may look like this:

         >>> import a, doctest
         >>> doctest.testmod(a)
         --Return--
         > <doctest a[1]>(3)g()->None
         -> import pdb; pdb.set_trace()
         (Pdb) list
           1     def g(x):
           2         print x+3
           3  ->     import pdb; pdb.set_trace()
         [EOF]
         (Pdb) print x
         6
         (Pdb) step
         --Return--
         > <doctest a[0]>(2)f()->None
         -> g(x*2)
         (Pdb) list
           1     def f(x):
           2  ->     g(x*2)
         [EOF]
         (Pdb) print x
         3
         (Pdb) step
         --Return--
         > <doctest a[2]>(1)?()->None
         -> f(3)
         (Pdb) cont
         (0, 3)
         >>>

     Changed in version 2.4: The ability to use *note pdb.set_trace():
     2219. usefully inside doctests was added.

  Functions that convert doctests to Python code, and possibly run the
synthesized code under the debugger:

 -- Function: doctest.script_from_examples (s)
     Convert text with examples to a script.

     Argument _s_ is a string containing doctest examples.  The string
     is converted to a Python script, where doctest examples in _s_ are
     converted to regular code, and everything else is converted to
     Python comments.  The generated script is returned as a string.
     For example,

         import doctest
         print doctest.script_from_examples(r"""
             Set x and y to 1 and 2.
             >>> x, y = 1, 2

             Print their sum:
             >>> print x+y
             3
         """)

     displays:

         # Set x and y to 1 and 2.
         x, y = 1, 2
         #
         # Print their sum:
         print x+y
         # Expected:
         ## 3

     This function is used internally by other functions (see below),
     but can also be useful when you want to transform an interactive
     Python session into a Python script.

     New in version 2.4.

 -- Function: doctest.testsource (module, name)
     Convert the doctest for an object to a script.

     Argument _module_ is a module object, or dotted name of a module,
     containing the object whose doctests are of interest.  Argument
     _name_ is the name (within the module) of the object with the
     doctests of interest.  The result is a string, containing the
     object's docstring converted to a Python script, as described for
     *note script_from_examples(): 221a. above.  For example, if module
     `a.py' contains a top-level function `f()', then

         import a, doctest
         print doctest.testsource(a, "a.f")

     prints a script version of function `f()''s docstring, with
     doctests converted to code, and the rest placed in comments.

     New in version 2.3.

 -- Function: doctest.debug (module, name[, pm])
     Debug the doctests for an object.

     The _module_ and _name_ arguments are the same as for function
     *note testsource(): 221b. above.  The synthesized Python script
     for the named object's docstring is written to a temporary file,
     and then that file is run under the control of the Python
     debugger, *note pdb: 12c.

     A shallow copy of `module.__dict__' is used for both local and
     global execution context.

     Optional argument _pm_ controls whether post-mortem debugging is
     used.  If _pm_ has a true value, the script file is run directly,
     and the debugger gets involved only if the script terminates via
     raising an unhandled exception.  If it does, then post-mortem
     debugging is invoked, via *note pdb.post_mortem(): 34e, passing
     the traceback object from the unhandled exception.  If _pm_ is not
     specified, or is false, the script is run under the debugger from
     the start, via passing an appropriate *note execfile(): 42f. call
     to *note pdb.run(): 221c.

     New in version 2.3.

     Changed in version 2.4: The _pm_ argument was added.

 -- Function: doctest.debug_src (src[, pm][, globs])
     Debug the doctests in a string.

     This is like function *note debug(): 2218. above, except that a
     string containing doctest examples is specified directly, via the
     _src_ argument.

     Optional argument _pm_ has the same meaning as in function *note
     debug(): 2218. above.

     Optional argument _globs_ gives a dictionary to use as both local
     and global execution context.  If not specified, or `None', an
     empty dictionary is used.  If specified, a shallow copy of the
     dictionary is used.

     New in version 2.4.

  The *note DebugRunner: 2217. class, and the special exceptions it may
raise, are of most interest to testing framework authors, and will only
be sketched here.  See the source code, and especially *note
DebugRunner: 2217.'s docstring (which is a doctest!) for more details:

 -- Class: doctest.DebugRunner ([checker][, verbose][, optionflags])
     A subclass of *note DocTestRunner: 21db. that raises an exception
     as soon as a failure is encountered.  If an unexpected exception
     occurs, an *note UnexpectedException: 221e. exception is raised,
     containing the test, the example, and the original exception.  If
     the output doesn't match, then a *note DocTestFailure: 221f.
     exception is raised, containing the test, the example, and the
     actual output.

     For information about the constructor parameters and methods, see
     the documentation for *note DocTestRunner: 21db. in section *note
     Advanced API: 21ee.

  There are two exceptions that may be raised by *note DebugRunner:
2217. instances:

 -- Exception: doctest.DocTestFailure (test, example, got)
     An exception raised by *note DocTestRunner: 21db. to signal that a
     doctest example's actual output did not match its expected output.
     The constructor arguments are used to initialize the attributes of
     the same names.

  *note DocTestFailure: 221f. defines the following attributes:

 -- Attribute: DocTestFailure.test
     The *note DocTest: 21ea. object that was being run when the
     example failed.

 -- Attribute: DocTestFailure.example
     The *note Example: 21ef. that failed.

 -- Attribute: DocTestFailure.got
     The example's actual output.

 -- Exception: doctest.UnexpectedException (test, example, exc_info)
     An exception raised by *note DocTestRunner: 21db. to signal that a
     doctest example raised an unexpected exception.  The constructor
     arguments are used to initialize the attributes of the same names.

  *note UnexpectedException: 221e. defines the following attributes:

 -- Attribute: UnexpectedException.test
     The *note DocTest: 21ea. object that was being run when the
     example failed.

 -- Attribute: UnexpectedException.example
     The *note Example: 21ef. that failed.

 -- Attribute: UnexpectedException.exc_info
     A tuple containing information about the unexpected exception, as
     returned by *note sys.exc_info(): 2ec.


File: python.info,  Node: Soapbox,  Prev: Debugging,  Up: doctest --- Test interactive Python examples

5.25.2.21 Soapbox
.................

As mentioned in the introduction, *note doctest: b5. has grown to have
three primary uses:

  1. Checking examples in docstrings.

  2. Regression testing.

  3. Executable documentation / literate testing.

  These uses have different requirements, and it is important to
distinguish them.  In particular, filling your docstrings with obscure
test cases makes for bad documentation.

  When writing a docstring, choose docstring examples with care.
There's an art to this that needs to be learned--it may not be natural
at first.  Examples should add genuine value to the documentation.  A
good example can often be worth many words. If done with care, the
examples will be invaluable for your users, and will pay back the time
it takes to collect them many times over as the years go by and things
change.  I'm still amazed at how often one of my *note doctest: b5.
examples stops working after a "harmless" change.

  Doctest also makes an excellent tool for regression testing,
especially if you don't skimp on explanatory text.  By interleaving
prose and examples, it becomes much easier to keep track of what's
actually being tested, and why.  When a test fails, good prose can make
it much easier to figure out what the problem is, and how it should be
fixed.  It's true that you could write extensive comments in code-based
testing, but few programmers do. Many have found that using doctest
approaches instead leads to much clearer tests.  Perhaps this is simply
because doctest makes writing prose a little easier than writing code,
while writing comments in code is a little harder.  I think it goes
deeper than just that: the natural attitude when writing a
doctest-based test is that you want to explain the fine points of your
software, and illustrate them with examples.  This in turn naturally
leads to test files that start with the simplest features, and
logically progress to complications and edge cases.  A coherent
narrative is the result, instead of a collection of isolated functions
that test isolated bits of functionality seemingly at random.  It's a
different attitude, and produces different results, blurring the
distinction between testing and explaining.

  Regression testing is best confined to dedicated objects or files.
There are several options for organizing tests:

   * Write text files containing test cases as interactive examples,
     and test the files using *note testfile(): 21c1. or *note
     DocFileSuite(): 21e9.  This is recommended, although is easiest to
     do for new projects, designed from the start to use doctest.

   * Define functions named `_regrtest_topic' that consist of single
     docstrings, containing test cases for the named topics.  These
     functions can be included in the same file as the module, or
     separated out into a separate test file.

   * Define a `__test__' dictionary mapping from regression test topics
     to docstrings containing test cases.


File: python.info,  Node: unittest --- Unit testing framework,  Next: 2to3 - Automated Python 2 to 3 code translation,  Prev: doctest --- Test interactive Python examples,  Up: Development Tools

5.25.3 `unittest' -- Unit testing framework
-------------------------------------------

New in version 2.1.

  (If you are already familiar with the basic concepts of testing, you
might want to skip to *note the list of assert methods: 222a.)

  The Python unit testing framework, sometimes referred to as "PyUnit,"
is a Python language version of JUnit, by Kent Beck and Erich Gamma.
JUnit is, in turn, a Java version of Kent's Smalltalk testing
framework.  Each is the de facto standard unit testing framework for
its respective language.

  *note unittest: 187. supports test automation, sharing of setup and
shutdown code for tests, aggregation of tests into collections, and
independence of the tests from the reporting framework.  The *note
unittest: 187. module provides classes that make it easy to support
these qualities for a set of tests.

  To achieve this, *note unittest: 187. supports some important
concepts:

test fixture
     A _test fixture_ represents the preparation needed to perform one
     or more tests, and any associate cleanup actions.  This may
     involve, for example, creating temporary or proxy databases,
     directories, or starting a server process.

test case
     A _test case_ is the smallest unit of testing.  It checks for a
     specific response to a particular set of inputs.  *note unittest:
     187. provides a base class, *note TestCase: 27f, which may be used
     to create new test cases.

test suite
     A _test suite_ is a collection of test cases, test suites, or
     both.  It is used to aggregate tests that should be executed
     together.

test runner
     A _test runner_ is a component which orchestrates the execution of
     tests and provides the outcome to the user.  The runner may use a
     graphical interface, a textual interface, or return a special
     value to indicate the results of executing the tests.

  The test case and test fixture concepts are supported through the
*note TestCase: 27f. and *note FunctionTestCase: 222b. classes; the
former should be used when creating new tests, and the latter can be
used when integrating existing test code with a *note unittest:
187.-driven framework. When building test fixtures using *note
TestCase: 27f, the *note setUp(): 285. and *note tearDown(): 286.
methods can be overridden to provide initialization and cleanup for the
fixture.  With *note FunctionTestCase: 222b, existing functions can be
passed to the constructor for these purposes.  When the test is run, the
fixture initialization is run first; if it succeeds, the cleanup method
is run after the test has been executed, regardless of the outcome of
the test.  Each instance of the *note TestCase: 27f. will only be used
to run a single test method, so a new fixture is created for each test.

  Test suites are implemented by the *note TestSuite: 456. class.  This
class allows individual tests and test suites to be aggregated; when
the suite is executed, all tests added directly to the suite and in
"child" test suites are run.

  A test runner is an object that provides a single method, `run()',
which accepts a *note TestCase: 27f. or *note TestSuite: 456.  object
as a parameter, and returns a result object.  The class *note
TestResult: 2a5. is provided for use as the result object. *note
unittest: 187.  provides the *note TextTestRunner: 222c. as an example
test runner which reports test results on the standard error stream by
default.  Alternate runners can be implemented for other environments
(such as graphical environments) without any need to derive from a
specific class.

See also
........

Module *note doctest: b5.
     Another test-support module with a very different flavor.

unittest2: A backport of new unittest features for Python 2.4-2.6(1)
     Many new features were added to unittest in Python 2.7, including
     test discovery. unittest2 allows you to use these features with
     earlier versions of Python.

Simple Smalltalk Testing: With Patterns(2)
     Kent Beck's original paper on testing frameworks using the pattern
     shared by *note unittest: 187.

Nose(3) and py.test(4)
     Third-party unittest frameworks with a lighter-weight syntax for
     writing tests.  For example, `assert func(10) == 42'.

The Python Testing Tools Taxonomy(5)
     An extensive list of Python testing tools including functional
     testing frameworks and mock object libraries.

Testing in Python Mailing List(6)
     A special-interest-group for discussion of testing, and testing
     tools, in Python.

* Menu:

* Basic example::
* Command-Line Interface::
* Test Discovery::
* Organizing test code::
* Re-using old test code::
* Skipping tests and expected failures::
* Classes and functions::
* Class and Module Fixtures::
* Signal Handling::

  ---------- Footnotes ----------

  (1) http://pypi.python.org/pypi/unittest2

  (2) http://www.XProgramming.com/testfram.htm

  (3) http://code.google.com/p/python-nose/

  (4) http://pytest.org

  (5) http://pycheesecake.org/wiki/PythonTestingToolsTaxonomy

  (6) http://lists.idyll.org/listinfo/testing-in-python


File: python.info,  Node: Basic example,  Next: Command-Line Interface,  Up: unittest --- Unit testing framework

5.25.3.1 Basic example
......................

The *note unittest: 187. module provides a rich set of tools for
constructing and running tests.  This section demonstrates that a small
subset of the tools suffice to meet the needs of most users.

  Here is a short script to test three functions from the *note random:
142. module:

    import random
    import unittest

    class TestSequenceFunctions(unittest.TestCase):

        def setUp(self):
            self.seq = range(10)

        def test_shuffle(self):
            # make sure the shuffled sequence does not lose any elements
            random.shuffle(self.seq)
            self.seq.sort()
            self.assertEqual(self.seq, range(10))

            # should raise an exception for an immutable sequence
            self.assertRaises(TypeError, random.shuffle, (1,2,3))

        def test_choice(self):
            element = random.choice(self.seq)
            self.assertTrue(element in self.seq)

        def test_sample(self):
            with self.assertRaises(ValueError):
                random.sample(self.seq, 20)
            for element in random.sample(self.seq, 5):
                self.assertTrue(element in self.seq)

    if __name__ == '__main__':
        unittest.main()

A testcase is created by subclassing *note unittest.TestCase: 27f.  The
three individual tests are defined with methods whose names start with
the letters `test'.  This naming convention informs the test runner
about which methods represent tests.

  The crux of each test is a call to *note assertEqual(): 27b. to check
for an expected result; *note assertTrue(): 27c. to verify a condition;
or *note assertRaises(): 280. to verify that an expected exception gets
raised.  These methods are used instead of the *note assert: 44b.
statement so the test runner can accumulate all test results and
produce a report.

  When a *note setUp(): 285. method is defined, the test runner will
run that method prior to each test.  Likewise, if a *note tearDown():
286. method is defined, the test runner will invoke that method after
each test.  In the example, *note setUp(): 285. was used to create a
fresh sequence for each test.

  The final block shows a simple way to run the tests. *note
unittest.main(): 277.  provides a command-line interface to the test
script.  When run from the command line, the above script produces an
output that looks like this:

    ...
    ----------------------------------------------------------------------
    Ran 3 tests in 0.000s

    OK

Instead of *note unittest.main(): 277, there are other ways to run the
tests with a finer level of control, less terse output, and no
requirement to be run from the command line.  For example, the last two
lines may be replaced with:

    suite = unittest.TestLoader().loadTestsFromTestCase(TestSequenceFunctions)
    unittest.TextTestRunner(verbosity=2).run(suite)

Running the revised script from the interpreter or another script
produces the following output:

    test_choice (__main__.TestSequenceFunctions) ... ok
    test_sample (__main__.TestSequenceFunctions) ... ok
    test_shuffle (__main__.TestSequenceFunctions) ... ok

    ----------------------------------------------------------------------
    Ran 3 tests in 0.110s

    OK

The above examples show the most commonly used *note unittest: 187.
features which are sufficient to meet many everyday testing needs.  The
remainder of the documentation explores the full feature set from first
principles.


File: python.info,  Node: Command-Line Interface,  Next: Test Discovery,  Prev: Basic example,  Up: unittest --- Unit testing framework

5.25.3.2 Command-Line Interface
...............................

The unittest module can be used from the command line to run tests from
modules, classes or even individual test methods:

    python -m unittest test_module1 test_module2
    python -m unittest test_module.TestClass
    python -m unittest test_module.TestClass.test_method

You can pass in a list with any combination of module names, and fully
qualified class or method names.

  You can run tests with more detail (higher verbosity) by passing in
the -v flag:

    python -m unittest -v test_module

For a list of all the command-line options:

    python -m unittest -h

Changed in version 2.7: In earlier versions it was only possible to run
individual test methods and not modules or classes.

* Menu:

* Command-line options::


File: python.info,  Node: Command-line options,  Up: Command-Line Interface

5.25.3.3 Command-line options
.............................

*unittest* supports these command-line options:

 -- Program Option: -b, -buffer
     The standard output and standard error streams are buffered during
     the test run. Output during a passing test is discarded. Output is
     echoed normally on test fail or error and is added to the failure
     messages.

 -- Program Option: -c, -catch
     Control-C during the test run waits for the current test to end
     and then reports all the results so far. A second control-C raises
     the normal *note KeyboardInterrupt: 24e. exception.

     See *note Signal Handling: 2234. for the functions that provide
     this functionality.

 -- Program Option: -f, -failfast
     Stop the test run on the first error or failure.

  New in version 2.7: The command-line options `-b', `-c' and `-f' were
added.

  The command line can also be used for test discovery, for running all
of the tests in a project or just a subset.


File: python.info,  Node: Test Discovery,  Next: Organizing test code,  Prev: Command-Line Interface,  Up: unittest --- Unit testing framework

5.25.3.4 Test Discovery
.......................

New in version 2.7.

  Unittest supports simple test discovery. In order to be compatible
with test discovery, all of the test files must be *note modules: 56c.
or *note packages: 580. importable from the top-level directory of the
project (this means that their filenames must be valid *note
identifiers: 69f.).

  Test discovery is implemented in *note TestLoader.discover(): 2238,
but can also be used from the command line. The basic command-line
usage is:

    cd project_directory
    python -m unittest discover

The `discover' sub-command has the following options:

 -- Program Option: -v, -verbose
     Verbose output

 -- Program Option: -s, -start-directory directory
     Directory to start discovery (`.' default)

 -- Program Option: -p, -pattern pattern
     Pattern to match test files (`test*.py' default)

 -- Program Option: -t, -top-level-directory directory
     Top level directory of project (defaults to start directory)

  The *note -s: 223a, *note -p: 223b, and *note -t: 223c. options can
be passed in as positional arguments in that order. The following two
command lines are equivalent:

    python -m unittest discover -s project_directory -p '*_test.py'
    python -m unittest discover project_directory '*_test.py'

As well as being a path it is possible to pass a package name, for
example `myproject.subpackage.test', as the start directory. The
package name you supply will then be imported and its location on the
filesystem will be used as the start directory.

     Caution: Test discovery loads tests by importing them. Once test
     discovery has found all the test files from the start directory
     you specify it turns the paths into package names to import. For
     example `foo/bar/baz.py' will be imported as `foo.bar.baz'.

     If you have a package installed globally and attempt test
     discovery on a different copy of the package then the import
     _could_ happen from the wrong place. If this happens test
     discovery will warn you and exit.

     If you supply the start directory as a package name rather than a
     path to a directory then discover assumes that whichever location
     it imports from is the location you intended, so you will not get
     the warning.

  Test modules and packages can customize test loading and discovery by
through the *note load_tests protocol: 223d.


File: python.info,  Node: Organizing test code,  Next: Re-using old test code,  Prev: Test Discovery,  Up: unittest --- Unit testing framework

5.25.3.5 Organizing test code
.............................

The basic building blocks of unit testing are _test cases_ -- single
scenarios that must be set up and checked for correctness.  In *note
unittest: 187, test cases are represented by instances of *note
unittest: 187.'s *note TestCase: 27f.  class. To make your own test
cases you must write subclasses of *note TestCase: 27f, or use *note
FunctionTestCase: 222b.

  An instance of a *note TestCase: 27f.-derived class is an object that
can completely run a single test method, together with optional set-up
and tidy-up code.

  The testing code of a *note TestCase: 27f. instance should be
entirely self contained, such that it can be run either in isolation or
in arbitrary combination with any number of other test cases.

  The simplest *note TestCase: 27f. subclass will simply override the
`runTest()' method in order to perform specific testing code:

    import unittest

    class DefaultWidgetSizeTestCase(unittest.TestCase):
        def runTest(self):
            widget = Widget('The widget')
            self.assertEqual(widget.size(), (50, 50), 'incorrect default size')

Note that in order to test something, we use one of the `assert*()'
methods provided by the *note TestCase: 27f. base class.  If the test
fails, an exception will be raised, and *note unittest: 187. will
identify the test case as a _failure_.  Any other exceptions will be
treated as _errors_. This helps you identify where the problem is:
_failures_ are caused by incorrect results - a 5 where you expected a
6. _Errors_ are caused by incorrect code - e.g., a *note TypeError:
215. caused by an incorrect function call.

  The way to run a test case will be described later.  For now, note
that to construct an instance of such a test case, we call its
constructor without arguments:

    testCase = DefaultWidgetSizeTestCase()

Now, such test cases can be numerous, and their set-up can be
repetitive.  In the above case, constructing a `Widget' in each of 100
Widget test case subclasses would mean unsightly duplication.

  Luckily, we can factor out such set-up code by implementing a method
called *note setUp(): 285, which the testing framework will
automatically call for us when we run the test:

    import unittest

    class SimpleWidgetTestCase(unittest.TestCase):
        def setUp(self):
            self.widget = Widget('The widget')

    class DefaultWidgetSizeTestCase(SimpleWidgetTestCase):
        def runTest(self):
            self.assertEqual(self.widget.size(), (50,50),
                             'incorrect default size')

    class WidgetResizeTestCase(SimpleWidgetTestCase):
        def runTest(self):
            self.widget.resize(100,150)
            self.assertEqual(self.widget.size(), (100,150),
                             'wrong size after resize')

If the *note setUp(): 285. method raises an exception while the test is
running, the framework will consider the test to have suffered an
error, and the `runTest()' method will not be executed.

  Similarly, we can provide a *note tearDown(): 286. method that tidies
up after the `runTest()' method has been run:

    import unittest

    class SimpleWidgetTestCase(unittest.TestCase):
        def setUp(self):
            self.widget = Widget('The widget')

        def tearDown(self):
            self.widget.dispose()
            self.widget = None

If *note setUp(): 285. succeeded, the *note tearDown(): 286. method will
be run whether `runTest()' succeeded or not.

  Such a working environment for the testing code is called a _fixture_.

  Often, many small test cases will use the same fixture.  In this
case, we would end up subclassing `SimpleWidgetTestCase' into many
small one-method classes such as `DefaultWidgetSizeTestCase'.  This is
time-consuming and discouraging, so in the same vein as JUnit, *note
unittest: 187. provides a simpler mechanism:

    import unittest

    class WidgetTestCase(unittest.TestCase):
        def setUp(self):
            self.widget = Widget('The widget')

        def tearDown(self):
            self.widget.dispose()
            self.widget = None

        def test_default_size(self):
            self.assertEqual(self.widget.size(), (50,50),
                             'incorrect default size')

        def test_resize(self):
            self.widget.resize(100,150)
            self.assertEqual(self.widget.size(), (100,150),
                             'wrong size after resize')

Here we have not provided a `runTest()' method, but have instead
provided two different test methods.  Class instances will now each run
one of the `test_*()' methods, with `self.widget' created and destroyed
separately for each instance.  When creating an instance we must
specify the test method it is to run.  We do this by passing the method
name in the constructor:

    defaultSizeTestCase = WidgetTestCase('test_default_size')
    resizeTestCase = WidgetTestCase('test_resize')

Test case instances are grouped together according to the features they
test.  *note unittest: 187. provides a mechanism for this: the _test
suite_, represented by *note unittest: 187.'s *note TestSuite: 456.
class:

    widgetTestSuite = unittest.TestSuite()
    widgetTestSuite.addTest(WidgetTestCase('test_default_size'))
    widgetTestSuite.addTest(WidgetTestCase('test_resize'))

For the ease of running tests, as we will see later, it is a good idea
to provide in each test module a callable object that returns a
pre-built test suite:

    def suite():
        suite = unittest.TestSuite()
        suite.addTest(WidgetTestCase('test_default_size'))
        suite.addTest(WidgetTestCase('test_resize'))
        return suite

or even:

    def suite():
        tests = ['test_default_size', 'test_resize']

        return unittest.TestSuite(map(WidgetTestCase, tests))

Since it is a common pattern to create a *note TestCase: 27f. subclass
with many similarly named test functions, *note unittest: 187. provides
a *note TestLoader: 2a2.  class that can be used to automate the
process of creating a test suite and populating it with individual
tests. For example,

    suite = unittest.TestLoader().loadTestsFromTestCase(WidgetTestCase)

will create a test suite that will run
`WidgetTestCase.test_default_size()' and `WidgetTestCase.test_resize'.
*note TestLoader: 2a2. uses the `'test'' method name prefix to identify
test methods automatically.

  Note that the order in which the various test cases will be run is
determined by sorting the test function names with respect to the
built-in ordering for strings.

  Often it is desirable to group suites of test cases together, so as
to run tests for the whole system at once.  This is easy, since *note
TestSuite: 456. instances can be added to a *note TestSuite: 456. just
as *note TestCase: 27f. instances can be added to a *note TestSuite:
456.:

    suite1 = module1.TheTestSuite()
    suite2 = module2.TheTestSuite()
    alltests = unittest.TestSuite([suite1, suite2])

You can place the definitions of test cases and test suites in the same
modules as the code they are to test (such as `widget.py'), but there
are several advantages to placing the test code in a separate module,
such as `test_widget.py':

   * The test module can be run standalone from the command line.

   * The test code can more easily be separated from shipped code.

   * There is less temptation to change test code to fit the code it
     tests without a good reason.

   * Test code should be modified much less frequently than the code it
     tests.

   * Tested code can be refactored more easily.

   * Tests for modules written in C must be in separate modules anyway,
     so why not be consistent?

   * If the testing strategy changes, there is no need to change the
     source code.


File: python.info,  Node: Re-using old test code,  Next: Skipping tests and expected failures,  Prev: Organizing test code,  Up: unittest --- Unit testing framework

5.25.3.6 Re-using old test code
...............................

Some users will find that they have existing test code that they would
like to run from *note unittest: 187, without converting every old test
function to a *note TestCase: 27f. subclass.

  For this reason, *note unittest: 187. provides a *note
FunctionTestCase: 222b. class.  This subclass of *note TestCase: 27f.
can be used to wrap an existing test function.  Set-up and tear-down
functions can also be provided.

  Given the following test function:

    def testSomething():
        something = makeSomething()
        assert something.name is not None
        # ...

one can create an equivalent test case instance as follows:

    testcase = unittest.FunctionTestCase(testSomething)

If there are additional set-up and tear-down methods that should be
called as part of the test case's operation, they can also be provided
like so:

    testcase = unittest.FunctionTestCase(testSomething,
                                         setUp=makeSomethingDB,
                                         tearDown=deleteSomethingDB)

To make migrating existing test suites easier, *note unittest: 187.
supports tests raising *note AssertionError: 7f7. to indicate test
failure. However, it is recommended that you use the explicit
`TestCase.fail*()' and `TestCase.assert*()' methods instead, as future
versions of *note unittest: 187.  may treat *note AssertionError: 7f7.
differently.

     Note: Even though *note FunctionTestCase: 222b. can be used to
     quickly convert an existing test base over to a *note unittest:
     187.-based system, this approach is not recommended.  Taking the
     time to set up proper *note TestCase: 27f.  subclasses will make
     future test refactorings infinitely easier.

  In some cases, the existing tests may have been written using the
*note doctest: b5.  module.  If so, *note doctest: b5. provides a
`DocTestSuite' class that can automatically build *note
unittest.TestSuite: 456. instances from the existing *note doctest:
b5.-based tests.


File: python.info,  Node: Skipping tests and expected failures,  Next: Classes and functions,  Prev: Re-using old test code,  Up: unittest --- Unit testing framework

5.25.3.7 Skipping tests and expected failures
.............................................

New in version 2.7.

  Unittest supports skipping individual test methods and even whole
classes of tests.  In addition, it supports marking a test as a
"expected failure," a test that is broken and will fail, but shouldn't
be counted as a failure on a *note TestResult: 2a5.

  Skipping a test is simply a matter of using the *note skip(): 2244.
*note decorator: 841.  or one of its conditional variants.

  Basic skipping looks like this:

    class MyTestCase(unittest.TestCase):

        @unittest.skip("demonstrating skipping")
        def test_nothing(self):
            self.fail("shouldn't happen")

        @unittest.skipIf(mylib.__version__ < (1, 3),
                         "not supported in this library version")
        def test_format(self):
            # Tests that work for only a certain version of the library.
            pass

        @unittest.skipUnless(sys.platform.startswith("win"), "requires Windows")
        def test_windows_support(self):
            # windows specific testing code
            pass

This is the output of running the example above in verbose mode:

    test_format (__main__.MyTestCase) ... skipped 'not supported in this library version'
    test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'
    test_windows_support (__main__.MyTestCase) ... skipped 'requires Windows'

    ----------------------------------------------------------------------
    Ran 3 tests in 0.005s

    OK (skipped=3)

Classes can be skipped just like methods:

    @unittest.skip("showing class skipping")
    class MySkippedTestCase(unittest.TestCase):
        def test_not_run(self):
            pass

*note TestCase.setUp(): 285. can also skip the test.  This is useful
when a resource that needs to be set up is not available.

  Expected failures use the *note expectedFailure(): 2245. decorator.

    class ExpectedFailureTestCase(unittest.TestCase):
        @unittest.expectedFailure
        def test_fail(self):
            self.assertEqual(1, 0, "broken")

It's easy to roll your own skipping decorators by making a decorator
that calls *note skip(): 2244. on the test when it wants it to be
skipped.  This decorator skips the test unless the passed object has a
certain attribute:

    def skipUnlessHasattr(obj, attr):
        if hasattr(obj, attr):
            return lambda func: func
        return unittest.skip("{!r} doesn't have {!r}".format(obj, attr))

The following decorators implement test skipping and expected failures:

 -- Function: unittest.skip (reason)
     Unconditionally skip the decorated test.  _reason_ should describe
     why the test is being skipped.

 -- Function: unittest.skipIf (condition, reason)
     Skip the decorated test if _condition_ is true.

 -- Function: unittest.skipUnless (condition, reason)
     Skip the decorated test unless _condition_ is true.

 -- Function: unittest.expectedFailure ()
     Mark the test as an expected failure.  If the test fails when run,
     the test is not counted as a failure.

 -- Exception: unittest.SkipTest (reason)
     This exception is raised to skip a test.

     Usually you can use *note TestCase.skipTest(): 2248. or one of the
     skipping decorators instead of raising this directly.

  Skipped tests will not have `setUp()' or `tearDown()' run around them.
Skipped classes will not have `setUpClass()' or `tearDownClass()' run.


File: python.info,  Node: Classes and functions,  Next: Class and Module Fixtures,  Prev: Skipping tests and expected failures,  Up: unittest --- Unit testing framework

5.25.3.8 Classes and functions
..............................

This section describes in depth the API of *note unittest: 187.

* Menu:

* Test cases::
* Grouping tests::
* Loading and running tests::

Test cases

* Deprecated aliases::

Loading and running tests

* load_tests Protocol::


File: python.info,  Node: Test cases,  Next: Grouping tests,  Up: Classes and functions

5.25.3.9 Test cases
...................

 -- Class: unittest.TestCase (methodName='runTest')
     Instances of the *note TestCase: 27f. class represent the smallest
     testable units in the *note unittest: 187. universe.  This class
     is intended to be used as a base class, with specific tests being
     implemented by concrete subclasses.  This class implements the
     interface needed by the test runner to allow it to drive the test,
     and methods that the test code can use to check for and report
     various kinds of failure.

     Each instance of *note TestCase: 27f. will run a single test
     method: the method named _methodName_.  If you remember, we had an
     earlier example that went something like this:

         def suite():
             suite = unittest.TestSuite()
             suite.addTest(WidgetTestCase('test_default_size'))
             suite.addTest(WidgetTestCase('test_resize'))
             return suite

     Here, we create two instances of `WidgetTestCase', each of which
     runs a single test.

     _methodName_ defaults to `runTest()'.

     *note TestCase: 27f. instances provide three groups of methods:
     one group used to run the test, another used by the test
     implementation to check conditions and report failures, and some
     inquiry methods allowing information about the test itself to be
     gathered.

     Methods in the first group (running the test) are:

      -- Method: setUp ()
          Method called to prepare the test fixture.  This is called
          immediately before calling the test method; any exception
          raised by this method will be considered an error rather than
          a test failure. The default implementation does nothing.

      -- Method: tearDown ()
          Method called immediately after the test method has been
          called and the result recorded.  This is called even if the
          test method raised an exception, so the implementation in
          subclasses may need to be particularly careful about checking
          internal state.  Any exception raised by this method will be
          considered an error rather than a test failure.  This method
          will only be called if the *note setUp(): 285. succeeds,
          regardless of the outcome of the test method. The default
          implementation does nothing.

      -- Method: setUpClass ()
          A class method called before tests in an individual class run.
          `setUpClass' is called with the class as the only argument
          and must be decorated as a *note classmethod(): 3ef.:

              @classmethod
              def setUpClass(cls):
                  ...

          See *note Class and Module Fixtures: 224d. for more details.

          New in version 2.7.

      -- Method: tearDownClass ()
          A class method called after tests in an individual class have
          run.  `tearDownClass' is called with the class as the only
          argument and must be decorated as a *note classmethod(): 3ef.:

              @classmethod
              def tearDownClass(cls):
                  ...

          See *note Class and Module Fixtures: 224d. for more details.

          New in version 2.7.

      -- Method: run (result=None)
          Run the test, collecting the result into the test result
          object passed as _result_.  If _result_ is omitted or `None',
          a temporary result object is created (by calling the *note
          defaultTestResult(): 224f. method) and used. The result
          object is not returned to *note run(): 224e.'s caller.

          The same effect may be had by simply calling the *note
          TestCase: 27f.  instance.

      -- Method: skipTest (reason)
          Calling this during a test method or *note setUp(): 285.
          skips the current test.  See *note Skipping tests and
          expected failures: 2243. for more information.

          New in version 2.7.

      -- Method: debug ()
          Run the test without collecting the result.  This allows
          exceptions raised by the test to be propagated to the caller,
          and can be used to support running tests under a debugger.
     The *note TestCase: 27f. class provides a number of methods to
     check for and report failures, such as:

     Method                                        Checks that                       New in
     ---------------------------------------------------------------------------------------------------- 
     *note assertEqual(a, b): 27b.                 `a == b'                          
     *note assertNotEqual(a, b): 2251.             `a != b'                          
     *note assertTrue(x): 27c.                     `bool(x) is True'                 
     *note assertFalse(x): 27d.                    `bool(x) is False'                
     *note assertIs(a, b): 289.                    `a is b'                          2.7
     *note assertIsNot(a, b): 28a.                 `a is not b'                      2.7
     *note assertIsNone(x): 287.                   `x is None'                       2.7
     *note assertIsNotNone(x): 288.                `x is not None'                   2.7
     *note assertIn(a, b): 295.                    `a in b'                          2.7
     *note assertNotIn(a, b): 296.                 `a not in b'                      2.7
     *note assertIsInstance(a, b): 28b.            `isinstance(a, b)'                2.7
     *note assertNotIsInstance(a, b): 28c.         `not isinstance(a, b)'            2.7

     All the assert methods (except *note assertRaises(): 280, *note
     assertRaisesRegexp(): 294.)  accept a _msg_ argument that, if
     specified, is used as the error message on failure (see also *note
     longMessage: 27e.).

      -- Method: assertEqual (first, second, msg=None)
          Test that _first_ and _second_ are equal.  If the values do
          not compare equal, the test will fail.

          In addition, if _first_ and _second_ are the exact same type
          and one of list, tuple, dict, set, frozenset or unicode or
          any type that a subclass registers with *note
          addTypeEqualityFunc(): 2a3. the type-specific equality
          function will be called in order to generate a more useful
          default error message (see also the *note list of
          type-specific methods: 2252.).

          Changed in version 2.7: Added the automatic calling of
          type-specific equality function.

      -- Method: assertNotEqual (first, second, msg=None)
          Test that _first_ and _second_ are not equal.  If the values
          do compare equal, the test will fail.

      -- Method: assertTrue (expr, msg=None)
      -- Method: assertFalse (expr, msg=None)
          Test that _expr_ is true (or false).

          Note that this is equivalent to `bool(expr) is True' and not
          to `expr is True' (use `assertIs(expr, True)' for the
          latter).  This method should also be avoided when more
          specific methods are available (e.g.  `assertEqual(a, b)'
          instead of `assertTrue(a == b)'), because they provide a
          better error message in case of failure.

      -- Method: assertIs (first, second, msg=None)
      -- Method: assertIsNot (first, second, msg=None)
          Test that _first_ and _second_ evaluate (or don't evaluate)
          to the same object.

          New in version 2.7.

      -- Method: assertIsNone (expr, msg=None)
      -- Method: assertIsNotNone (expr, msg=None)
          Test that _expr_ is (or is not) None.

          New in version 2.7.

      -- Method: assertIn (first, second, msg=None)
      -- Method: assertNotIn (first, second, msg=None)
          Test that _first_ is (or is not) in _second_.

          New in version 2.7.

      -- Method: assertIsInstance (obj, cls, msg=None)
      -- Method: assertNotIsInstance (obj, cls, msg=None)
          Test that _obj_ is (or is not) an instance of _cls_ (which
          can be a class or a tuple of classes, as supported by *note
          isinstance(): 317.).  To check for the exact type, use *note
          assertIs(type(obj), cls): 289.

          New in version 2.7.

     It is also possible to check that exceptions and warnings are
     raised using the following methods:

     Method                                                        Checks that                                New in
     -------------------------------------------------------------------------------------------------------------------------- 
     *note assertRaises(exc, fun, *args, **kwds): 280.             `fun(*args, **kwds)' raises _exc_          
     *note assertRaisesRegexp(exc, re, fun, *args, **kwds): 294.   `fun(*args, **kwds)' raises _exc_ and the  2.7
                                                                   message matches _re_                       

      -- Method: assertRaises (exception, callable, *args, **kwds)
      -- Method: assertRaises (exception)
          Test that an exception is raised when _callable_ is called
          with any positional or keyword arguments that are also passed
          to *note assertRaises(): 280.  The test passes if _exception_
          is raised, is an error if another exception is raised, or
          fails if no exception is raised.  To catch any of a group of
          exceptions, a tuple containing the exception classes may be
          passed as _exception_.

          If only the _exception_ argument is given, returns a context
          manager so that the code under test can be written inline
          rather than as a function:

              with self.assertRaises(SomeException):
                  do_something()

          The context manager will store the caught exception object in
          its `exception' attribute.  This can be useful if the
          intention is to perform additional checks on the exception
          raised:

              with self.assertRaises(SomeException) as cm:
                  do_something()

              the_exception = cm.exception
              self.assertEqual(the_exception.error_code, 3)

          Changed in version 2.7: Added the ability to use *note
          assertRaises(): 280. as a context manager.

      -- Method: assertRaisesRegexp (exception, regexp, callable,
               *args, **kwds)
      -- Method: assertRaisesRegexp (exception, regexp)
          Like *note assertRaises(): 280. but also tests that _regexp_
          matches on the string representation of the raised exception.
          _regexp_ may be a regular expression object or a string
          containing a regular expression suitable for use by *note
          re.search(): 9af.  Examples:

              self.assertRaisesRegexp(ValueError, 'invalid literal for.*XYZ$',
                                      int, 'XYZ')

          or:

              with self.assertRaisesRegexp(ValueError, 'literal'):
                 int('XYZ')

          New in version 2.7.

     There are also other methods used to perform more specific checks,
     such as:

     Method                                      Checks that                          New in
     ---------------------------------------------------------------------------------------------------- 
     *note assertAlmostEqual(a, b): 29e.         `round(a-b, 7) == 0'                 
     *note assertNotAlmostEqual(a, b): 29f.      `round(a-b, 7) != 0'                 
     *note assertGreater(a, b): 28d.             `a > b'                              2.7
     *note assertGreaterEqual(a, b): 28e.        `a >= b'                             2.7
     *note assertLess(a, b): 28f.                `a < b'                              2.7
     *note assertLessEqual(a, b): 290.           `a <= b'                             2.7
     *note assertRegexpMatches(s, re): 292.      `regex.search(s)'                    2.7
     *note assertNotRegexpMatches(s, re): 293.   `not regex.search(s)'                2.7
     *note assertItemsEqual(a, b): 297.          sorted(a) == sorted(b) and works     2.7
                                                 with unhashable objs                 
     *note assertDictContainsSubset(a, b): 29d.  all the key/value pairs in _a_       2.7
                                                 exist in _b_                         

      -- Method: assertAlmostEqual (first, second, places=7, msg=None,
               delta=None)
      -- Method: assertNotAlmostEqual (first, second, places=7,
               msg=None, delta=None)
          Test that _first_ and _second_ are approximately (or not
          approximately) equal by computing the difference, rounding to
          the given number of decimal _places_ (default 7), and
          comparing to zero.  Note that these methods round the values
          to the given number of _decimal places_ (i.e.  like the *note
          round(): 1c2. function) and not _significant digits_.

          If _delta_ is supplied instead of _places_ then the difference
          between _first_ and _second_ must be less (or more) than
          _delta_.

          Supplying both _delta_ and _places_ raises a `TypeError'.

          Changed in version 2.7: *note assertAlmostEqual(): 29e.
          automatically considers almost equal objects that compare
          equal.  *note assertNotAlmostEqual(): 29f. automatically fails
          if the objects compare equal.  Added the _delta_ keyword
          argument.

      -- Method: assertGreater (first, second, msg=None)
      -- Method: assertGreaterEqual (first, second, msg=None)
      -- Method: assertLess (first, second, msg=None)
      -- Method: assertLessEqual (first, second, msg=None)
          Test that _first_ is respectively >, >=, < or <= than
          _second_ depending on the method name.  If not, the test will
          fail:

              >>> self.assertGreaterEqual(3, 4)
              AssertionError: "3" unexpectedly not greater than or equal to "4"

          New in version 2.7.

      -- Method: assertRegexpMatches (text, regexp, msg=None)
          Test that a _regexp_ search matches _text_.  In case of
          failure, the error message will include the pattern and the
          _text_ (or the pattern and the part of _text_ that
          unexpectedly matched).  _regexp_ may be a regular expression
          object or a string containing a regular expression suitable
          for use by *note re.search(): 9af.

          New in version 2.7.

      -- Method: assertNotRegexpMatches (text, regexp, msg=None)
          Verifies that a _regexp_ search does not match _text_.  Fails
          with an error message including the pattern and the part of
          _text_ that matches.  _regexp_ may be a regular expression
          object or a string containing a regular expression suitable
          for use by *note re.search(): 9af.

          New in version 2.7.

      -- Method: assertItemsEqual (actual, expected, msg=None)
          Test that sequence _expected_ contains the same elements as
          _actual_, regardless of their order. When they don't, an
          error message listing the differences between the sequences
          will be generated.

          Duplicate elements are _not_ ignored when comparing _actual_
          and _expected_. It verifies if each element has the same
          count in both sequences. It is the equivalent of
          `assertEqual(sorted(expected), sorted(actual))' but it works
          with sequences of unhashable objects as well.

          In Python 3, this method is named `assertCountEqual'.

          New in version 2.7.

      -- Method: assertDictContainsSubset (expected, actual, msg=None)
          Tests whether the key/value pairs in dictionary _actual_ are a
          superset of those in _expected_.  If not, an error message
          listing the missing keys and mismatched values is generated.

          New in version 2.7.

          Deprecated since version 3.2.
     The *note assertEqual(): 27b. method dispatches the equality check
     for objects of the same type to different type-specific methods.
     These methods are already implemented for most of the built-in
     types, but it's also possible to register new methods using *note
     addTypeEqualityFunc(): 2a3.:

      -- Method: addTypeEqualityFunc (typeobj, function)
          Registers a type-specific method called by *note
          assertEqual(): 27b. to check if two objects of exactly the
          same _typeobj_ (not subclasses) compare equal.  _function_
          must take two positional arguments and a third msg=None
          keyword argument just as *note assertEqual(): 27b. does.  It
          must raise *note self.failureException(msg): 2253. when
          inequality between the first two parameters is detected -
          possibly providing useful information and explaining the
          inequalities in details in the error message.

          New in version 2.7.

     The list of type-specific methods automatically used by *note
     assertEqual(): 27b. are summarized in the following table.  Note
     that it's usually not necessary to invoke these methods directly.

     Method                                        Used to compare                   New in
     --------------------------------------------------------------------------------------------------- 
     *note assertMultiLineEqual(a, b): 291.        strings                           2.7
     *note assertSequenceEqual(a, b): 29b.         sequences                         2.7
     *note assertListEqual(a, b): 299.             lists                             2.7
     *note assertTupleEqual(a, b): 29a.            tuples                            2.7
     *note assertSetEqual(a, b): 298.              sets or frozensets                2.7
     *note assertDictEqual(a, b): 29c.             dicts                             2.7

      -- Method: assertMultiLineEqual (first, second, msg=None)
          Test that the multiline string _first_ is equal to the string
          _second_.  When not equal a diff of the two strings
          highlighting the differences will be included in the error
          message. This method is used by default when comparing
          strings with *note assertEqual(): 27b.

          New in version 2.7.

      -- Method: assertSequenceEqual (seq1, seq2, msg=None,
               seq_type=None)
          Tests that two sequences are equal.  If a _seq_type_ is
          supplied, both _seq1_ and _seq2_ must be instances of
          _seq_type_ or a failure will be raised.  If the sequences are
          different an error message is constructed that shows the
          difference between the two.

          This method is not called directly by *note assertEqual():
          27b, but it's used to implement *note assertListEqual(): 299.
          and *note assertTupleEqual(): 29a.

          New in version 2.7.

      -- Method: assertListEqual (list1, list2, msg=None)
      -- Method: assertTupleEqual (tuple1, tuple2, msg=None)
          Tests that two lists or tuples are equal.  If not, an error
          message is constructed that shows only the differences
          between the two.  An error is also raised if either of the
          parameters are of the wrong type.  These methods are used by
          default when comparing lists or tuples with *note
          assertEqual(): 27b.

          New in version 2.7.

      -- Method: assertSetEqual (set1, set2, msg=None)
          Tests that two sets are equal.  If not, an error message is
          constructed that lists the differences between the sets.
          This method is used by default when comparing sets or
          frozensets with *note assertEqual(): 27b.

          Fails if either of _set1_ or _set2_ does not have a *note
          set.difference(): 8d2.  method.

          New in version 2.7.

      -- Method: assertDictEqual (expected, actual, msg=None)
          Test that two dictionaries are equal.  If not, an error
          message is constructed that shows the differences in the
          dictionaries. This method will be used by default to compare
          dictionaries in calls to *note assertEqual(): 27b.

          New in version 2.7.
     Finally the *note TestCase: 27f. provides the following methods
     and attributes:

      -- Method: fail (msg=None)
          Signals a test failure unconditionally, with _msg_ or `None'
          for the error message.

      -- Attribute: failureException
          This class attribute gives the exception raised by the test
          method.  If a test framework needs to use a specialized
          exception, possibly to carry additional information, it must
          subclass this exception in order to "play fair" with the
          framework.  The initial value of this attribute is *note
          AssertionError: 7f7.

      -- Attribute: longMessage
          If set to `True' then any explicit failure message you pass
          in to the *note assert methods: 222a. will be appended to the
          end of the normal failure message.  The normal messages
          contain useful information about the objects involved, for
          example the message from assertEqual shows you the repr of
          the two unequal objects. Setting this attribute to `True'
          allows you to have a custom error message in addition to the
          normal one.

          This attribute defaults to `False', meaning that a custom
          message passed to an assert method will silence the normal
          message.

          The class setting can be overridden in individual tests by
          assigning an instance attribute to `True' or `False' before
          calling the assert methods.

          New in version 2.7.

      -- Attribute: maxDiff
          This attribute controls the maximum length of diffs output by
          assert methods that report diffs on failure. It defaults to
          80*8 characters.  Assert methods affected by this attribute
          are *note assertSequenceEqual(): 29b. (including all the
          sequence comparison methods that delegate to it), *note
          assertDictEqual(): 29c. and *note assertMultiLineEqual(): 291.

          Setting `maxDiff' to None means that there is no maximum
          length of diffs.

          New in version 2.7.

     Testing frameworks can use the following methods to collect
     information on the test:

      -- Method: countTestCases ()
          Return the number of tests represented by this test object.
          For *note TestCase: 27f. instances, this will always be `1'.

      -- Method: defaultTestResult ()
          Return an instance of the test result class that should be
          used for this test case class (if no other result instance is
          provided to the *note run(): 224e. method).

          For *note TestCase: 27f. instances, this will always be an
          instance of *note TestResult: 2a5.; subclasses of *note
          TestCase: 27f. should override this as necessary.

      -- Method: id ()
          Return a string identifying the specific test case.  This is
          usually the full name of the test method, including the
          module and class name.

      -- Method: shortDescription ()
          Returns a description of the test, or `None' if no description
          has been provided.  The default implementation of this method
          returns the first line of the test method's docstring, if
          available, or *note None: 393.

      -- Method: addCleanup (function, *args, **kwargs)
          Add a function to be called after *note tearDown(): 286. to
          cleanup resources used during the test. Functions will be
          called in reverse order to the order they are added (LIFO).
          They are called with any arguments and keyword arguments
          passed into *note addCleanup(): 283. when they are added.

          If *note setUp(): 285. fails, meaning that *note tearDown():
          286. is not called, then any cleanup functions added will
          still be called.

          New in version 2.7.

      -- Method: doCleanups ()
          This method is called unconditionally after *note tearDown():
          286, or after *note setUp(): 285. if *note setUp(): 285.
          raises an exception.

          It is responsible for calling all the cleanup functions added
          by *note addCleanup(): 283. If you need cleanup functions to
          be called _prior_ to *note tearDown(): 286. then you can call
          *note doCleanups(): 284.  yourself.

          *note doCleanups(): 284. pops methods off the stack of cleanup
          functions one at a time, so it can be called at any time.

          New in version 2.7.

 -- Class: unittest.FunctionTestCase (testFunc, setUp=None,
          tearDown=None, description=None)
     This class implements the portion of the *note TestCase: 27f.
     interface which allows the test runner to drive the test, but does
     not provide the methods which test code can use to check and
     report errors.  This is used to create test cases using legacy
     test code, allowing it to be integrated into a *note unittest:
     187.-based test framework.

* Menu:

* Deprecated aliases::


File: python.info,  Node: Deprecated aliases,  Up: Test cases

5.25.3.10 Deprecated aliases
............................

For historical reasons, some of the *note TestCase: 27f. methods had
one or more aliases that are now deprecated.  The following table lists
the correct names along with their deprecated aliases:

      Method Name                        Deprecated alias(es)
     ----------------------------------------------------------------------- 
     *note assertEqual(): 27b.          failUnlessEqual, assertEquals
     *note assertNotEqual(): 2251.      failIfEqual
     *note assertTrue(): 27c.           failUnless, assert_
     *note assertFalse(): 27d.          failIf
     *note assertRaises(): 280.         failUnlessRaises
     *note assertAlmostEqual(): 29e.    failUnlessAlmostEqual
     *note assertNotAlmostEqual(): 29f. failIfAlmostEqual

     Deprecated since version 2.7: the aliases listed in the second
     column


File: python.info,  Node: Grouping tests,  Next: Loading and running tests,  Prev: Test cases,  Up: Classes and functions

5.25.3.11 Grouping tests
........................

 -- Class: unittest.TestSuite (tests=())
     This class represents an aggregation of individual tests cases and
     test suites.  The class presents the interface needed by the test
     runner to allow it to be run as any other test case.  Running a
     *note TestSuite: 456. instance is the same as iterating over the
     suite, running each test individually.

     If _tests_ is given, it must be an iterable of individual test
     cases or other test suites that will be used to build the suite
     initially. Additional methods are provided to add test cases and
     suites to the collection later on.

     *note TestSuite: 456. objects behave much like *note TestCase:
     27f. objects, except they do not actually implement a test.
     Instead, they are used to aggregate tests into groups of tests
     that should be run together. Some additional methods are available
     to add tests to *note TestSuite: 456. instances:

      -- Method: addTest (test)
          Add a *note TestCase: 27f. or *note TestSuite: 456. to the
          suite.

      -- Method: addTests (tests)
          Add all the tests from an iterable of *note TestCase: 27f.
          and *note TestSuite: 456.  instances to this test suite.

          This is equivalent to iterating over _tests_, calling *note
          addTest(): 225d. for each element.

     *note TestSuite: 456. shares the following methods with *note
     TestCase: 27f.:

      -- Method: run (result)
          Run the tests associated with this suite, collecting the
          result into the test result object passed as _result_.  Note
          that unlike *note TestCase.run(): 224e, *note
          TestSuite.run(): 225f. requires the result object to be
          passed in.

      -- Method: debug ()
          Run the tests associated with this suite without collecting
          the result. This allows exceptions raised by the test to be
          propagated to the caller and can be used to support running
          tests under a debugger.

      -- Method: countTestCases ()
          Return the number of tests represented by this test object,
          including all individual tests and sub-suites.

      -- Method: __iter__ ()
          Tests grouped by a *note TestSuite: 456. are always accessed
          by iteration.  Subclasses can lazily provide tests by
          overriding *note __iter__(): 2262. Note that this method
          maybe called several times on a single suite (for example
          when counting tests or comparing for equality) so the tests
          returned must be the same for repeated iterations.

          Changed in version 2.7: In earlier versions the *note
          TestSuite: 456. accessed tests directly rather than through
          iteration, so overriding *note __iter__(): 2262. wasn't
          sufficient for providing tests.

     In the typical usage of a *note TestSuite: 456. object, the *note
     run(): 225f. method is invoked by a `TestRunner' rather than by
     the end-user test harness.


File: python.info,  Node: Loading and running tests,  Prev: Grouping tests,  Up: Classes and functions

5.25.3.12 Loading and running tests
...................................

 -- Class: unittest.TestLoader
     The *note TestLoader: 2a2. class is used to create test suites
     from classes and modules.  Normally, there is no need to create an
     instance of this class; the *note unittest: 187. module provides
     an instance that can be shared as *note
     unittest.defaultTestLoader: 2264.  Using a subclass or instance,
     however, allows customization of some configurable properties.

     *note TestLoader: 2a2. objects have the following methods:

      -- Method: loadTestsFromTestCase (testCaseClass)
          Return a suite of all tests cases contained in the *note
          TestCase: 27f.-derived `testCaseClass'.

      -- Method: loadTestsFromModule (module)
          Return a suite of all tests cases contained in the given
          module. This method searches _module_ for classes derived
          from *note TestCase: 27f. and creates an instance of the
          class for each test method defined for the class.

               Note: While using a hierarchy of *note TestCase:
               27f.-derived classes can be convenient in sharing
               fixtures and helper functions, defining test methods on
               base classes that are not intended to be instantiated
               directly does not play well with this method.  Doing so,
               however, can be useful when the fixtures are different
               and defined in subclasses.

          If a module provides a `load_tests' function it will be
          called to load the tests. This allows modules to customize
          test loading.  This is the *note load_tests protocol: 223d.

          Changed in version 2.7: Support for `load_tests' added.

      -- Method: loadTestsFromName (name, module=None)
          Return a suite of all tests cases given a string specifier.

          The specifier _name_ is a "dotted name" that may resolve
          either to a module, a test case class, a test method within a
          test case class, a *note TestSuite: 456. instance, or a
          callable object which returns a *note TestCase: 27f. or *note
          TestSuite: 456. instance.  These checks are applied in the
          order listed here; that is, a method on a possible test case
          class will be picked up as "a test method within a test case
          class", rather than "a callable object".

          For example, if you have a module `SampleTests' containing a
          *note TestCase: 27f.-derived class `SampleTestCase' with
          three test methods (`test_one()', `test_two()', and
          `test_three()'), the specifier `'SampleTests.SampleTestCase''
          would cause this method to return a suite which will run all
          three test methods. Using the specifier
          `'SampleTests.SampleTestCase.test_two'' would cause it to
          return a test suite which will run only the `test_two()' test
          method. The specifier can refer to modules and packages which
          have not been imported; they will be imported as a
          side-effect.

          The method optionally resolves _name_ relative to the given
          _module_.

      -- Method: loadTestsFromNames (names, module=None)
          Similar to *note loadTestsFromName(): 2a0, but takes a
          sequence of names rather than a single name.  The return
          value is a test suite which supports all the tests defined
          for each name.

      -- Method: getTestCaseNames (testCaseClass)
          Return a sorted sequence of method names found within
          _testCaseClass_; this should be a subclass of *note TestCase:
          27f.

      -- Method: discover (start_dir, pattern='test*.py',
               top_level_dir=None)
          Find and return all test modules from the specified start
          directory, recursing into subdirectories to find them. Only
          test files that match _pattern_ will be loaded. (Using shell
          style pattern matching.) Only module names that are
          importable (i.e. are valid Python identifiers) will be loaded.

          All test modules must be importable from the top level of the
          project. If the start directory is not the top level
          directory then the top level directory must be specified
          separately.

          If importing a module fails, for example due to a syntax
          error, then this will be recorded as a single error and
          discovery will continue.

          If a test package name (directory with `__init__.py') matches
          the pattern then the package will be checked for a
          `load_tests' function. If this exists then it will be called
          with _loader_, _tests_, _pattern_.

          If load_tests exists then discovery does _not_ recurse into
          the package, `load_tests' is responsible for loading all
          tests in the package.

          The pattern is deliberately not stored as a loader attribute
          so that packages can continue discovery themselves.
          _top_level_dir_ is stored so `load_tests' does not need to
          pass this argument in to `loader.discover()'.

          _start_dir_ can be a dotted module name as well as a
          directory.

          New in version 2.7.

     The following attributes of a *note TestLoader: 2a2. can be
     configured either by subclassing or assignment on an instance:

      -- Attribute: testMethodPrefix
          String giving the prefix of method names which will be
          interpreted as test methods.  The default value is `'test''.

          This affects *note getTestCaseNames(): 2268. and all the
          `loadTestsFrom*()' methods.

      -- Attribute: sortTestMethodsUsing
          Function to be used to compare method names when sorting them
          in *note getTestCaseNames(): 2268. and all the
          `loadTestsFrom*()' methods. The default value is the built-in
          *note cmp(): 4b5. function; the attribute can also be set to
          *note None: 393. to disable the sort.

      -- Attribute: suiteClass
          Callable object that constructs a test suite from a list of
          tests. No methods on the resulting object are needed.  The
          default value is the *note TestSuite: 456. class.

          This affects all the `loadTestsFrom*()' methods.

 -- Class: unittest.TestResult
     This class is used to compile information about which tests have
     succeeded and which have failed.

     A *note TestResult: 2a5. object stores the results of a set of
     tests.  The *note TestCase: 27f. and *note TestSuite: 456. classes
     ensure that results are properly recorded; test authors do not
     need to worry about recording the outcome of tests.

     Testing frameworks built on top of *note unittest: 187. may want
     access to the *note TestResult: 2a5. object generated by running a
     set of tests for reporting purposes; a *note TestResult: 2a5.
     instance is returned by the `TestRunner.run()' method for this
     purpose.

     *note TestResult: 2a5. instances have the following attributes
     that will be of interest when inspecting the results of running a
     set of tests:

      -- Attribute: errors
          A list containing 2-tuples of *note TestCase: 27f. instances
          and strings holding formatted tracebacks. Each tuple
          represents a test which raised an unexpected exception.

          Changed in version 2.2: Contains formatted tracebacks instead
          of *note sys.exc_info(): 2ec. results.

      -- Attribute: failures
          A list containing 2-tuples of *note TestCase: 27f. instances
          and strings holding formatted tracebacks. Each tuple
          represents a test where a failure was explicitly signalled
          using the `TestCase.fail*()' or `TestCase.assert*()' methods.

          Changed in version 2.2: Contains formatted tracebacks instead
          of *note sys.exc_info(): 2ec. results.

      -- Attribute: skipped
          A list containing 2-tuples of *note TestCase: 27f. instances
          and strings holding the reason for skipping the test.

          New in version 2.7.

      -- Attribute: expectedFailures
          A list containing 2-tuples of *note TestCase: 27f. instances
          and strings holding formatted tracebacks.  Each tuple
          represents an expected failure of the test case.

      -- Attribute: unexpectedSuccesses
          A list containing *note TestCase: 27f. instances that were
          marked as expected failures, but succeeded.

      -- Attribute: shouldStop
          Set to `True' when the execution of tests should stop by
          *note stop(): 2271.

      -- Attribute: testsRun
          The total number of tests run so far.

      -- Attribute: buffer
          If set to true, `sys.stdout' and `sys.stderr' will be
          buffered in between *note startTest(): 2274. and *note
          stopTest(): 2275. being called. Collected output will only be
          echoed onto the real `sys.stdout' and `sys.stderr' if the test
          fails or errors. Any output is also attached to the failure /
          error message.

          New in version 2.7.

      -- Attribute: failfast
          If set to true *note stop(): 2271. will be called on the
          first failure or error, halting the test run.

          New in version 2.7.

      -- Method: wasSuccessful ()
          Return `True' if all tests run so far have passed, otherwise
          returns `False'.

      -- Method: stop ()
          This method can be called to signal that the set of tests
          being run should be aborted by setting the *note shouldStop:
          2270. attribute to `True'.  `TestRunner' objects should
          respect this flag and return without running any additional
          tests.

          For example, this feature is used by the *note
          TextTestRunner: 222c. class to stop the test framework when
          the user signals an interrupt from the keyboard.  Interactive
          tools which provide `TestRunner' implementations can use this
          in a similar manner.

     The following methods of the *note TestResult: 2a5. class are used
     to maintain the internal data structures, and may be extended in
     subclasses to support additional reporting requirements.  This is
     particularly useful in building tools which support interactive
     reporting while tests are being run.

      -- Method: startTest (test)
          Called when the test case _test_ is about to be run.

      -- Method: stopTest (test)
          Called after the test case _test_ has been executed,
          regardless of the outcome.

      -- Method: startTestRun (test)
          Called once before any tests are executed.

          New in version 2.7.

      -- Method: stopTestRun (test)
          Called once after all tests are executed.

          New in version 2.7.

      -- Method: addError (test, err)
          Called when the test case _test_ raises an unexpected
          exception _err_ is a tuple of the form returned by *note
          sys.exc_info(): 2ec.: `(type, value, traceback)'.

          The default implementation appends a tuple `(test,
          formatted_err)' to the instance's *note errors: 226b.
          attribute, where _formatted_err_ is a formatted traceback
          derived from _err_.

      -- Method: addFailure (test, err)
          Called when the test case _test_ signals a failure. _err_ is
          a tuple of the form returned by *note sys.exc_info(): 2ec.:
          `(type, value, traceback)'.

          The default implementation appends a tuple `(test,
          formatted_err)' to the instance's *note failures: 226c.
          attribute, where _formatted_err_ is a formatted traceback
          derived from _err_.

      -- Method: addSuccess (test)
          Called when the test case _test_ succeeds.

          The default implementation does nothing.

      -- Method: addSkip (test, reason)
          Called when the test case _test_ is skipped.  _reason_ is the
          reason the test gave for skipping.

          The default implementation appends a tuple `(test, reason)'
          to the instance's *note skipped: 226d. attribute.

      -- Method: addExpectedFailure (test, err)
          Called when the test case _test_ fails, but was marked with
          the *note expectedFailure(): 2245. decorator.

          The default implementation appends a tuple `(test,
          formatted_err)' to the instance's *note expectedFailures:
          226e. attribute, where _formatted_err_ is a formatted
          traceback derived from _err_.

      -- Method: addUnexpectedSuccess (test)
          Called when the test case _test_ was marked with the *note
          expectedFailure(): 2245. decorator, but succeeded.

          The default implementation appends the test to the instance's
          *note unexpectedSuccesses: 226f. attribute.

 -- Class: unittest.TextTestResult (stream, descriptions, verbosity)
     A concrete implementation of *note TestResult: 2a5. used by the
     *note TextTestRunner: 222c.

     New in version 2.7: This class was previously named
     `_TextTestResult'. The old name still exists as an alias but is
     deprecated.

 -- Data: unittest.defaultTestLoader
     Instance of the *note TestLoader: 2a2. class intended to be
     shared.  If no customization of the *note TestLoader: 2a2. is
     needed, this instance can be used instead of repeatedly creating
     new instances.

 -- Class: unittest.TextTestRunner (stream=sys.stderr,
          descriptions=True, verbosity=1)
     A basic test runner implementation which prints results on
     standard error.  It has a few configurable parameters, but is
     essentially very simple.  Graphical applications which run test
     suites should provide alternate implementations.

      -- Method: _makeResult ()
          This method returns the instance of `TestResult' used by
          `run()'.  It is not intended to be called directly, but can
          be overridden in subclasses to provide a custom `TestResult'.

          `_makeResult()' instantiates the class or callable passed in
          the `TextTestRunner' constructor as the `resultclass'
          argument. It defaults to *note TextTestResult: 227e. if no
          `resultclass' is provided.  The result class is instantiated
          with the following arguments:

              stream, descriptions, verbosity



 -- Function: unittest.main ([module[, defaultTest[, argv[,
          testRunner[, testLoader[, exit[, verbosity[, failfast[,
          catchbreak[, buffer]]]]]]]]]])
     A command-line program that loads a set of tests from _module_ and
     runs them; this is primarily for making test modules conveniently
     executable.  The simplest use for this function is to include the
     following line at the end of a test script:

         if __name__ == '__main__':
             unittest.main()

     You can run tests with more detailed information by passing in the
     verbosity argument:

         if __name__ == '__main__':
             unittest.main(verbosity=2)

     The _argv_ argument can be a list of options passed to the
     program, with the first element being the program name.  If not
     specified or `None', the values of *note sys.argv: 621. are used.

     The _testRunner_ argument can either be a test runner class or an
     already created instance of it. By default `main' calls *note
     sys.exit(): 2a4. with an exit code indicating success or failure
     of the tests run.

     The _testLoader_ argument has to be a *note TestLoader: 2a2.
     instance, and defaults to *note defaultTestLoader: 2264.

     `main' supports being used from the interactive interpreter by
     passing in the argument `exit=False'. This displays the result on
     standard output without calling *note sys.exit(): 2a4.:

         >>> from unittest import main
         >>> main(module='test_module', exit=False)

     The _failfast_, _catchbreak_ and _buffer_ parameters have the same
     effect as the same-name *note command-line options: 2231.

     Calling `main' actually returns an instance of the `TestProgram'
     class.  This stores the result of the tests run as the `result'
     attribute.

     Changed in version 2.7: The _exit_, _verbosity_, _failfast_,
     _catchbreak_ and _buffer_ parameters were added.

* Menu:

* load_tests Protocol::


File: python.info,  Node: load_tests Protocol,  Up: Loading and running tests

5.25.3.13 load_tests Protocol
.............................

New in version 2.7.

  Modules or packages can customize how tests are loaded from them
during normal test runs or test discovery by implementing a function
called `load_tests'.

  If a test module defines `load_tests' it will be called by *note
TestLoader.loadTestsFromModule(): 2266. with the following arguments:

    load_tests(loader, standard_tests, None)

It should return a *note TestSuite: 456.

  _loader_ is the instance of *note TestLoader: 2a2. doing the loading.
_standard_tests_ are the tests that would be loaded by default from the
module. It is common for test modules to only want to add or remove
tests from the standard set of tests.  The third argument is used when
loading packages as part of test discovery.

  A typical `load_tests' function that loads tests from a specific set
of *note TestCase: 27f. classes may look like:

    test_cases = (TestCase1, TestCase2, TestCase3)

    def load_tests(loader, tests, pattern):
        suite = TestSuite()
        for test_class in test_cases:
            tests = loader.loadTestsFromTestCase(test_class)
            suite.addTests(tests)
        return suite

If discovery is started, either from the command line or by calling
*note TestLoader.discover(): 2238, with a pattern that matches a package
name then the package `__init__.py' will be checked for `load_tests'.

     Note: The default pattern is `'test*.py''. This matches all Python
     files that start with `'test'' but _won't_ match any test
     directories.

     A pattern like `'test*'' will match test packages as well as
     modules.

  If the package `__init__.py' defines `load_tests' then it will be
called and discovery not continued into the package. `load_tests' is
called with the following arguments:

    load_tests(loader, standard_tests, pattern)

This should return a *note TestSuite: 456. representing all the tests
from the package. (`standard_tests' will only contain tests collected
from `__init__.py'.)

  Because the pattern is passed into `load_tests' the package is free to
continue (and potentially modify) test discovery. A 'do nothing'
`load_tests' function for a test package would look like:

    def load_tests(loader, standard_tests, pattern):
        # top level directory cached on loader instance
        this_dir = os.path.dirname(__file__)
        package_tests = loader.discover(start_dir=this_dir, pattern=pattern)
        standard_tests.addTests(package_tests)
        return standard_tests



File: python.info,  Node: Class and Module Fixtures,  Next: Signal Handling,  Prev: Classes and functions,  Up: unittest --- Unit testing framework

5.25.3.14 Class and Module Fixtures
...................................

Class and module level fixtures are implemented in *note TestSuite:
456. When the test suite encounters a test from a new class then
`tearDownClass()' from the previous class (if there is one) is called,
followed by `setUpClass()' from the new class.

  Similarly if a test is from a different module from the previous test
then `tearDownModule' from the previous module is run, followed by
`setUpModule' from the new module.

  After all the tests have run the final `tearDownClass' and
`tearDownModule' are run.

  Note that shared fixtures do not play well with [potential] features
like test parallelization and they break test isolation. They should be
used with care.

  The default ordering of tests created by the unittest test loaders is
to group all tests from the same modules and classes together. This
will lead to `setUpClass' / `setUpModule' (etc) being called exactly
once per class and module. If you randomize the order, so that tests
from different modules and classes are adjacent to each other, then
these shared fixture functions may be called multiple times in a single
test run.

  Shared fixtures are not intended to work with suites with non-standard
ordering. A `BaseTestSuite' still exists for frameworks that don't want
to support shared fixtures.

  If there are any exceptions raised during one of the shared fixture
functions the test is reported as an error. Because there is no
corresponding test instance an `_ErrorHolder' object (that has the same
interface as a *note TestCase: 27f.) is created to represent the error.
If you are just using the standard unittest test runner then this
detail doesn't matter, but if you are a framework author it may be
relevant.

* Menu:

* setUpClass and tearDownClass::
* setUpModule and tearDownModule::


File: python.info,  Node: setUpClass and tearDownClass,  Next: setUpModule and tearDownModule,  Up: Class and Module Fixtures

5.25.3.15 setUpClass and tearDownClass
......................................

These must be implemented as class methods:

    import unittest

    class Test(unittest.TestCase):
        @classmethod
        def setUpClass(cls):
            cls._connection = createExpensiveConnectionObject()

        @classmethod
        def tearDownClass(cls):
            cls._connection.destroy()

If you want the `setUpClass' and `tearDownClass' on base classes called
then you must call up to them yourself. The implementations in *note
TestCase: 27f. are empty.

  If an exception is raised during a `setUpClass' then the tests in the
class are not run and the `tearDownClass' is not run. Skipped classes
will not have `setUpClass' or `tearDownClass' run. If the exception is a
*note SkipTest: 27a. exception then the class will be reported as
having been skipped instead of as an error.


File: python.info,  Node: setUpModule and tearDownModule,  Prev: setUpClass and tearDownClass,  Up: Class and Module Fixtures

5.25.3.16 setUpModule and tearDownModule
........................................

These should be implemented as functions:

    def setUpModule():
        createConnection()

    def tearDownModule():
        closeConnection()

If an exception is raised in a `setUpModule' then none of the tests in
the module will be run and the `tearDownModule' will not be run. If the
exception is a *note SkipTest: 27a. exception then the module will be
reported as having been skipped instead of as an error.


File: python.info,  Node: Signal Handling,  Prev: Class and Module Fixtures,  Up: unittest --- Unit testing framework

5.25.3.17 Signal Handling
.........................

The *note -c/-catch: 2233. command-line option to unittest, along with
the `catchbreak' parameter to *note unittest.main(): 277, provide more
friendly handling of control-C during a test run. With catch break
behavior enabled control-C will allow the currently running test to
complete, and the test run will then end and report all the results so
far. A second control-c will raise a *note KeyboardInterrupt: 24e. in
the usual way.

  The control-c handling signal handler attempts to remain compatible
with code or tests that install their own `signal.SIGINT' handler. If
the `unittest' handler is called but _isn't_ the installed
`signal.SIGINT' handler, i.e. it has been replaced by the system under
test and delegated to, then it calls the default handler. This will
normally be the expected behavior by code that replaces an installed
handler and delegates to it. For individual tests that need `unittest'
control-c handling disabled the *note removeHandler(): 279.  decorator
can be used.

  There are a few utility functions for framework authors to enable
control-c handling functionality within test frameworks.

 -- Function: unittest.installHandler ()
     Install the control-c handler. When a `signal.SIGINT' is received
     (usually in response to the user pressing control-c) all
     registered results have *note stop(): 2271. called.

     New in version 2.7.

 -- Function: unittest.registerResult (result)
     Register a *note TestResult: 2a5. object for control-c handling.
     Registering a result stores a weak reference to it, so it doesn't
     prevent the result from being garbage collected.

     Registering a *note TestResult: 2a5. object has no side-effects if
     control-c handling is not enabled, so test frameworks can
     unconditionally register all results they create independently of
     whether or not handling is enabled.

     New in version 2.7.

 -- Function: unittest.removeResult (result)
     Remove a registered result. Once a result has been removed then
     *note stop(): 2271. will no longer be called on that result object
     in response to a control-c.

     New in version 2.7.

 -- Function: unittest.removeHandler (function=None)
     When called without arguments this function removes the control-c
     handler if it has been installed. This function can also be used
     as a test decorator to temporarily remove the handler whilst the
     test is being executed:

         @unittest.removeHandler
         def test_signal_handling(self):
             ...

     New in version 2.7.


File: python.info,  Node: 2to3 - Automated Python 2 to 3 code translation,  Next: test --- Regression tests package for Python,  Prev: unittest --- Unit testing framework,  Up: Development Tools

5.25.4 2to3 - Automated Python 2 to 3 code translation
------------------------------------------------------

2to3 is a Python program that reads Python 2.x source code and applies
a series of _fixers_ to transform it into valid Python 3.x code.  The
standard library contains a rich set of fixers that will handle almost
all code.  2to3 supporting library *note lib2to3: fe. is, however, a
flexible and generic library, so it is possible to write your own
fixers for 2to3.  *note lib2to3: fe. could also be adapted to custom
applications in which Python code needs to be edited automatically.

* Menu:

* Using 2to3::
* Fixers::
* lib2to3 - 2to3's library::


File: python.info,  Node: Using 2to3,  Next: Fixers,  Up: 2to3 - Automated Python 2 to 3 code translation

5.25.4.1 Using 2to3
...................

2to3 will usually be installed with the Python interpreter as a script.
It is also located in the `Tools/scripts' directory of the Python root.

  2to3's basic arguments are a list of files or directories to
transform.  The directories are recursively traversed for Python
sources.

  Here is a sample Python 2.x source file, `example.py':

    def greet(name):
        print "Hello, {0}!".format(name)
    print "What's your name?"
    name = raw_input()
    greet(name)

It can be converted to Python 3.x code via 2to3 on the command line:

    $ 2to3 example.py

A diff against the original source file is printed.  2to3 can also
write the needed modifications right back to the source file.  (A
backup of the original file is made unless `-n' is also given.)
Writing the changes back is enabled with the `-w' flag:

    $ 2to3 -w example.py

After transformation, `example.py' looks like this:

    def greet(name):
        print("Hello, {0}!".format(name))
    print("What's your name?")
    name = input()
    greet(name)

Comments and exact indentation are preserved throughout the translation
process.

  By default, 2to3 runs a set of *note predefined fixers: 2289.  The
`-l' flag lists all available fixers.  An explicit set of fixers to run
can be given with `-f'.  Likewise the *note -x: 635. explicitly
disables a fixer.  The following example runs only the `imports' and
`has_key' fixers:

    $ 2to3 -f imports -f has_key example.py

This command runs every fixer except the `apply' fixer:

    $ 2to3 -x apply example.py

Some fixers are _explicit_, meaning they aren't run by default and must
be listed on the command line to be run.  Here, in addition to the
default fixers, the `idioms' fixer is run:

    $ 2to3 -f all -f idioms example.py

Notice how passing `all' enables all default fixers.

  Sometimes 2to3 will find a place in your source code that needs to be
changed, but 2to3 cannot fix automatically.  In this case, 2to3 will
print a warning beneath the diff for a file.  You should address the
warning in order to have compliant 3.x code.

  2to3 can also refactor doctests.  To enable this mode, use the *note
-d: 627.  flag.  Note that _only_ doctests will be refactored.  This
also doesn't require the module to be valid Python.  For example,
doctest like examples in a reST document could also be refactored with
this option.

  The *note -v: 3ac. option enables output of more information on the
translation process.

  Since some print statements can be parsed as function calls or
statements, 2to3 cannot always read files containing the print
function.  When 2to3 detects the presence of the `from __future__
import print_function' compiler directive, it modifies its internal
grammar to interpret *note print(): 304. as a function.  This change
can also be enabled manually with the `-p' flag.  Use `-p' to run
fixers on code that already has had its print statements converted.

  The `-o' or `--output-dir' option allows specification of an
alternate directory for processed output files to be written to.  The
`-n' flag is required when using this as backup files do not make sense
when not overwriting the input files.

  New in version 2.7.3: The `-o' option was added.

  The *note -W: 1ba. or `--write-unchanged-files' flag tells 2to3 to
always write output files even if no changes were required to the file.
This is most useful with `-o' so that an entire Python source tree is
copied with translation from one directory to another.  This option
implies the `-w' flag as it would not make sense otherwise.

  New in version 2.7.3: The *note -W: 1ba. flag was added.

  The `--add-suffix' option specifies a string to append to all output
filenames.  The `-n' flag is required when specifying this as backups
are not necessary when writing to different filenames.  Example:

    $ 2to3 -n -W --add-suffix=3 example.py

Will cause a converted file named `example.py3' to be written.

  New in version 2.7.3: The `--add-suffix' option was added.

  To translate an entire project from one directory tree to another use:

    $ 2to3 --output-dir=python3-version/mycode -W -n python2-version/mycode



File: python.info,  Node: Fixers,  Next: lib2to3 - 2to3's library,  Prev: Using 2to3,  Up: 2to3 - Automated Python 2 to 3 code translation

5.25.4.2 Fixers
...............

Each step of transforming code is encapsulated in a fixer.  The command
`2to3 -l' lists them.  As *note documented above: 2287, each can be
turned on and off individually.  They are described here in more detail.

 -- 2to3fixer: apply
     Removes usage of *note apply(): 2fc.  For example `apply(function,
     *args, **kwargs)' is converted to `function(*args, **kwargs)'.

 -- 2to3fixer: basestring
     Converts *note basestring: 451. to *note str: 1e7.

 -- 2to3fixer: buffer
     Converts *note buffer: 30f. to *note memoryview: 1c0.  This fixer
     is optional because the *note memoryview: 1c0. API is similar but
     not exactly the same as that of *note buffer: 30f.

 -- 2to3fixer: callable
     Converts `callable(x)' to `isinstance(x, collections.Callable)',
     adding an import to *note collections: 65. if needed. Note
     `callable(x)' has returned in Python 3.2, so if you do not intend
     to support Python 3.1, you can disable this fixer.

 -- 2to3fixer: dict
     Fixes dictionary iteration methods.  *note dict.iteritems(): 8e7.
     is converted to *note dict.items(): 1e0, *note dict.iterkeys():
     8e2. to *note dict.keys(): 1de, and *note dict.itervalues(): 8e8.
     to *note dict.values(): 1df.  Similarly, *note dict.viewitems():
     1e3, *note dict.viewkeys(): 1e1. and *note dict.viewvalues(): 1e2.
     are converted respectively to *note dict.items(): 1e0, *note
     dict.keys(): 1de. and *note dict.values(): 1df.  It also wraps
     existing usages of *note dict.items(): 1e0, *note dict.keys():
     1de, and *note dict.values(): 1df. in a call to *note list: 3b5.

 -- 2to3fixer: except
     Converts `except X, T' to `except X as T'.

 -- 2to3fixer: exec
     Converts the *note exec: 3fd. statement to the `exec()' function.

 -- 2to3fixer: execfile
     Removes usage of *note execfile(): 42f.  The argument to *note
     execfile(): 42f. is wrapped in calls to *note open(): 2d3, *note
     compile(): 1f8, and `exec()'.

 -- 2to3fixer: exitfunc
     Changes assignment of *note sys.exitfunc: 407. to use of the *note
     atexit: 12.  module.

 -- 2to3fixer: filter
     Wraps *note filter(): 402. usage in a *note list: 3b5. call.

 -- 2to3fixer: funcattrs
     Fixes function attributes that have been renamed.  For example,
     `my_function.func_closure' is converted to
     `my_function.__closure__'.

 -- 2to3fixer: future
     Removes `from __future__ import new_feature' statements.

 -- 2to3fixer: getcwdu
     Renames *note os.getcwdu(): 42b. to *note os.getcwd(): 111f.

 -- 2to3fixer: has_key
     Changes `dict.has_key(key)' to `key in dict'.

 -- 2to3fixer: idioms
     This optional fixer performs several transformations that make
     Python code more idiomatic.  Type comparisons like `type(x) is
     SomeClass' and `type(x) == SomeClass' are converted to
     `isinstance(x, SomeClass)'.  `while 1' becomes `while True'.  This
     fixer also tries to make use of *note sorted(): 220. in
     appropriate places.  For example, this block

         L = list(some_iterable)
         L.sort()

     is changed to

         L = sorted(some_iterable)



 -- 2to3fixer: import
     Detects sibling imports and converts them to relative imports.

 -- 2to3fixer: imports
     Handles module renames in the standard library.

 -- 2to3fixer: imports2
     Handles other modules renames in the standard library.  It is
     separate from the *note imports: 229b. fixer only because of
     technical limitations.

 -- 2to3fixer: input
     Converts `input(prompt)' to `eval(input(prompt))'

 -- 2to3fixer: intern
     Converts *note intern(): 866. to `sys.intern()'.

 -- 2to3fixer: isinstance
     Fixes duplicate types in the second argument of *note
     isinstance(): 317.  For example, `isinstance(x, (int, int))' is
     converted to `isinstance(x, (int))'.

 -- 2to3fixer: itertools_imports
     Removes imports of *note itertools.ifilter(): 86b, *note
     itertools.izip(): 3ff, and *note itertools.imap(): d47.  Imports
     of *note itertools.ifilterfalse(): 86c. are also changed to
     `itertools.filterfalse()'.

 -- 2to3fixer: itertools
     Changes usage of *note itertools.ifilter(): 86b, *note
     itertools.izip(): 3ff, and *note itertools.imap(): d47. to their
     built-in equivalents.  *note itertools.ifilterfalse(): 86c. is
     changed to `itertools.filterfalse()'.

 -- 2to3fixer: long
     Renames *note long: 1f0. to *note int: 1ef.

 -- 2to3fixer: map
     Wraps *note map(): 2fd. in a *note list: 3b5. call.  It also
     changes `map(None, x)' to `list(x)'.  Using `from future_builtins
     import map' disables this fixer.

 -- 2to3fixer: metaclass
     Converts the old metaclass syntax (`__metaclass__ = Meta' in the
     class body) to the new (`class X(metaclass=Meta)').

 -- 2to3fixer: methodattrs
     Fixes old method attribute names.  For example, `meth.im_func' is
     converted to `meth.__func__'.

 -- 2to3fixer: ne
     Converts the old not-equal syntax, `<>', to `!='.

 -- 2to3fixer: next
     Converts the use of iterator's *note next(): 871. methods to the
     *note next(): 392. function.  It also renames *note next(): 392.
     methods to `__next__()'.

 -- 2to3fixer: nonzero
     Renames *note __nonzero__(): 6f9. to `__bool__()'.

 -- 2to3fixer: numliterals
     Converts octal literals into the new syntax.

 -- 2to3fixer: paren
     Add extra parenthesis where they are required in list
     comprehensions.  For example, `[x for x in 1, 2]' becomes `[x for
     x in (1, 2)]'.

 -- 2to3fixer: print
     Converts the *note print: 4d7. statement to the *note print():
     304. function.

 -- 2to3fixer: raise
     Converts `raise E, V' to `raise E(V)', and `raise E, V, T' to
     `raise E(V).with_traceback(T)'.  If `E' is a tuple, the
     translation will be incorrect because substituting tuples for
     exceptions has been removed in Python 3.

 -- 2to3fixer: raw_input
     Converts *note raw_input(): 854. to *note input(): 3b8.

 -- 2to3fixer: reduce
     Handles the move of *note reduce(): 2e2. to *note
     functools.reduce(): 2e1.

 -- 2to3fixer: renames
     Changes *note sys.maxint: 22b0. to *note sys.maxsize: 7c1.

 -- 2to3fixer: repr
     Replaces backtick repr with the *note repr(): 145. function.

 -- 2to3fixer: set_literal
     Replaces use of the *note set: 363. constructor with set literals.
     This fixer is optional.

 -- 2to3fixer: standard_error
     Renames *note StandardError: 37b. to *note Exception: 332.

 -- 2to3fixer: sys_exc
     Changes the deprecated *note sys.exc_value: 22b5, *note
     sys.exc_type: 22b6, *note sys.exc_traceback: 22b7. to use *note
     sys.exc_info(): 2ec.

 -- 2to3fixer: throw
     Fixes the API change in generator's `throw()' method.

 -- 2to3fixer: tuple_params
     Removes implicit tuple parameter unpacking.  This fixer inserts
     temporary variables.

 -- 2to3fixer: types
     Fixes code broken from the removal of some members in the *note
     types: 185.  module.

 -- 2to3fixer: unicode
     Renames *note unicode: 1f2. to *note str: 1e7.

 -- 2to3fixer: urllib
     Handles the rename of *note urllib: 188. and *note urllib2: 189.
     to the *note urllib: 188.  package.

 -- 2to3fixer: ws_comma
     Removes excess whitespace from comma separated items.  This fixer
     is optional.

 -- 2to3fixer: xrange
     Renames *note xrange(): 454. to *note range(): 2d6. and wraps
     existing *note range(): 2d6.  calls with *note list: 3b5.

 -- 2to3fixer: xreadlines
     Changes `for x in file.xreadlines()' to `for x in file'.

 -- 2to3fixer: zip
     Wraps *note zip(): 3fe. usage in a *note list: 3b5. call.  This is
     disabled when `from future_builtins import zip' appears.


File: python.info,  Node: lib2to3 - 2to3's library,  Prev: Fixers,  Up: 2to3 - Automated Python 2 to 3 code translation

5.25.4.3 `lib2to3' - 2to3's library
...................................

     Note: The *note lib2to3: fe. API should be considered unstable and
     may change drastically in the future.


File: python.info,  Node: test --- Regression tests package for Python,  Next: test test_support --- Utility functions for tests,  Prev: 2to3 - Automated Python 2 to 3 code translation,  Up: Development Tools

5.25.5 `test' -- Regression tests package for Python
----------------------------------------------------

     Note: The *note test: 175. package is meant for internal use by
     Python only. It is documented for the benefit of the core
     developers of Python. Any use of this package outside of Python's
     standard library is discouraged as code mentioned here can change
     or be removed without notice between releases of Python.

The *note test: 175. package contains all regression tests for Python
as well as the modules *note test.test_support: 176. and
`test.regrtest'.  *note test.test_support: 176. is used to enhance your
tests while `test.regrtest' drives the testing suite.

  Each module in the *note test: 175. package whose name starts with
`test_' is a testing suite for a specific module or feature. All new
tests should be written using the *note unittest: 187. or *note
doctest: b5. module.  Some older tests are written using a
"traditional" testing style that compares output printed to
`sys.stdout'; this style of test is considered deprecated.

See also
........

Module *note unittest: 187.
     Writing PyUnit regression tests.

Module *note doctest: b5.
     Tests embedded in documentation strings.

* Menu:

* Writing Unit Tests for the test package::
* Running tests using the command-line interface::


File: python.info,  Node: Writing Unit Tests for the test package,  Next: Running tests using the command-line interface,  Up: test --- Regression tests package for Python

5.25.5.1 Writing Unit Tests for the `test' package
..................................................

It is preferred that tests that use the *note unittest: 187. module
follow a few guidelines. One is to name the test module by starting it
with `test_' and end it with the name of the module being tested. The
test methods in the test module should start with `test_' and end with
a description of what the method is testing. This is needed so that the
methods are recognized by the test driver as test methods. Also, no
documentation string for the method should be included. A comment (such
as `# Tests function returns only True or False') should be used to
provide documentation for test methods. This is done because
documentation strings get printed out if they exist and thus what test
is being run is not stated.

  A basic boilerplate is often used:

    import unittest
    from test import test_support

    class MyTestCase1(unittest.TestCase):

        # Only use setUp() and tearDown() if necessary

        def setUp(self):
            ... code to execute in preparation for tests ...

        def tearDown(self):
            ... code to execute to clean up after tests ...

        def test_feature_one(self):
            # Test feature one.
            ... testing code ...

        def test_feature_two(self):
            # Test feature two.
            ... testing code ...

        ... more test methods ...

    class MyTestCase2(unittest.TestCase):
        ... same structure as MyTestCase1 ...

    ... more test classes ...

    def test_main():
        test_support.run_unittest(MyTestCase1,
                                  MyTestCase2,
                                  ... list other tests ...
                                 )

    if __name__ == '__main__':
        test_main()

This boilerplate code allows the testing suite to be run by
`test.regrtest' as well as on its own as a script.

  The goal for regression testing is to try to break code. This leads
to a few guidelines to be followed:

   * The testing suite should exercise all classes, functions, and
     constants. This includes not just the external API that is to be
     presented to the outside world but also "private" code.

   * Whitebox testing (examining the code being tested when the tests
     are being written) is preferred. Blackbox testing (testing only
     the published user interface) is not complete enough to make sure
     all boundary and edge cases are tested.

   * Make sure all possible values are tested including invalid ones.
     This makes sure that not only all valid values are acceptable but
     also that improper values are handled correctly.

   * Exhaust as many code paths as possible. Test where branching
     occurs and thus tailor input to make sure as many different paths
     through the code are taken.

   * Add an explicit test for any bugs discovered for the tested code.
     This will make sure that the error does not crop up again if the
     code is changed in the future.

   * Make sure to clean up after your tests (such as close and remove
     all temporary files).

   * If a test is dependent on a specific condition of the operating
     system then verify the condition already exists before attempting
     the test.

   * Import as few modules as possible and do it as soon as possible.
     This minimizes external dependencies of tests and also minimizes
     possible anomalous behavior from side-effects of importing a
     module.

   * Try to maximize code reuse. On occasion, tests will vary by
     something as small as what type of input is used. Minimize code
     duplication by subclassing a basic test class with a class that
     specifies the input:

         class TestFuncAcceptsSequences(unittest.TestCase):

             func = mySuperWhammyFunction

             def test_func(self):
                 self.func(self.arg)

         class AcceptLists(TestFuncAcceptsSequences):
             arg = [1, 2, 3]

         class AcceptStrings(TestFuncAcceptsSequences):
             arg = 'abc'

         class AcceptTuples(TestFuncAcceptsSequences):
             arg = (1, 2, 3)



See also
........

Test Driven Development
     A book by Kent Beck on writing tests before code.


File: python.info,  Node: Running tests using the command-line interface,  Prev: Writing Unit Tests for the test package,  Up: test --- Regression tests package for Python

5.25.5.2 Running tests using the command-line interface
.......................................................

The `test.regrtest' module can be run as a script to drive Python's
regression test suite, thanks to the *note -m: 2f4. option: *python -m
test.regrtest*.  Running the script by itself automatically starts
running all regression tests in the *note test: 175. package. It does
this by finding all modules in the package whose name starts with
`test_', importing them, and executing the function `test_main()' if
present. The names of tests to execute may also be passed to the
script. Specifying a single regression test (*python -m test.regrtest
test_spam*) will minimize output and only print whether the test passed
or failed and thus minimize output.

  Running `test.regrtest' directly allows what resources are available
for tests to use to be set. You do this by using the `-u' command-line
option. Specifying `all' as the value for the `-u' option enables all
possible resources: *python -m test -uall*.  If all but one resource is
desired (a more common case), a comma-separated list of resources that
are not desired may be listed after `all'. The command *python -m
test.regrtest -uall,-audio,-largefile* will run `test.regrtest' with
all resources except the `audio' and `largefile' resources. For a list
of all resources and more command-line options, run *python -m
test.regrtest -h*.

  Some other ways to execute the regression tests depend on what
platform the tests are being executed on. On Unix, you can run *make
test* at the top-level directory where Python was built. On Windows,
executing *rt.bat* from your `PCBuild' directory will run all regression
tests.


File: python.info,  Node: test test_support --- Utility functions for tests,  Prev: test --- Regression tests package for Python,  Up: Development Tools

5.25.6 `test.test_support' -- Utility functions for tests
---------------------------------------------------------

     Note: The *note test.test_support: 176. module has been renamed to
     `test.support' in Python 3.x.

The *note test.test_support: 176. module provides support for Python's
regression tests.

  This module defines the following exceptions:

 -- Exception: test.test_support.TestFailed
     Exception to be raised when a test fails. This is deprecated in
     favor of *note unittest: 187.-based tests and *note
     unittest.TestCase: 27f.'s assertion methods.

 -- Exception: test.test_support.ResourceDenied
     Subclass of *note unittest.SkipTest: 27a. Raised when a resource
     (such as a network connection) is not available. Raised by the
     *note requires(): 22cb.  function.

  The *note test.test_support: 176. module defines the following
constants:

 -- Data: test.test_support.verbose
     *note True: 3a9. when verbose output is enabled. Should be checked
     when more detailed information is desired about a running test.
     _verbose_ is set by `test.regrtest'.

 -- Data: test.test_support.have_unicode
     *note True: 3a9. when Unicode support is available.

 -- Data: test.test_support.is_jython
     *note True: 3a9. if the running interpreter is Jython.

 -- Data: test.test_support.TESTFN
     Set to a name that is safe to use as the name of a temporary file.
     Any temporary file that is created should be closed and unlinked
     (removed).

  The *note test.test_support: 176. module defines the following
functions:

 -- Function: test.test_support.forget (module_name)
     Remove the module named _module_name_ from `sys.modules' and
     delete any byte-compiled files of the module.

 -- Function: test.test_support.is_resource_enabled (resource)
     Return *note True: 3a9. if _resource_ is enabled and available.
     The list of available resources is only set when `test.regrtest'
     is executing the tests.

 -- Function: test.test_support.requires (resource[, msg])
     Raise *note ResourceDenied: 22ca. if _resource_ is not available.
     _msg_ is the argument to *note ResourceDenied: 22ca. if it is
     raised. Always returns *note True: 3a9. if called by a function
     whose `__name__' is `'__main__''.  Used when tests are executed by
     `test.regrtest'.

 -- Function: test.test_support.findfile (filename)
     Return the path to the file named _filename_. If no match is found
     _filename_ is returned. This does not equal a failure since it
     could be the path to the file.

 -- Function: test.test_support.run_unittest (*classes)
     Execute *note unittest.TestCase: 27f. subclasses passed to the
     function. The function scans the classes for methods starting with
     the prefix `test_' and executes the tests individually.

     It is also legal to pass strings as parameters; these should be
     keys in `sys.modules'. Each associated module will be scanned by
     `unittest.TestLoader.loadTestsFromModule()'. This is usually seen
     in the following `test_main()' function:

         def test_main():
             test_support.run_unittest(__name__)

     This will run all tests defined in the named module.

 -- Function: test.test_support.check_warnings (*filters, quiet=True)
     A convenience wrapper for *note warnings.catch_warnings(): 22d5.
     that makes it easier to test that a warning was correctly raised.
     It is approximately equivalent to calling
     `warnings.catch_warnings(record=True)' with *note
     warnings.simplefilter(): 22d6. set to `always' and with the option
     to automatically validate the results that are recorded.

     `check_warnings' accepts 2-tuples of the form `("message regexp",
     WarningCategory)' as positional arguments. If one or more
     _filters_ are provided, or if the optional keyword argument
     _quiet_ is *note False: 3aa, it checks to make sure the warnings
     are as expected:  each specified filter must match at least one of
     the warnings raised by the enclosed code or the test fails, and if
     any warnings are raised that do not match any of the specified
     filters the test fails.  To disable the first of these checks, set
     _quiet_ to *note True: 3a9.

     If no arguments are specified, it defaults to:

         check_warnings(("", Warning), quiet=True)

     In this case all warnings are caught and no errors are raised.

     On entry to the context manager, a `WarningRecorder' instance is
     returned. The underlying warnings list from *note
     catch_warnings(): 22d5. is available via the recorder object's
     *note warnings: 193. attribute.  As a convenience, the attributes
     of the object representing the most recent warning can also be
     accessed directly through the recorder object (see example below).
     If no warning has been raised, then any of the attributes that
     would otherwise be expected on an object representing a warning
     will return *note None: 393.

     The recorder object also has a `reset()' method, which clears the
     warnings list.

     The context manager is designed to be used like this:

         with check_warnings(("assertion is always true", SyntaxWarning),
                             ("", UserWarning)):
             exec('assert(False, "Hey!")')
             warnings.warn(UserWarning("Hide me!"))

     In this case if either warning was not raised, or some other
     warning was raised, *note check_warnings(): 22d4. would raise an
     error.

     When a test needs to look more deeply into the warnings, rather
     than just checking whether or not they occurred, code like this
     can be used:

         with check_warnings(quiet=True) as w:
             warnings.warn("foo")
             assert str(w.args[0]) == "foo"
             warnings.warn("bar")
             assert str(w.args[0]) == "bar"
             assert str(w.warnings[0].args[0]) == "foo"
             assert str(w.warnings[1].args[0]) == "bar"
             w.reset()
             assert len(w.warnings) == 0

     Here all warnings will be caught, and the test code tests the
     captured warnings directly.

     New in version 2.6.

     Changed in version 2.7: New optional arguments _filters_ and
     _quiet_.

 -- Function: test.test_support.check_py3k_warnings (*filters,
          quiet=False)
     Similar to *note check_warnings(): 22d4, but for Python 3
     compatibility warnings.  If `sys.py3kwarning == 1', it checks if
     the warning is effectively raised.  If `sys.py3kwarning == 0', it
     checks that no warning is raised.  It accepts 2-tuples of the form
     `("message regexp", WarningCategory)' as positional arguments.
     When the optional keyword argument _quiet_ is *note True: 3a9, it
     does not fail if a filter catches nothing.  Without arguments, it
     defaults to:

         check_py3k_warnings(("", DeprecationWarning), quiet=False)

     New in version 2.7.

 -- Function: test.test_support.captured_stdout ()
     This is a context manager that runs the *note with: 1bd. statement
     body using a *note StringIO.StringIO: 2da. object as sys.stdout.
     That object can be retrieved using the `as' clause of the *note
     with: 1bd. statement.

     Example use:

         with captured_stdout() as s:
             print "hello"
         assert s.getvalue() == "hello\n"

     New in version 2.6.

 -- Function: test.test_support.import_module (name, deprecated=False)
     This function imports and returns the named module. Unlike a normal
     import, this function raises *note unittest.SkipTest: 27a. if the
     module cannot be imported.

     Module and package deprecation messages are suppressed during this
     import if _deprecated_ is *note True: 3a9.

     New in version 2.7.

 -- Function: test.test_support.import_fresh_module (name, fresh=(),
          blocked=(), deprecated=False)
     This function imports and returns a fresh copy of the named Python
     module by removing the named module from `sys.modules' before
     doing the import.  Note that unlike *note reload(): 571, the
     original module is not affected by this operation.

     _fresh_ is an iterable of additional module names that are also
     removed from the `sys.modules' cache before doing the import.

     _blocked_ is an iterable of module names that are replaced with `0'
     in the module cache during the import to ensure that attempts to
     import them raise *note ImportError: 369.

     The named module and any modules named in the _fresh_ and _blocked_
     parameters are saved before starting the import and then
     reinserted into `sys.modules' when the fresh import is complete.

     Module and package deprecation messages are suppressed during this
     import if _deprecated_ is *note True: 3a9.

     This function will raise *note unittest.SkipTest: 27a. is the
     named module cannot be imported.

     Example use:

         # Get copies of the warnings module for testing without
         # affecting the version being used by the rest of the test suite
         # One copy uses the C implementation, the other is forced to use
         # the pure Python fallback implementation
         py_warnings = import_fresh_module('warnings', blocked=['_warnings'])
         c_warnings = import_fresh_module('warnings', fresh=['_warnings'])

     New in version 2.7.

  The *note test.test_support: 176. module defines the following
classes:

 -- Class: test.test_support.TransientResource (exc[, **kwargs])
     Instances are a context manager that raises *note ResourceDenied:
     22ca. if the specified exception type is raised.  Any keyword
     arguments are treated as attribute/value pairs to be compared
     against any exception raised within the *note with: 1bd.
     statement.  Only if all pairs match properly against attributes on
     the exception is *note ResourceDenied: 22ca. raised.

     New in version 2.6.

 -- Class: test.test_support.EnvironmentVarGuard
     Class used to temporarily set or unset environment variables.
     Instances can be used as a context manager and have a complete
     dictionary interface for querying/modifying the underlying
     `os.environ'. After exit from the context manager all changes to
     environment variables done through this instance will be rolled
     back.

     New in version 2.6.

     Changed in version 2.7: Added dictionary interface.

 -- Method: EnvironmentVarGuard.set (envvar, value)
     Temporarily set the environment variable `envvar' to the value of
     `value'.

 -- Method: EnvironmentVarGuard.unset (envvar)
     Temporarily unset the environment variable `envvar'.

 -- Class: test.test_support.WarningsRecorder
     Class used to record warnings for unit tests. See documentation of
     *note check_warnings(): 22d4. above for more details.

     New in version 2.6.


File: python.info,  Node: Debugging and Profiling,  Next: Python Runtime Services,  Prev: Development Tools,  Up: The Python Standard Library

5.26 Debugging and Profiling
============================

These libraries help you with Python development: the debugger enables
you to step through code, analyze stack frames and set breakpoints
etc., and the profilers run code and give you a detailed breakdown of
execution times, allowing you to identify bottlenecks in your programs.

* Menu:

* bdb: bdb --- Debugger framework. Debugger framework
* pdb: pdb --- The Python Debugger. The Python Debugger
* Debugger Commands::
* The Python Profilers::
* hotshot: hotshot --- High performance logging profiler. High performance logging profiler
* timeit: timeit --- Measure execution time of small code snippets. Measure execution time of small code snippets
* trace: trace --- Trace or track Python statement execution. Trace or track Python statement execution


File: python.info,  Node: bdb --- Debugger framework,  Next: pdb --- The Python Debugger,  Up: Debugging and Profiling

5.26.1 `bdb' -- Debugger framework
----------------------------------

*Source code:* Lib/bdb.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  The *note bdb: 18. module handles basic debugger functions, like
setting breakpoints or managing execution via the debugger.

  The following exception is defined:

 -- Exception: bdb.BdbQuit
     Exception raised by the *note Bdb: 200. class for quitting the
     debugger.

  The *note bdb: 18. module also defines two classes:

 -- Class: bdb.Breakpoint (self, file, line, temporary=0, cond=None,
          funcname=None)
     This class implements temporary breakpoints, ignore counts,
     disabling and (re-)enabling, and conditionals.

     Breakpoints are indexed by number through a list called
     `bpbynumber' and by `(file, line)' pairs through `bplist'.  The
     former points to a single instance of class *note Breakpoint:
     22e5.  The latter points to a list of such instances since there
     may be more than one breakpoint per line.

     When creating a breakpoint, its associated filename should be in
     canonical form.  If a _funcname_ is defined, a breakpoint hit will
     be counted when the first line of that function is executed.  A
     conditional breakpoint always counts a hit.

     *note Breakpoint: 22e5. instances have the following methods:

      -- Method: deleteMe ()
          Delete the breakpoint from the list associated to a
          file/line.  If it is the last breakpoint in that position, it
          also deletes the entry for the file/line.

      -- Method: enable ()
          Mark the breakpoint as enabled.

      -- Method: disable ()
          Mark the breakpoint as disabled.

      -- Method: pprint ([out])
          Print all the information about the breakpoint:

             * The breakpoint number.

             * If it is temporary or not.

             * Its file,line position.

             * The condition that causes a break.

             * If it must be ignored the next N times.

             * The breakpoint hit count.

 -- Class: bdb.Bdb (skip=None)
     The *note Bdb: 200. class acts as a generic Python debugger base
     class.

     This class takes care of the details of the trace facility; a
     derived class should implement user interaction.  The standard
     debugger class (*note pdb.Pdb: 22ea.) is an example.

     The _skip_ argument, if given, must be an iterable of glob-style
     module name patterns.  The debugger will not step into frames that
     originate in a module that matches one of these patterns. Whether a
     frame is considered to originate in a certain module is determined
     by the `__name__' in the frame globals.

     New in version 2.7: The _skip_ argument.

     The following methods of *note Bdb: 200. normally don't need to be
     overridden.

      -- Method: canonic (filename)
          Auxiliary method for getting a filename in a canonical form,
          that is, as a case-normalized (on case-insensitive
          filesystems) absolute path, stripped of surrounding angle
          brackets.

      -- Method: reset ()
          Set the `botframe', `stopframe', `returnframe' and `quitting'
          attributes with values ready to start debugging.

      -- Method: trace_dispatch (frame, event, arg)
          This function is installed as the trace function of debugged
          frames.  Its return value is the new trace function (in most
          cases, that is, itself).

          The default implementation decides how to dispatch a frame,
          depending on the type of event (passed as a string) that is
          about to be executed.  _event_ can be one of the following:

             * `"line"': A new line of code is going to be executed.

             * `"call"': A function is about to be called, or another
               code block entered.

             * `"return"': A function or other code block is about to
               return.

             * `"exception"': An exception has occurred.

             * `"c_call"': A C function is about to be called.

             * `"c_return"': A C function has returned.

             * `"c_exception"': A C function has raised an exception.

          For the Python events, specialized functions (see below) are
          called.  For the C events, no action is taken.

          The _arg_ parameter depends on the previous event.

          See the documentation for *note sys.settrace(): 497. for more
          information on the trace function.  For more information on
          code and frame objects, refer to *note The standard type
          hierarchy: 6de.

      -- Method: dispatch_line (frame)
          If the debugger should stop on the current line, invoke the
          *note user_line(): 22ef. method (which should be overridden
          in subclasses).  Raise a *note BdbQuit: 22e4. exception if
          the `Bdb.quitting' flag is set (which can be set from *note
          user_line(): 22ef.).  Return a reference to the *note
          trace_dispatch(): 22ed. method for further tracing in that
          scope.

      -- Method: dispatch_call (frame, arg)
          If the debugger should stop on this function call, invoke the
          *note user_call(): 22f1. method (which should be overridden
          in subclasses).  Raise a *note BdbQuit: 22e4. exception if
          the `Bdb.quitting' flag is set (which can be set from *note
          user_call(): 22f1.).  Return a reference to the *note
          trace_dispatch(): 22ed. method for further tracing in that
          scope.

      -- Method: dispatch_return (frame, arg)
          If the debugger should stop on this function return, invoke
          the *note user_return(): 22f3. method (which should be
          overridden in subclasses).  Raise a *note BdbQuit: 22e4.
          exception if the `Bdb.quitting' flag is set (which can be set
          from *note user_return(): 22f3.).  Return a reference to the
          *note trace_dispatch(): 22ed. method for further tracing in
          that scope.

      -- Method: dispatch_exception (frame, arg)
          If the debugger should stop at this exception, invokes the
          *note user_exception(): 22f5. method (which should be
          overridden in subclasses).  Raise a *note BdbQuit: 22e4.
          exception if the `Bdb.quitting' flag is set (which can be set
          from *note user_exception(): 22f5.).  Return a reference to
          the *note trace_dispatch(): 22ed. method for further tracing
          in that scope.

     Normally derived classes don't override the following methods, but
     they may if they want to redefine the definition of stopping and
     breakpoints.

      -- Method: stop_here (frame)
          This method checks if the _frame_ is somewhere below
          `botframe' in the call stack.  `botframe' is the frame in
          which debugging started.

      -- Method: break_here (frame)
          This method checks if there is a breakpoint in the filename
          and line belonging to _frame_ or, at least, in the current
          function.  If the breakpoint is a temporary one, this method
          deletes it.

      -- Method: break_anywhere (frame)
          This method checks if there is a breakpoint in the filename
          of the current frame.

     Derived classes should override these methods to gain control over
     debugger operation.

      -- Method: user_call (frame, argument_list)
          This method is called from *note dispatch_call(): 22f0. when
          there is the possibility that a break might be necessary
          anywhere inside the called function.

      -- Method: user_line (frame)
          This method is called from *note dispatch_line(): 22ee. when
          either *note stop_here(): 22f6. or *note break_here(): 22f7.
          yields True.

      -- Method: user_return (frame, return_value)
          This method is called from *note dispatch_return(): 22f2.
          when *note stop_here(): 22f6.  yields True.

      -- Method: user_exception (frame, exc_info)
          This method is called from *note dispatch_exception(): 22f4.
          when *note stop_here(): 22f6. yields True.

      -- Method: do_clear (arg)
          Handle how a breakpoint must be removed when it is a
          temporary one.

          This method must be implemented by derived classes.

     Derived classes and clients can call the following methods to
     affect the stepping state.

      -- Method: set_step ()
          Stop after one line of code.

      -- Method: set_next (frame)
          Stop on the next line in or below the given frame.

      -- Method: set_return (frame)
          Stop when returning from the given frame.

      -- Method: set_until (frame)
          Stop when the line with the line no greater than the current
          one is reached or when returning from current frame

      -- Method: set_trace ([frame])
          Start debugging from _frame_.  If _frame_ is not specified,
          debugging starts from caller's frame.

      -- Method: set_continue ()
          Stop only at breakpoints or when finished.  If there are no
          breakpoints, set the system trace function to None.

      -- Method: set_quit ()
          Set the `quitting' attribute to True.  This raises *note
          BdbQuit: 22e4. in the next call to one of the `dispatch_*()'
          methods.

     Derived classes and clients can call the following methods to
     manipulate breakpoints.  These methods return a string containing
     an error message if something went wrong, or `None' if all is well.

      -- Method: set_break (filename, lineno, temporary=0, cond=None,
               funcname=None)
          Set a new breakpoint.  If the _lineno_ line doesn't exist for
          the _filename_ passed as argument, return an error message.
          The _filename_ should be in canonical form, as described in
          the *note canonic(): 22eb. method.

      -- Method: clear_break (filename, lineno)
          Delete the breakpoints in _filename_ and _lineno_.  If none
          were set, an error message is returned.

      -- Method: clear_bpbynumber (arg)
          Delete the breakpoint which has the index _arg_ in the
          `Breakpoint.bpbynumber'.  If _arg_ is not numeric or out of
          range, return an error message.

      -- Method: clear_all_file_breaks (filename)
          Delete all breakpoints in _filename_.  If none were set, an
          error message is returned.

      -- Method: clear_all_breaks ()
          Delete all existing breakpoints.

      -- Method: get_break (filename, lineno)
          Check if there is a breakpoint for _lineno_ of _filename_.

      -- Method: get_breaks (filename, lineno)
          Return all breakpoints for _lineno_ in _filename_, or an
          empty list if none are set.

      -- Method: get_file_breaks (filename)
          Return all breakpoints in _filename_, or an empty list if
          none are set.

      -- Method: get_all_breaks ()
          Return all breakpoints that are set.

     Derived classes and clients can call the following methods to get
     a data structure representing a stack trace.

      -- Method: get_stack (f, t)
          Get a list of records for a frame and all higher (calling)
          and lower frames, and the size of the higher part.

      -- Method: format_stack_entry (frame_lineno[, lprefix=': '])
          Return a string with information about a stack entry,
          identified by a `(frame, lineno)' tuple:

             * The canonical form of the filename which contains the
               frame.

             * The function name, or `"<lambda>"'.

             * The input arguments.

             * The return value.

             * The line of code (if it exists).

     The following two methods can be called by clients to use a
     debugger to debug a *note statement: dac, given as a string.

      -- Method: run (cmd[, globals[, locals]])
          Debug a statement executed via the *note exec: 3fd.
          statement.  _globals_ defaults to `__main__.__dict__',
          _locals_ defaults to _globals_.

      -- Method: runeval (expr[, globals[, locals]])
          Debug an expression executed via the *note eval(): 359.
          function.  _globals_ and _locals_ have the same meaning as in
          *note run(): 230c.

      -- Method: runctx (cmd, globals, locals)
          For backwards compatibility.  Calls the *note run(): 230c.
          method.

      -- Method: runcall (func, *args, **kwds)
          Debug a single function call, and return its result.

  Finally, the module defines the following functions:

 -- Function: bdb.checkfuncname (b, frame)
     Check whether we should break here, depending on the way the
     breakpoint _b_ was set.

     If it was set via line number, it checks if `b.line' is the same
     as the one in the frame also passed as argument.  If the
     breakpoint was set via function name, we have to check we are in
     the right frame (the right function) and if we are in its first
     executable line.

 -- Function: bdb.effective (file, line, frame)
     Determine if there is an effective (active) breakpoint at this
     line of code.  Return a tuple of the breakpoint and a boolean that
     indicates if it is ok to delete a temporary breakpoint.  Return
     `(None, None)' if there is no matching breakpoint.

 -- Function: bdb.set_trace ()
     Start debugging with a *note Bdb: 200. instance from caller's
     frame.

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/bdb.py


File: python.info,  Node: pdb --- The Python Debugger,  Next: Debugger Commands,  Prev: bdb --- Debugger framework,  Up: Debugging and Profiling

5.26.2 `pdb' -- The Python Debugger
-----------------------------------

The module *note pdb: 12c. defines an interactive source code debugger
for Python programs.  It supports setting (conditional) breakpoints and
single stepping at the source line level, inspection of stack frames,
source code listing, and evaluation of arbitrary Python code in the
context of any stack frame.  It also supports post-mortem debugging and
can be called under program control.

  The debugger is extensible -- it is actually defined as the class
*note Pdb: 22ea.  This is currently undocumented but easily understood
by reading the source.  The extension interface uses the modules *note
bdb: 18. and *note cmd: 61.

  The debugger's prompt is `(Pdb)'. Typical usage to run a program
under control of the debugger is:

    >>> import pdb
    >>> import mymodule
    >>> pdb.run('mymodule.test()')
    > <string>(0)?()
    (Pdb) continue
    > <string>(1)?()
    (Pdb) continue
    NameError: 'spam'
    > <string>(1)?()
    (Pdb)

`pdb.py' can also be invoked as a script to debug other scripts.  For
example:

    python -m pdb myscript.py

When invoked as a script, pdb will automatically enter post-mortem
debugging if the program being debugged exits abnormally. After
post-mortem debugging (or after normal exit of the program), pdb will
restart the program. Automatic restarting preserves pdb's state (such
as breakpoints) and in most cases is more useful than quitting the
debugger upon program's exit.

  New in version 2.4: Restarting post-mortem behavior added.

  The typical usage to break into the debugger from a running program
is to insert

    import pdb; pdb.set_trace()

at the location you want to break into the debugger.  You can then step
through the code following this statement, and continue running without
the debugger using the `c' command.

  The typical usage to inspect a crashed program is:

    >>> import pdb
    >>> import mymodule
    >>> mymodule.test()
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
      File "./mymodule.py", line 4, in test
        test2()
      File "./mymodule.py", line 3, in test2
        print spam
    NameError: spam
    >>> pdb.pm()
    > ./mymodule.py(3)test2()
    -> print spam
    (Pdb)

The module defines the following functions; each enters the debugger in
a slightly different way:

 -- Function: pdb.run (statement[, globals[, locals]])
     Execute the _statement_ (given as a string) under debugger
     control.  The debugger prompt appears before any code is executed;
     you can set breakpoints and type `continue', or you can step
     through the statement using `step' or `next' (all these commands
     are explained below).  The optional _globals_ and _locals_
     arguments specify the environment in which the code is executed; by
     default the dictionary of the module *note __main__: 2. is used.
     (See the explanation of the *note exec: 3fd. statement or the
     *note eval(): 359. built-in function.)

 -- Function: pdb.runeval (expression[, globals[, locals]])
     Evaluate the _expression_ (given as a string) under debugger
     control.  When *note runeval(): 2316. returns, it returns the
     value of the expression.  Otherwise this function is similar to
     *note run(): 221c.

 -- Function: pdb.runcall (function[, argument, ...])
     Call the _function_ (a function or method object, not a string)
     with the given arguments.  When *note runcall(): 2317. returns, it
     returns whatever the function call returned.  The debugger prompt
     appears as soon as the function is entered.

 -- Function: pdb.set_trace ()
     Enter the debugger at the calling stack frame.  This is useful to
     hard-code a breakpoint at a given point in a program, even if the
     code is not otherwise being debugged (e.g. when an assertion
     fails).

 -- Function: pdb.post_mortem ([traceback])
     Enter post-mortem debugging of the given _traceback_ object.  If no
     _traceback_ is given, it uses the one of the exception that is
     currently being handled (an exception must be being handled if the
     default is to be used).

 -- Function: pdb.pm ()
     Enter post-mortem debugging of the traceback found in *note
     sys.last_traceback: 2319.

  The `run*' functions and *note set_trace(): 2219. are aliases for
instantiating the *note Pdb: 22ea. class and calling the method of the
same name.  If you want to access further features, you have to do this
yourself:

 -- Class: pdb.Pdb (completekey='tab', stdin=None, stdout=None,
          skip=None)
     *note Pdb: 22ea. is the debugger class.

     The _completekey_, _stdin_ and _stdout_ arguments are passed to the
     underlying *note cmd.Cmd: 201d. class; see the description there.

     The _skip_ argument, if given, must be an iterable of glob-style
     module name patterns.  The debugger will not step into frames that
     originate in a module that matches one of these patterns. (1)

     Example call to enable tracing with _skip_:

         import pdb; pdb.Pdb(skip=['django.*']).set_trace()

     New in version 2.7: The _skip_ argument.

      -- Method: run (statement[, globals[, locals]])
      -- Method: runeval (expression[, globals[, locals]])
      -- Method: runcall (function[, argument, ...])
      -- Method: set_trace ()
          See the documentation for the functions explained above.

  ---------- Footnotes ----------

  (1) Whether a frame is considered to originate in a certain module is
determined by the `__name__' in the frame globals.


File: python.info,  Node: Debugger Commands,  Next: The Python Profilers,  Prev: pdb --- The Python Debugger,  Up: Debugging and Profiling

5.26.3 Debugger Commands
------------------------

The debugger recognizes the following commands.  Most commands can be
abbreviated to one or two letters; e.g. `h(elp)' means that either `h'
or `help' can be used to enter the help command (but not `he' or `hel',
nor `H' or `Help' or `HELP').  Arguments to commands must be separated
by whitespace (spaces or tabs).  Optional arguments are enclosed in
square brackets (`[]') in the command syntax; the square brackets must
not be typed.  Alternatives in the command syntax are separated by a
vertical bar (`|').

  Entering a blank line repeats the last command entered.  Exception:
if the last command was a `list' command, the next 11 lines are listed.

  Commands that the debugger doesn't recognize are assumed to be Python
statements and are executed in the context of the program being
debugged.  Python statements can also be prefixed with an exclamation
point (`!').  This is a powerful way to inspect the program being
debugged; it is even possible to change a variable or call a function.
When an exception occurs in such a statement, the exception name is
printed but the debugger's state is not changed.

  Multiple commands may be entered on a single line, separated by `;;'.
(A single `;' is not used as it is the separator for multiple commands
in a line that is passed to the Python parser.) No intelligence is
applied to separating the commands; the input is split at the first
`;;' pair, even if it is in the middle of a quoted string.

  The debugger supports aliases.  Aliases can have parameters which
allows one a certain level of adaptability to the context under
examination.

  If a file `.pdbrc'  exists in the user's home directory or in the
current directory, it is read in and executed as if it had been typed
at the debugger prompt. This is particularly useful for aliases.  If
both files exist, the one in the home directory is read first and
aliases defined there can be overridden by the local file.

h(elp) [_command_]
     Without argument, print the list of available commands.  With a
     _command_ as argument, print help about that command.  `help pdb'
     displays the full documentation file; if the environment variable `PAGER'
     is defined, the file is piped through that command instead.  Since
     the _command_ argument must be an identifier, `help exec' must be
     entered to get help on the `!' command.

w(here)
     Print a stack trace, with the most recent frame at the bottom.  An
     arrow indicates the current frame, which determines the context of
     most commands.

d(own)
     Move the current frame one level down in the stack trace (to a
     newer frame).

u(p)
     Move the current frame one level up in the stack trace (to an
     older frame).

b(reak) [[_filename_:]_lineno_ | _function_[, _condition_]]
     With a _lineno_ argument, set a break there in the current file.
     With a _function_ argument, set a break at the first executable
     statement within that function. The line number may be prefixed
     with a filename and a colon, to specify a breakpoint in another
     file (probably one that hasn't been loaded yet).  The file is
     searched on `sys.path'. Note that each breakpoint is assigned a
     number to which all the other breakpoint commands refer.

     If a second argument is present, it is an expression which must
     evaluate to true before the breakpoint is honored.

     Without argument, list all breaks, including for each breakpoint,
     the number of times that breakpoint has been hit, the current
     ignore count, and the associated condition if any.

tbreak [[_filename_:]_lineno_ | _function_[, _condition_]]
     Temporary breakpoint, which is removed automatically when it is
     first hit.  The arguments are the same as break.

cl(ear) [_filename:lineno_ | _bpnumber_ [_bpnumber ..._]]
     With a _filename:lineno_ argument, clear all the breakpoints at
     this line.  With a space separated list of breakpoint numbers,
     clear those breakpoints.  Without argument, clear all breaks (but
     first ask confirmation).

disable [_bpnumber_ [_bpnumber ..._]]
     Disables the breakpoints given as a space separated list of
     breakpoint numbers.  Disabling a breakpoint means it cannot cause
     the program to stop execution, but unlike clearing a breakpoint,
     it remains in the list of breakpoints and can be (re-)enabled.

enable [_bpnumber_ [_bpnumber ..._]]
     Enables the breakpoints specified.

ignore _bpnumber_ [_count_]
     Sets the ignore count for the given breakpoint number.  If count
     is omitted, the ignore count is set to 0.  A breakpoint becomes
     active when the ignore count is zero.  When non-zero, the count is
     decremented each time the breakpoint is reached and the breakpoint
     is not disabled and any associated condition evaluates to true.

condition _bpnumber_ [_condition_]
     Condition is an expression which must evaluate to true before the
     breakpoint is honored.  If condition is absent, any existing
     condition is removed; i.e., the breakpoint is made unconditional.

commands [_bpnumber_]
     Specify a list of commands for breakpoint number _bpnumber_.  The
     commands themselves appear on the following lines.  Type a line
     containing just 'end' to terminate the commands. An example:

         (Pdb) commands 1
         (com) print some_variable
         (com) end
         (Pdb)

     To remove all commands from a breakpoint, type commands and follow
     it immediately with  end; that is, give no commands.

     With no _bpnumber_ argument, commands refers to the last
     breakpoint set.

     You can use breakpoint commands to start your program up again.
     Simply use the continue command, or step, or any other command
     that resumes execution.

     Specifying any command resuming execution (currently continue,
     step, next, return, jump, quit and their abbreviations) terminates
     the command list (as if that command was immediately followed by
     end). This is because any time you resume execution (even with a
     simple next or step), you may encounter another breakpoint-which
     could have its own command list, leading to ambiguities about
     which list to execute.

     If you use the 'silent' command in the command list, the usual
     message about stopping at a breakpoint is not printed.  This may
     be desirable for breakpoints that are to print a specific message
     and then continue.  If none of the other commands print anything,
     you see no sign that the breakpoint was reached.

     New in version 2.5.

s(tep)
     Execute the current line, stop at the first possible occasion
     (either in a function that is called or on the next line in the
     current function).

n(ext)
     Continue execution until the next line in the current function is
     reached or it returns.  (The difference between `next' and `step'
     is that `step' stops inside a called function, while `next'
     executes called functions at (nearly) full speed, only stopping at
     the next line in the current function.)

unt(il)
     Continue execution until the line with the line number greater
     than the current one is reached or when returning from current
     frame.

     New in version 2.6.

r(eturn)
     Continue execution until the current function returns.

c(ont(inue))
     Continue execution, only stop when a breakpoint is encountered.

j(ump) _lineno_
     Set the next line that will be executed.  Only available in the
     bottom-most frame.  This lets you jump back and execute code
     again, or jump forward to skip code that you don't want to run.

     It should be noted that not all jumps are allowed -- for instance
     it is not possible to jump into the middle of a *note for: 2e9.
     loop or out of a *note finally: 38f. clause.

l(ist) [_first_[, _last_]]
     List source code for the current file.  Without arguments, list 11
     lines around the current line or continue the previous listing.
     With one argument, list 11 lines around at that line.  With two
     arguments, list the given range; if the second argument is less
     than the first, it is interpreted as a count.

a(rgs)
     Print the argument list of the current function.

p _expression_
     Evaluate the _expression_ in the current context and print its
     value.

          Note: `print' can also be used, but is not a debugger command
          -- this executes the Python *note print: 4d7. statement.

pp _expression_
     Like the `p' command, except the value of the expression is
     pretty-printed using the *note pprint: 138. module.

alias [_name_ [command]]
     Creates an alias called _name_ that executes _command_.  The
     command must _not_ be enclosed in quotes.  Replaceable parameters
     can be indicated by `%1', `%2', and so on, while `%*' is replaced
     by all the parameters.  If no command is given, the current alias
     for _name_ is shown. If no arguments are given, all aliases are
     listed.

     Aliases may be nested and can contain anything that can be legally
     typed at the pdb prompt.  Note that internal pdb commands _can_ be
     overridden by aliases.  Such a command is then hidden until the
     alias is removed.  Aliasing is recursively applied to the first
     word of the command line; all other words in the line are left
     alone.

     As an example, here are two useful aliases (especially when placed
     in the `.pdbrc' file):

         #Print instance variables (usage "pi classInst")
         alias pi for k in %1.__dict__.keys(): print "%1.",k,"=",%1.__dict__[k]
         #Print instance variables in self
         alias ps pi self


unalias _name_
     Deletes the specified alias.

[!]_statement_
     Execute the (one-line) _statement_ in the context of the current
     stack frame.  The exclamation point can be omitted unless the
     first word of the statement resembles a debugger command. To set a
     global variable, you can prefix the assignment command with a
     `global' command on the same line, e.g.:

         (Pdb) global list_options; list_options = ['-l']
         (Pdb)


run [_args_ ...]
     Restart the debugged Python program. If an argument is supplied,
     it is split with "shlex" and the result is used as the new
     sys.argv. History, breakpoints, actions and debugger options are
     preserved. "restart" is an alias for "run".

     New in version 2.6.

q(uit)
     Quit from the debugger. The program being executed is aborted.


File: python.info,  Node: The Python Profilers,  Next: hotshot --- High performance logging profiler,  Prev: Debugger Commands,  Up: Debugging and Profiling

5.26.4 The Python Profilers
---------------------------

*Source code:* Lib/profile.py(1) and Lib/pstats.py(2)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


* Menu:

* Introduction to the profilers::
* Instant User's Manual::
* profile and cProfile Module Reference::
* The Stats Class::
* What Is Deterministic Profiling?::
* Limitations::
* Calibration::
* Using a customer timer::

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/profile.py

  (2) http://hg.python.org/cpython/file/2.7/Lib/pstats.py


File: python.info,  Node: Introduction to the profilers,  Next: Instant User's Manual,  Up: The Python Profilers

5.26.4.1 Introduction to the profilers
......................................

*note cProfile: 74. and *note profile: 139. provide _deterministic
profiling_ of Python programs. A _profile_ is a set of statistics that
describes how often and for how long various parts of the program
executed. These statistics can be formatted into reports via the *note
pstats: 13a. module.

  The Python standard library provides three different implementations
of the same profiling interface:

  1. *note cProfile: 74. is recommended for most users; it's a C
     extension with reasonable overhead that makes it suitable for
     profiling long-running programs.  Based on `lsprof', contributed
     by Brett Rosen and Ted Czotter.

     New in version 2.5.

  2. *note profile: 139, a pure Python module whose interface is
     imitated by *note cProfile: 74, but which adds significant
     overhead to profiled programs.  If you're trying to extend the
     profiler in some way, the task might be easier with this module.

     Changed in version 2.4: Now also reports the time spent in calls
     to built-in functions and methods.

  3. *note hotshot: e9. was an experimental C module that focused on
     minimizing the overhead of profiling, at the expense of longer data
     post-processing times.  It is no longer maintained and may be
     dropped in a future version of Python.

     Changed in version 2.5: The results should be more meaningful than
     in the past: the timing core contained a critical bug.

  The *note profile: 139. and *note cProfile: 74. modules export the
same interface, so they are mostly interchangeable; *note cProfile: 74.
has a much lower overhead but is newer and might not be available on
all systems.  *note cProfile: 74. is really a compatibility layer on
top of the internal `_lsprof' module.  The *note hotshot: e9. module is
reserved for specialized usage.

     Note: The profiler modules are designed to provide an execution
     profile for a given program, not for benchmarking purposes (for
     that, there is *note timeit: 17b. for reasonably accurate
     results).  This particularly applies to benchmarking Python code
     against C code: the profilers introduce overhead for Python code,
     but not for C-level functions, and so the C code would seem faster
     than any Python one.


File: python.info,  Node: Instant User's Manual,  Next: profile and cProfile Module Reference,  Prev: Introduction to the profilers,  Up: The Python Profilers

5.26.4.2 Instant User's Manual
..............................

This section is provided for users that "don't want to read the
manual." It provides a very brief overview, and allows a user to
rapidly perform profiling on an existing application.

  To profile a function that takes a single argument, you can do:

    import cProfile
    import re
    cProfile.run('re.compile("foo|bar")')

(Use *note profile: 139. instead of *note cProfile: 74. if the latter
is not available on your system.)

  The above action would run *note re.compile(): 9ae. and print profile
results like the following:

          197 function calls (192 primitive calls) in 0.002 seconds

    Ordered by: standard name

    ncalls  tottime  percall  cumtime  percall filename:lineno(function)
         1    0.000    0.000    0.001    0.001 <string>:1(<module>)
         1    0.000    0.000    0.001    0.001 re.py:212(compile)
         1    0.000    0.000    0.001    0.001 re.py:268(_compile)
         1    0.000    0.000    0.000    0.000 sre_compile.py:172(_compile_charset)
         1    0.000    0.000    0.000    0.000 sre_compile.py:201(_optimize_charset)
         4    0.000    0.000    0.000    0.000 sre_compile.py:25(_identityfunction)
       3/1    0.000    0.000    0.000    0.000 sre_compile.py:33(_compile)

The first line indicates that 197 calls were monitored.  Of those
calls, 192 were _primitive_, meaning that the call was not induced via
recursion. The next line: `Ordered by: standard name', indicates that
the text string in the far right column was used to sort the output.
The column headings include:

ncalls
     for the number of calls,

tottime
     for the total time spent in the given function (and excluding time
     made in calls to sub-functions)

percall
     is the quotient of `tottime' divided by `ncalls'

cumtime
     is the cumulative time spent in this and all subfunctions (from
     invocation till exit). This figure is accurate _even_ for
     recursive functions.

percall
     is the quotient of `cumtime' divided by primitive calls

filename:lineno(function)
     provides the respective data of each function

  When there are two numbers in the first column (for example `3/1'),
it means that the function recursed.  The second value is the number of
primitive calls and the former is the total number of calls.  Note that
when the function does not recurse, these two values are the same, and
only the single figure is printed.

  Instead of printing the output at the end of the profile run, you can
save the results to a file by specifying a filename to the `run()'
function:

    import cProfile
    import re
    cProfile.run('re.compile("foo|bar")', 'restats')

The *note pstats.Stats: 2327. class reads profile results from a file
and formats them in various ways.

  The file *note cProfile: 74. can also be invoked as a script to
profile another script.  For example:

    python -m cProfile [-o output_file] [-s sort_order] myscript.py

`-o' writes the profile results to a file instead of to stdout

  `-s' specifies one of the *note sort_stats(): 2328. sort values to
sort the output by. This only applies when `-o' is not supplied.

  The *note pstats: 13a. module's *note Stats: 2327. class has a
variety of methods for manipulating and printing the data saved into a
profile results file:

    import pstats
    p = pstats.Stats('restats')
    p.strip_dirs().sort_stats(-1).print_stats()

The *note strip_dirs(): 2329. method removed the extraneous path from
all the module names. The *note sort_stats(): 2328. method sorted all
the entries according to the standard module/line/name string that is
printed. The *note print_stats(): 232a. method printed out all the
statistics.  You might try the following sort calls:

    p.sort_stats('name')
    p.print_stats()

The first call will actually sort the list by function name, and the
second call will print out the statistics.  The following are some
interesting calls to experiment with:

    p.sort_stats('cumulative').print_stats(10)

This sorts the profile by cumulative time in a function, and then only
prints the ten most significant lines.  If you want to understand what
algorithms are taking time, the above line is what you would use.

  If you were looking to see what functions were looping a lot, and
taking a lot of time, you would do:

    p.sort_stats('time').print_stats(10)

to sort according to time spent within each function, and then print the
statistics for the top ten functions.

  You might also try:

    p.sort_stats('file').print_stats('__init__')

This will sort all the statistics by file name, and then print out
statistics for only the class init methods (since they are spelled with
`__init__' in them).  As one final example, you could try:

    p.sort_stats('time', 'cum').print_stats(.5, 'init')

This line sorts statistics with a primary key of time, and a secondary
key of cumulative time, and then prints out some of the statistics. To
be specific, the list is first culled down to 50% (re: `.5') of its
original size, then only lines containing `init' are maintained, and
that sub-sub-list is printed.

  If you wondered what functions called the above functions, you could
now (`p' is still sorted according to the last criteria) do:

    p.print_callers(.5, 'init')

and you would get a list of callers for each of the listed functions.

  If you want more functionality, you're going to have to read the
manual, or guess what the following functions do:

    p.print_callees()
    p.add('restats')

Invoked as a script, the *note pstats: 13a. module is a statistics
browser for reading and examining profile dumps.  It has a simple
line-oriented interface (implemented using *note cmd: 61.) and
interactive help.


File: python.info,  Node: profile and cProfile Module Reference,  Next: The Stats Class,  Prev: Instant User's Manual,  Up: The Python Profilers

5.26.4.3 `profile' and `cProfile' Module Reference
..................................................

Both the *note profile: 139. and *note cProfile: 74. modules provide
the following functions:

 -- Function: profile.run (command, filename=None, sort=-1)
     This function takes a single argument that can be passed to the
     `exec()' function, and an optional file name.  In all cases this
     routine executes:

         exec(command, __main__.__dict__, __main__.__dict__)

     and gathers profiling statistics from the execution. If no file
     name is present, then this function automatically creates a *note
     Stats: 2327.  instance and prints a simple profiling report. If
     the sort value is specified it is passed to this *note Stats:
     2327. instance to control how the results are sorted.

 -- Function: profile.runctx (command, globals, locals, filename=None)
     This function is similar to *note run(): 232c, with added
     arguments to supply the globals and locals dictionaries for the
     _command_ string. This routine executes:

         exec(command, globals, locals)

     and gathers profiling statistics as in the *note run(): 232c.
     function above.

 -- Class: profile.Profile (timer=None, timeunit=0.0, subcalls=True,
          builtins=True)
     This class is normally only used if more precise control over
     profiling is needed than what the `cProfile.run()' function
     provides.

     A custom timer can be supplied for measuring how long code takes
     to run via the _timer_ argument. This must be a function that
     returns a single number representing the current time. If the
     number is an integer, the _timeunit_ specifies a multiplier that
     specifies the duration of each unit of time. For example, if the
     timer returns times measured in thousands of seconds, the time
     unit would be `.001'.

     Directly using the *note Profile: 232e. class allows formatting
     profile results without writing the profile data to a file:

         import cProfile, pstats, io
         pr = cProfile.Profile()
         pr.enable()
         ... do something ...
         pr.disable()
         s = io.StringIO()
         ps = pstats.Stats(pr, stream=s)
         ps.print_results()

     
      -- Method: enable ()
          Start collecting profiling data.

      -- Method: disable ()
          Stop collecting profiling data.

      -- Method: create_stats ()
          Stop collecting profiling data and record the results
          internally as the current profile.

      -- Method: print_stats (sort=-1)
          Create a *note Stats: 2327. object based on the current
          profile and print the results to stdout.

      -- Method: dump_stats (filename)
          Write the results of the current profile to _filename_.

      -- Method: run (cmd)
          Profile the cmd via `exec()'.

      -- Method: runctx (cmd, globals, locals)
          Profile the cmd via `exec()' with the specified global and
          local environment.

      -- Method: runcall (func, *args, **kwargs)
          Profile `func(*args, **kwargs)'


File: python.info,  Node: The Stats Class,  Next: What Is Deterministic Profiling?,  Prev: profile and cProfile Module Reference,  Up: The Python Profilers

5.26.4.4 The `Stats' Class
..........................

Analysis of the profiler data is done using the *note Stats: 2327.
class.  

 -- Class: pstats.Stats (*filenames or profile, stream=sys.stdout)
     This class constructor creates an instance of a "statistics
     object" from a _filename_ (or list of filenames) or from a
     `Profile' instance. Output will be printed to the stream specified
     by _stream_.

     The file selected by the above constructor must have been created
     by the corresponding version of *note profile: 139. or *note
     cProfile: 74.  To be specific, there is _no_ file compatibility
     guaranteed with future versions of this profiler, and there is no
     compatibility with files produced by other profilers.  If several
     files are provided, all the statistics for identical functions
     will be coalesced, so that an overall view of several processes can
     be considered in a single report.  If additional files need to be
     combined with data in an existing *note Stats: 2327. object, the
     *note add(): 2339. method can be used.

     Instead of reading the profile data from a file, a
     `cProfile.Profile' or *note profile.Profile: 232e. object can be
     used as the profile data source.

     *note Stats: 2327. objects have the following methods:

      -- Method: strip_dirs ()
          This method for the *note Stats: 2327. class removes all
          leading path information from file names.  It is very useful
          in reducing the size of the printout to fit within (close to)
          80 columns.  This method modifies the object, and the
          stripped information is lost.  After performing a strip
          operation, the object is considered to have its entries in a
          "random" order, as it was just after object initialization
          and loading.  If *note strip_dirs(): 2329. causes two
          function names to be indistinguishable (they are on the same
          line of the same filename, and have the same function name),
          then the statistics for these two entries are accumulated
          into a single entry.

      -- Method: add (*filenames)
          This method of the *note Stats: 2327. class accumulates
          additional profiling information into the current profiling
          object.  Its arguments should refer to filenames created by
          the corresponding version of *note profile.run(): 232c.  or
          `cProfile.run()'. Statistics for identically named (re: file,
          line, name) functions are automatically accumulated into
          single function statistics.

      -- Method: dump_stats (filename)
          Save the data loaded into the *note Stats: 2327. object to a
          file named _filename_.  The file is created if it does not
          exist, and is overwritten if it already exists.  This is
          equivalent to the method of the same name on the *note
          profile.Profile: 232e. and `cProfile.Profile' classes.

     New in version 2.3.

      -- Method: sort_stats (*keys)
          This method modifies the *note Stats: 2327. object by sorting
          it according to the supplied criteria.  The argument is
          typically a string identifying the basis of a sort (example:
          `'time'' or `'name'').

          When more than one key is provided, then additional keys are
          used as secondary criteria when there is equality in all keys
          selected before them.  For example, `sort_stats('name',
          'file')' will sort all the entries according to their
          function name, and resolve all ties (identical function
          names) by sorting by file name.

          Abbreviations can be used for any key names, as long as the
          abbreviation is unambiguous.  The following are the keys
          currently defined:

          Valid Arg              Meaning
          -------------------------------------------------- 
          `'calls''              call count
          `'cumulative''         cumulative time
          `'cumtime''            cumulative time
          `'file''               file name
          `'filename''           file name
          `'module''             file name
          `'ncalls''             call count
          `'pcalls''             primitive call count
          `'line''               line number
          `'name''               function name
          `'nfl''                name/file/line
          `'stdname''            standard name
          `'time''               internal time
          `'tottime''            internal time

          Note that all sorts on statistics are in descending order
          (placing most time consuming items first), where as name,
          file, and line number searches are in ascending order
          (alphabetical). The subtle distinction between `'nfl'' and
          `'stdname'' is that the standard name is a sort of the name
          as printed, which means that the embedded line numbers get
          compared in an odd way.  For example, lines 3, 20, and 40
          would (if the file names were the same) appear in the string
          order 20, 3 and 40.  In contrast, `'nfl'' does a numeric
          compare of the line numbers.  In fact, `sort_stats('nfl')' is
          the same as `sort_stats('name', 'file', 'line')'.

          For backward-compatibility reasons, the numeric arguments
          `-1', `0', `1', and `2' are permitted.  They are interpreted
          as `'stdname'', `'calls'', `'time'', and `'cumulative''
          respectively.  If this old style format (numeric) is used,
          only one sort key (the numeric key) will be used, and
          additional arguments will be silently ignored.


      -- Method: reverse_order ()
          This method for the *note Stats: 2327. class reverses the
          ordering of the basic list within the object.  Note that by
          default ascending vs descending order is properly selected
          based on the sort key of choice.


      -- Method: print_stats (*restrictions)
          This method for the *note Stats: 2327. class prints out a
          report as described in the *note profile.run(): 232c.
          definition.

          The order of the printing is based on the last *note
          sort_stats(): 2328. operation done on the object (subject to
          caveats in *note add(): 2339. and *note strip_dirs(): 2329.).

          The arguments provided (if any) can be used to limit the list
          down to the significant entries.  Initially, the list is
          taken to be the complete set of profiled functions.  Each
          restriction is either an integer (to select a count of
          lines), or a decimal fraction between 0.0 and 1.0 inclusive
          (to select a percentage of lines), or a regular expression
          (to pattern match the standard name that is printed.  If
          several restrictions are provided, then they are applied
          sequentially.  For example:

              print_stats(.1, 'foo:')

          would first limit the printing to first 10% of list, and then
          only print functions that were part of filename `.*foo:'.  In
          contrast, the command:

              print_stats('foo:', .1)

          would limit the list to all functions having file names
          `.*foo:', and then proceed to only print the first 10% of
          them.

      -- Method: print_callers (*restrictions)
          This method for the *note Stats: 2327. class prints a list of
          all functions that called each function in the profiled
          database.  The ordering is identical to that provided by
          *note print_stats(): 232a, and the definition of the
          restricting argument is also identical.  Each caller is
          reported on its own line.  The format differs slightly
          depending on the profiler that produced the stats:

             * With *note profile: 139, a number is shown in
               parentheses after each caller to show how many times
               this specific call was made.  For convenience, a second
               non-parenthesized number repeats the cumulative time
               spent in the function at the right.

             * With *note cProfile: 74, each caller is preceded by
               three numbers: the number of times this specific call
               was made, and the total and cumulative times spent in
               the current function while it was invoked by this
               specific caller.

      -- Method: print_callees (*restrictions)
          This method for the *note Stats: 2327. class prints a list of
          all function that were called by the indicated function.
          Aside from this reversal of direction of calls (re: called vs
          was called by), the arguments and ordering are identical to
          the *note print_callers(): 233c. method.


File: python.info,  Node: What Is Deterministic Profiling?,  Next: Limitations,  Prev: The Stats Class,  Up: The Python Profilers

5.26.4.5 What Is Deterministic Profiling?
.........................................

_Deterministic profiling_ is meant to reflect the fact that all
_function call_, _function return_, and _exception_ events are
monitored, and precise timings are made for the intervals between these
events (during which time the user's code is executing).  In contrast,
_statistical profiling_ (which is not done by this module) randomly
samples the effective instruction pointer, and deduces where time is
being spent.  The latter technique traditionally involves less overhead
(as the code does not need to be instrumented), but provides only
relative indications of where time is being spent.

  In Python, since there is an interpreter active during execution, the
presence of instrumented code is not required to do deterministic
profiling.  Python automatically provides a _hook_ (optional callback)
for each event.  In addition, the interpreted nature of Python tends to
add so much overhead to execution, that deterministic profiling tends
to only add small processing overhead in typical applications.  The
result is that deterministic profiling is not that expensive, yet
provides extensive run time statistics about the execution of a Python
program.

  Call count statistics can be used to identify bugs in code
(surprising counts), and to identify possible inline-expansion points
(high call counts).  Internal time statistics can be used to identify
"hot loops" that should be carefully optimized.  Cumulative time
statistics should be used to identify high level errors in the
selection of algorithms.  Note that the unusual handling of cumulative
times in this profiler allows statistics for recursive implementations
of algorithms to be directly compared to iterative implementations.


File: python.info,  Node: Limitations,  Next: Calibration,  Prev: What Is Deterministic Profiling?,  Up: The Python Profilers

5.26.4.6 Limitations
....................

One limitation has to do with accuracy of timing information. There is a
fundamental problem with deterministic profilers involving accuracy.
The most obvious restriction is that the underlying "clock" is only
ticking at a rate (typically) of about .001 seconds.  Hence no
measurements will be more accurate than the underlying clock.  If
enough measurements are taken, then the "error" will tend to average
out. Unfortunately, removing this first error induces a second source
of error.

  The second problem is that it "takes a while" from when an event is
dispatched until the profiler's call to get the time actually _gets_
the state of the clock.  Similarly, there is a certain lag when exiting
the profiler event handler from the time that the clock's value was
obtained (and then squirreled away), until the user's code is once
again executing.  As a result, functions that are called many times, or
call many functions, will typically accumulate this error. The error
that accumulates in this fashion is typically less than the accuracy of
the clock (less than one clock tick), but it _can_ accumulate and
become very significant.

  The problem is more important with *note profile: 139. than with the
lower-overhead *note cProfile: 74.  For this reason, *note profile:
139. provides a means of calibrating itself for a given platform so
that this error can be probabilistically (on the average) removed.
After the profiler is calibrated, it will be more accurate (in a least
square sense), but it will sometimes produce negative numbers (when
call counts are exceptionally low, and the gods of probability work
against you :-). )  Do _not_ be alarmed by negative numbers in the
profile.  They should _only_ appear if you have calibrated your
profiler, and the results are actually better than without calibration.


File: python.info,  Node: Calibration,  Next: Using a customer timer,  Prev: Limitations,  Up: The Python Profilers

5.26.4.7 Calibration
....................

The profiler of the *note profile: 139. module subtracts a constant
from each event handling time to compensate for the overhead of calling
the time function, and socking away the results.  By default, the
constant is 0. The following procedure can be used to obtain a better
constant for a given platform (see *note Limitations: 2340.).

    import profile
    pr = profile.Profile()
    for i in range(5):
        print pr.calibrate(10000)

The method executes the number of Python calls given by the argument,
directly and again under the profiler, measuring the time for both. It
then computes the hidden overhead per profiler event, and returns that
as a float.  For example, on a 1.8Ghz Intel Core i5 running Mac OS X,
and using Python's time.clock() as the timer, the magical number is
about 4.04e-6.

  The object of this exercise is to get a fairly consistent result. If
your computer is _very_ fast, or your timer function has poor
resolution, you might have to pass 100000, or even 1000000, to get
consistent results.

  When you have a consistent answer, there are three ways you can use
it: (1)

    import profile

    # 1. Apply computed bias to all Profile instances created hereafter.
    profile.Profile.bias = your_computed_bias

    # 2. Apply computed bias to a specific Profile instance.
    pr = profile.Profile()
    pr.bias = your_computed_bias

    # 3. Specify computed bias in instance constructor.
    pr = profile.Profile(bias=your_computed_bias)

If you have a choice, you are better off choosing a smaller constant,
and then your results will "less often" show up as negative in profile
statistics.

  ---------- Footnotes ----------

  (1) Prior to Python 2.2, it was necessary to edit the profiler source
code to embed the bias as a literal number.  You still can, but that
method is no longer described, because no longer needed.


File: python.info,  Node: Using a customer timer,  Prev: Calibration,  Up: The Python Profilers

5.26.4.8 Using a customer timer
...............................

If you want to change how current time is determined (for example, to
force use of wall-clock time or elapsed process time), pass the timing
function you want to the `Profile' class constructor:

    pr = profile.Profile(your_time_func)

The resulting profiler will then call `your_time_func'. Depending on
whether you are using *note profile.Profile: 232e. or
`cProfile.Profile', `your_time_func''s return value will be interpreted
differently:

*note profile.Profile: 232e.
     `your_time_func' should return a single number, or a list of
     numbers whose sum is the current time (like what *note os.times():
     1168. returns).  If the function returns a single time number, or
     the list of returned numbers has length 2, then you will get an
     especially fast version of the dispatch routine.

     Be warned that you should calibrate the profiler class for the
     timer function that you choose (see *note Calibration: 2343.).
     For most machines, a timer that returns a lone integer value will
     provide the best results in terms of low overhead during
     profiling.  (*note os.times(): 1168. is _pretty_ bad, as it
     returns a tuple of floating point values).  If you want to
     substitute a better timer in the cleanest fashion, derive a class
     and hardwire a replacement dispatch method that best handles your
     timer call, along with the appropriate calibration constant.

`cProfile.Profile'
     `your_time_func' should return a single number.  If it returns
     integers, you can also invoke the class constructor with a second
     argument specifying the real duration of one unit of time.  For
     example, if `your_integer_time_func' returns times measured in
     thousands of seconds, you would construct the `Profile' instance
     as follows:

         pr = cProfile.Profile(your_integer_time_func, 0.001)

     As the `cProfile.Profile' class cannot be calibrated, custom timer
     functions should be used with care and should be as fast as
     possible.  For the best results with a custom timer, it might be
     necessary to hard-code it in the C source of the internal
     `_lsprof' module.


File: python.info,  Node: hotshot --- High performance logging profiler,  Next: timeit --- Measure execution time of small code snippets,  Prev: The Python Profilers,  Up: Debugging and Profiling

5.26.5 `hotshot' -- High performance logging profiler
-----------------------------------------------------

New in version 2.2.

  This module provides a nicer interface to the `_hotshot' C module.
Hotshot is a replacement for the existing *note profile: 139. module.
As it's written mostly in C, it should result in a much smaller
performance impact than the existing *note profile: 139. module.

     Note: The *note hotshot: e9. module focuses on minimizing the
     overhead while profiling, at the expense of long data
     post-processing times. For common usage it is recommended to use
     *note cProfile: 74. instead. *note hotshot: e9. is not maintained
     and might be removed from the standard library in the future.

  Changed in version 2.5: The results should be more meaningful than in
the past: the timing core contained a critical bug.

     Note: The *note hotshot: e9. profiler does not yet work well with
     threads. It is useful to use an unthreaded script to run the
     profiler over the code you're interested in measuring if at all
     possible.

 -- Class: hotshot.Profile (logfile[, lineevents[, linetimings]])
     The profiler object. The argument _logfile_ is the name of a log
     file to use for logged profile data. The argument _lineevents_
     specifies whether to generate events for every source line, or
     just on function call/return. It defaults to `0' (only log
     function call/return). The argument _linetimings_ specifies
     whether to record timing information. It defaults to `1' (store
     timing information).

* Menu:

* Profile Objects::
* Using hotshot data::
* Example Usage::


File: python.info,  Node: Profile Objects,  Next: Using hotshot data,  Up: hotshot --- High performance logging profiler

5.26.5.1 Profile Objects
........................

Profile objects have the following methods:

 -- Method: Profile.addinfo (key, value)
     Add an arbitrary labelled value to the profile output.

 -- Method: Profile.close ()
     Close the logfile and terminate the profiler.

 -- Method: Profile.fileno ()
     Return the file descriptor of the profiler's log file.

 -- Method: Profile.run (cmd)
     Profile an *note exec: 3fd.-compatible string in the script
     environment. The globals from the *note __main__: 2. module are
     used as both the globals and locals for the script.

 -- Method: Profile.runcall (func, *args, **keywords)
     Profile a single call of a callable. Additional positional and
     keyword arguments may be passed along; the result of the call is
     returned, and exceptions are allowed to propagate cleanly, while
     ensuring that profiling is disabled on the way out.

 -- Method: Profile.runctx (cmd, globals, locals)
     Evaluate an *note exec: 3fd.-compatible string in a specific
     environment. The string is compiled before profiling begins.

 -- Method: Profile.start ()
     Start the profiler.

 -- Method: Profile.stop ()
     Stop the profiler.


File: python.info,  Node: Using hotshot data,  Next: Example Usage,  Prev: Profile Objects,  Up: hotshot --- High performance logging profiler

5.26.5.2 Using hotshot data
...........................

New in version 2.2.

  This module loads hotshot profiling data into the standard *note
pstats: 13a. Stats objects.

 -- Function: hotshot.stats.load (filename)
     Load hotshot data from _filename_. Returns an instance of the
     *note pstats.Stats: 2327. class.

See also
........

Module *note profile: 139.
     The *note profile: 139. module's `Stats' class


File: python.info,  Node: Example Usage,  Prev: Using hotshot data,  Up: hotshot --- High performance logging profiler

5.26.5.3 Example Usage
......................

Note that this example runs the Python "benchmark" pystones.  It can
take some time to run, and will produce large output files.

    >>> import hotshot, hotshot.stats, test.pystone
    >>> prof = hotshot.Profile("stones.prof")
    >>> benchtime, stones = prof.runcall(test.pystone.pystones)
    >>> prof.close()
    >>> stats = hotshot.stats.load("stones.prof")
    >>> stats.strip_dirs()
    >>> stats.sort_stats('time', 'calls')
    >>> stats.print_stats(20)
             850004 function calls in 10.090 CPU seconds

       Ordered by: internal time, call count

       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
            1    3.295    3.295   10.090   10.090 pystone.py:79(Proc0)
       150000    1.315    0.000    1.315    0.000 pystone.py:203(Proc7)
        50000    1.313    0.000    1.463    0.000 pystone.py:229(Func2)
     .
     .
     .



File: python.info,  Node: timeit --- Measure execution time of small code snippets,  Next: trace --- Trace or track Python statement execution,  Prev: hotshot --- High performance logging profiler,  Up: Debugging and Profiling

5.26.6 `timeit' -- Measure execution time of small code snippets
----------------------------------------------------------------

New in version 2.3.

  *Source code:* Lib/timeit.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  This module provides a simple way to time small bits of Python code.
It has both a *note Command-Line Interface: 2359. as well as a *note
callable: 235a.  one.  It avoids a number of common traps for measuring
execution times.  See also Tim Peters' introduction to the "Algorithms"
chapter in the _Python Cookbook_, published by O'Reilly.

* Menu:

* Basic Examples: Basic Examples<2>.
* Python Interface::
* Command-Line Interface: Command-Line Interface<2>.
* Examples: Examples<17>.

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/timeit.py


File: python.info,  Node: Basic Examples<2>,  Next: Python Interface,  Up: timeit --- Measure execution time of small code snippets

5.26.6.1 Basic Examples
.......................

The following example shows how the *note Command-Line Interface: 2359.
can be used to compare three different expressions:

    $ python -m timeit '"-".join(str(n) for n in range(100))'
    10000 loops, best of 3: 40.3 usec per loop
    $ python -m timeit '"-".join([str(n) for n in range(100)])'
    10000 loops, best of 3: 33.4 usec per loop
    $ python -m timeit '"-".join(map(str, range(100)))'
    10000 loops, best of 3: 25.2 usec per loop

This can be achieved from the *note Python Interface: 235a. with:

    >>> import timeit
    >>> timeit.timeit('"-".join(str(n) for n in range(100))', number=10000)
    0.8187260627746582
    >>> timeit.timeit('"-".join([str(n) for n in range(100)])', number=10000)
    0.7288308143615723
    >>> timeit.timeit('"-".join(map(str, range(100)))', number=10000)
    0.5858950614929199

Note however that *note timeit: 17b. will automatically determine the
number of repetitions only when the command-line interface is used.  In
the *note Examples: 235c. section you can find more advanced examples.


File: python.info,  Node: Python Interface,  Next: Command-Line Interface<2>,  Prev: Basic Examples<2>,  Up: timeit --- Measure execution time of small code snippets

5.26.6.2 Python Interface
.........................

The module defines three convenience functions and a public class:

 -- Function: timeit.timeit (stmt='pass', setup='pass', timer=<default
          timer>, number=1000000)
     Create a *note Timer: 235f. instance with the given statement,
     _setup_ code and _timer_ function and run its *note timeit():
     2360. method with _number_ executions.

     New in version 2.6.

 -- Function: timeit.repeat (stmt='pass', setup='pass', timer=<default
          timer>, repeat=3, number=1000000)
     Create a *note Timer: 235f. instance with the given statement,
     _setup_ code and _timer_ function and run its *note repeat():
     2362. method with the given _repeat_ count and _number_ executions.

     New in version 2.6.

 -- Function: timeit.default_timer ()
     Define a default timer, in a platform-specific manner.  On Windows,
     *note time.clock(): 11d4. has microsecond granularity, but *note
     time.time(): 45a.'s granularity is 1/60th of a second.  On Unix,
     *note time.clock(): 11d4. has 1/100th of a second granularity, and
     *note time.time(): 45a. is much more precise.  On either platform,
     *note default_timer(): 2363. measures wall clock time, not the CPU
     time.  This means that other processes running on the same
     computer may interfere with the timing.

 -- Class: timeit.Timer (stmt='pass', setup='pass', timer=<timer
          function>)
     Class for timing execution speed of small code snippets.

     The constructor takes a statement to be timed, an additional
     statement used for setup, and a timer function.  Both statements
     default to `'pass''; the timer function is platform-dependent (see
     the module doc string).  _stmt_ and _setup_ may also contain
     multiple statements separated by `;' or newlines, as long as they
     don't contain multi-line string literals.

     To measure the execution time of the first statement, use the
     *note timeit(): 2360.  method.  The *note repeat(): 2362. method
     is a convenience to call *note timeit(): 2360.  multiple times and
     return a list of results.

     Changed in version 2.6: The _stmt_ and _setup_ parameters can now
     also take objects that are callable without arguments.  This will
     embed calls to them in a timer function that will then be executed
     by *note timeit(): 2360.  Note that the timing overhead is a
     little larger in this case because of the extra function calls.

      -- Method: timeit (number=1000000)
          Time _number_ executions of the main statement.  This
          executes the setup statement once, and then returns the time
          it takes to execute the main statement a number of times,
          measured in seconds as a float.  The argument is the number
          of times through the loop, defaulting to one million.  The
          main statement, the setup statement and the timer function to
          be used are passed to the constructor.

               Note: By default, *note timeit(): 2360. temporarily
               turns off *note garbage collection: 5fc. during the
               timing.  The advantage of this approach is that it makes
               independent timings more comparable.  This disadvantage
               is that GC may be an important component of the
               performance of the function being measured.  If so, GC
               can be re-enabled as the first statement in the _setup_
               string.  For example:

                   timeit.Timer('for i in xrange(10): oct(i)', 'gc.enable()').timeit()



      -- Method: repeat (repeat=3, number=1000000)
          Call *note timeit(): 2360. a few times.

          This is a convenience function that calls the *note timeit():
          2360. repeatedly, returning a list of results.  The first
          argument specifies how many times to call *note timeit():
          2360.  The second argument specifies the _number_ argument
          for *note timeit(): 2360.

               Note: It's tempting to calculate mean and standard
               deviation from the result vector and report these.
               However, this is not very useful.  In a typical case,
               the lowest value gives a lower bound for how fast your
               machine can run the given code snippet; higher values in
               the result vector are typically not caused by
               variability in Python's speed, but by other processes
               interfering with your timing accuracy.  So the *note
               min(): 221. of the result is probably the only number you
               should be interested in.  After that, you should look at
               the entire vector and apply common sense rather than
               statistics.

      -- Method: print_exc (file=None)
          Helper to print a traceback from the timed code.

          Typical use:

              t = Timer(...)       # outside the try/except
              try:
                  t.timeit(...)    # or t.repeat(...)
              except:
                  t.print_exc()

          The advantage over the standard traceback is that source
          lines in the compiled template will be displayed. The
          optional _file_ argument directs where the traceback is sent;
          it defaults to *note sys.stderr: 634.


File: python.info,  Node: Command-Line Interface<2>,  Next: Examples<17>,  Prev: Python Interface,  Up: timeit --- Measure execution time of small code snippets

5.26.6.3 Command-Line Interface
...............................

When called as a program from the command line, the following form is
used:

    python -m timeit [-n N] [-r N] [-s S] [-t] [-c] [-h] [statement ...]

Where the following options are understood:

 -- Program Option: -n N, -number=N
     how many times to execute 'statement'

 -- Program Option: -r N, -repeat=N
     how many times to repeat the timer (default 3)

 -- Program Option: -s S, -setup=S
     statement to be executed once initially (default `pass')

 -- Program Option: -t, -time
     use *note time.time(): 45a. (default on all platforms but Windows)

 -- Program Option: -c, -clock
     use *note time.clock(): 11d4. (default on Windows)

 -- Program Option: -v, -verbose
     print raw timing results; repeat for more digits precision

 -- Program Option: -h, -help
     print a short usage message and exit

  A multi-line statement may be given by specifying each line as a
separate statement argument; indented lines are possible by enclosing
an argument in quotes and using leading spaces.  Multiple *note -s:
2368. options are treated similarly.

  If *note -n: 2366. is not given, a suitable number of loops is
calculated by trying successive powers of 10 until the total time is at
least 0.2 seconds.

  *note default_timer(): 2363. measurations can be affected by other
programs running on the same machine, so the best thing to do when
accurate timing is necessary is to repeat the timing a few times and
use the best time.  The *note -r: 2367. option is good for this; the
default of 3 repetitions is probably enough in most cases.  On Unix,
you can use *note time.clock(): 11d4. to measure CPU time.

     Note: There is a certain baseline overhead associated with
     executing a pass statement.  The code here doesn't try to hide it,
     but you should be aware of it.  The baseline overhead can be
     measured by invoking the program without arguments, and it might
     differ between Python versions.  Also, to fairly compare older
     Python versions to Python 2.3, you may want to use Python's `-O'
     option for the older versions to avoid timing `SET_LINENO'
     instructions.


File: python.info,  Node: Examples<17>,  Prev: Command-Line Interface<2>,  Up: timeit --- Measure execution time of small code snippets

5.26.6.4 Examples
.................

It is possible to provide a setup statement that is executed only once
at the beginning:

    $ python -m timeit -s 'text = "sample string"; char = "g"'  'char in text'
    10000000 loops, best of 3: 0.0877 usec per loop
    $ python -m timeit -s 'text = "sample string"; char = "g"'  'text.find(char)'
    1000000 loops, best of 3: 0.342 usec per loop


    >>> import timeit
    >>> timeit.timeit('char in text', setup='text = "sample string"; char = "g"')
    0.41440500499993504
    >>> timeit.timeit('text.find(char)', setup='text = "sample string"; char = "g"')
    1.7246671520006203

The same can be done using the *note Timer: 235f. class and its methods:

    >>> import timeit
    >>> t = timeit.Timer('char in text', setup='text = "sample string"; char = "g"')
    >>> t.timeit()
    0.3955516149999312
    >>> t.repeat()
    [0.40193588800002544, 0.3960157959998014, 0.39594301399984033]

The following examples show how to time expressions that contain
multiple lines.  Here we compare the cost of using *note hasattr():
329. vs. *note try: 38e./*note except: 390.  to test for missing and
present object attributes:

    $ python -m timeit 'try:' '  str.__nonzero__' 'except AttributeError:' '  pass'
    100000 loops, best of 3: 15.7 usec per loop
    $ python -m timeit 'if hasattr(str, "__nonzero__"): pass'
    100000 loops, best of 3: 4.26 usec per loop

    $ python -m timeit 'try:' '  int.__nonzero__' 'except AttributeError:' '  pass'
    1000000 loops, best of 3: 1.43 usec per loop
    $ python -m timeit 'if hasattr(int, "__nonzero__"): pass'
    100000 loops, best of 3: 2.23 usec per loop


    >>> import timeit
    >>> # attribute is missing
    >>> s = """\
    ... try:
    ...     str.__nonzero__
    ... except AttributeError:
    ...     pass
    ... """
    >>> timeit.timeit(stmt=s, number=100000)
    0.9138244460009446
    >>> s = "if hasattr(str, '__bool__'): pass"
    >>> timeit.timeit(stmt=s, number=100000)
    0.5829014980008651
    >>>
    >>> # attribute is present
    >>> s = """\
    ... try:
    ...     int.__nonzero__
    ... except AttributeError:
    ...     pass
    ... """
    >>> timeit.timeit(stmt=s, number=100000)
    0.04215312199994514
    >>> s = "if hasattr(int, '__bool__'): pass"
    >>> timeit.timeit(stmt=s, number=100000)
    0.08588060699912603

To give the *note timeit: 17b. module access to functions you define,
you can pass a _setup_ parameter which contains an import statement:

    def test():
        """Stupid test function"""
        L = []
        for i in range(100):
            L.append(i)

    if __name__ == '__main__':
        import timeit
        print(timeit.timeit("test()", setup="from __main__ import test"))



File: python.info,  Node: trace --- Trace or track Python statement execution,  Prev: timeit --- Measure execution time of small code snippets,  Up: Debugging and Profiling

5.26.7 `trace' -- Trace or track Python statement execution
-----------------------------------------------------------

*Source code:* Lib/trace.py(1)

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  The *note trace: 180. module allows you to trace program execution,
generate annotated statement coverage listings, print caller/callee
relationships and list functions executed during a program run.  It can
be used in another program or from the command line.

* Menu:

* Command-Line Usage::
* Programmatic Interface::

Command-Line Usage

* Main options::
* Modifiers::
* Filters::

  ---------- Footnotes ----------

  (1) http://hg.python.org/cpython/file/2.7/Lib/trace.py


File: python.info,  Node: Command-Line Usage,  Next: Programmatic Interface,  Up: trace --- Trace or track Python statement execution

5.26.7.1 Command-Line Usage
...........................

The *note trace: 180. module can be invoked from the command line.  It
can be as simple as

    python -m trace --count -C . somefile.py ...

The above will execute `somefile.py' and generate annotated listings of
all Python modules imported during the execution into the current
directory.

 -- Program Option: -help
     Display usage and exit.

 -- Program Option: -version
     Display the version of the module and exit.

* Menu:

* Main options::
* Modifiers::
* Filters::


File: python.info,  Node: Main options,  Next: Modifiers,  Up: Command-Line Usage

5.26.7.2 Main options
.....................

At least one of the following options must be specified when invoking
*note trace: 180.  The *note -listfuncs: 2375. option is mutually
exclusive with the *note -trace: 2376. and *note -counts: 2377. options
. When *note -listfuncs: 2375. is provided, neither *note -counts:
2377. nor *note -trace: 2376. are accepted, and vice versa.

 -- Program Option: -c, -count
     Produce a set of annotated listing files upon program completion
     that shows how many times each statement was executed.  See also
     *note -coverdir: 2378, *note -file: 2379. and *note -no-report:
     237a. below.

 -- Program Option: -t, -trace
     Display lines as they are executed.

 -- Program Option: -l, -listfuncs
     Display the functions executed by running the program.

 -- Program Option: -r, -report
     Produce an annotated list from an earlier program run that used the
     *note -count: 2377. and *note -file: 2379. option.  This does not
     execute any code.

 -- Program Option: -T, -trackcalls
     Display the calling relationships exposed by running the program.


File: python.info,  Node: Modifiers,  Next: Filters,  Prev: Main options,  Up: Command-Line Usage

5.26.7.3 Modifiers
..................

 -- Program Option: -f, -file=<file>
     Name of a file to accumulate counts over several tracing runs.
     Should be used with the *note -count: 2377. option.

 -- Program Option: -C, -coverdir=<dir>
     Directory where the report files go.  The coverage report for
     `package.module' is written to file
     `_dir_/_package_/_module_.cover'.

 -- Program Option: -m, -missing
     When generating annotated listings, mark lines which were not
     executed with `>>>>>>'.

 -- Program Option: -s, -summary
     When using *note -count: 2377. or *note -report: 237b, write a
     brief summary to stdout for each file processed.

 -- Program Option: -R, -no-report
     Do not generate annotated listings.  This is useful if you intend
     to make several runs with *note -count: 2377, and then produce a
     single set of annotated listings at the end.

 -- Program Option: -g, -timing
     Prefix each line with the time since the program started.  Only
     used while tracing.


File: python.info,  Node: Filters,  Prev: Modifiers,  Up: Command-Line Usage

5.26.7.4 Filters
................

These options may be repeated multiple times.

 -- Program Option: -ignore-module=<mod>
     Ignore each of the given module names and its submodules (if it is
     a package).  The argument can be a list of names separated by a
     comma.

 -- Program Option: -ignore-dir=<dir>
     Ignore all modules and packages in the named directory and
     subdirectories.  The argument can be a list of directories
     separated by *note os.pathsep: 63e.


File: python.info,  Node: Programmatic Interface,  Prev: Command-Line Usage,  Up: trace --- Trace or track Python statement execution

5.26.7.5 Programmatic Interface
...............................

 -- Class: trace.Trace ([count=1[, trace=1[, countfuncs=0[,
          countcallers=0[, ignoremods=()[, ignoredirs=()[,
          infile=None[, outfile=None[, timing=False]]]]]]]]])
     Create an object to trace execution of a single statement or
     expression.  All parameters are optional.  _count_ enables
     counting of line numbers.  _trace_ enables line execution tracing.
     _countfuncs_ enables listing of the functions called during the
     run.  _countcallers_ enables call relationship tracking.
     _ignoremods_ is a list of modules or packages to ignore.
     _ignoredirs_ is a list of directories whose modules or packages
     should be ignored.  _infile_ is the name of the file from which to
     read stored count information.  _outfile_ is the name of the file
     in which to write updated count information.  _timing_ enables a
     timestamp relative to when tracing was started to be displayed.

           -- Method: run (cmd)
               Execute the command and gather statistics from the
               execution with the current tracing parameters.  _cmd_
               must be a string or code object, suitable for passing
               into `exec()'.

           -- Method: runctx (cmd, globals=None, locals=None)
               Execute the command and gather statistics from the
               execution with the current tracing parameters, in the
               defined global and local environments.  If not defined,
               _globals_ and _locals_ default to empty dictionaries.

           -- Method: runfunc (func, *args, **kwds)
               Call _func_ with the given arguments under control of
               the *note Trace: 2386.  object with the current tracing
               parameters.

           -- Method: results ()
               Return a *note CoverageResults: 238b. object that
               contains the cumulative results of all previous calls to
               `run', `runctx' and `runfunc' for the given *note Trace:
               2386. instance.  Does not reset the accumulated trace
               results.

 -- Class: trace.CoverageResults
     A container for coverage results, created by *note
     Trace.results(): 238a.  Should not be created directly by the user.

           -- Method: update (other)
               Merge in data from another *note CoverageResults: 238b.
               object.

           -- Method: write_results ([show_missing=True[,
                    summary=False[, coverdir=None]]])
               Write coverage results.  Set _show_missing_ to show
               lines that had no hits.  Set _summary_ to include in the
               output the coverage summary per module.  _coverdir_
               specifies the directory into which the coverage result
               files will be output.  If `None', the results for each
               source file are placed in its directory.

A simple example demonstrating the use of the programmatic interface:

    import sys
    import trace

    # create a Trace object, telling it what to ignore, and whether to
    # do tracing or line-counting or both.
    tracer = trace.Trace(
        ignoredirs=[sys.prefix, sys.exec_prefix],
        trace=0,
        count=1)

    # run the new command using the given tracer
    tracer.run('main()')

    # make a report, placing output in the current directory
    r = tracer.results()
    r.write_results(show_missing=True, coverdir=".")




Local Variables:
coding: utf-8
End:
