This is
/home/melpa/melpa/working/python-info-20130916.1420/python.info,
produced by makeinfo version 4.13 from
/home/melpa/melpa/working/python-info/python.texi.

Generated by Sphinx 1.1.3.
INFO-DIR-SECTION Programming
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY

     Python 2.7.5, September 16, 2013

     Georg Brandl

     Copyright (C) 1990-2013, Python Software Foundation


File: python.info,  Node: distutils unixccompiler --- Unix C Compiler,  Next: distutils msvccompiler --- Microsoft Compiler,  Prev: distutils ccompiler --- CCompiler base class,  Up: API Reference

8.10.3 `distutils.unixccompiler' -- Unix C Compiler
---------------------------------------------------

This module provides the `UnixCCompiler' class, a subclass of
`CCompiler' that handles the typical Unix-style command-line  C
compiler:

   * macros defined with `-Dname[=value]'

   * macros undefined with `-Uname'

   * include search directories specified with `-Idir'

   * libraries specified with `-llib'

   * library search directories specified with `-Ldir'

   * compile handled by *cc* (or similar) executable with *note -c: 278.
     option: compiles `.c' to `.o'

   * link static library handled by *ar* command (possibly with
     *ranlib*)

   * link shared library handled by *cc* `-shared'


File: python.info,  Node: distutils msvccompiler --- Microsoft Compiler,  Next: distutils bcppcompiler --- Borland Compiler,  Prev: distutils unixccompiler --- Unix C Compiler,  Up: API Reference

8.10.4 `distutils.msvccompiler' -- Microsoft Compiler
-----------------------------------------------------

This module provides `MSVCCompiler', an implementation of the abstract
`CCompiler' class for Microsoft Visual Studio. Typically, extension
modules need to be compiled with the same compiler that was used to
compile Python. For Python 2.3 and earlier, the compiler was Visual
Studio 6. For Python 2.4 and 2.5, the compiler is Visual Studio .NET
2003. The AMD64 and Itanium binaries are created using the Platform SDK.

  `MSVCCompiler' will normally choose the right compiler, linker etc. on
its own. To override this choice, the environment variables
_DISTUTILS_USE_SDK_ and _MSSdk_ must be both set. _MSSdk_ indicates
that the current environment has been setup by the SDK's `SetEnv.Cmd'
script, or that the environment variables had been registered when the
SDK was installed; _DISTUTILS_USE_SDK_ indicates that the distutils
user has made an explicit choice to override the compiler selection by
`MSVCCompiler'.


File: python.info,  Node: distutils bcppcompiler --- Borland Compiler,  Next: distutils cygwincompiler --- Cygwin Compiler,  Prev: distutils msvccompiler --- Microsoft Compiler,  Up: API Reference

8.10.5 `distutils.bcppcompiler' -- Borland Compiler
---------------------------------------------------

This module provides `BorlandCCompiler', an subclass of the abstract
`CCompiler' class for the Borland C++ compiler.


File: python.info,  Node: distutils cygwincompiler --- Cygwin Compiler,  Next: distutils emxccompiler --- OS/2 EMX Compiler,  Prev: distutils bcppcompiler --- Borland Compiler,  Up: API Reference

8.10.6 `distutils.cygwincompiler' -- Cygwin Compiler
----------------------------------------------------

This module provides the `CygwinCCompiler' class, a subclass of
`UnixCCompiler' that handles the Cygwin port of the GNU C compiler to
Windows.  It also contains the Mingw32CCompiler class which handles the
mingw32 port of GCC (same as cygwin in no-cygwin mode).


File: python.info,  Node: distutils emxccompiler --- OS/2 EMX Compiler,  Next: distutils archive_util --- Archiving utilities,  Prev: distutils cygwincompiler --- Cygwin Compiler,  Up: API Reference

8.10.7 `distutils.emxccompiler' -- OS/2 EMX Compiler
----------------------------------------------------

This module provides the EMXCCompiler class, a subclass of
`UnixCCompiler' that handles the EMX port of the GNU C compiler to OS/2.


File: python.info,  Node: distutils archive_util --- Archiving utilities,  Next: distutils dep_util --- Dependency checking,  Prev: distutils emxccompiler --- OS/2 EMX Compiler,  Up: API Reference

8.10.8 `distutils.archive_util' --  Archiving utilities
-------------------------------------------------------

This module provides a few functions for creating archive files, such as
tarballs or zipfiles.

 -- Function: distutils.archive_util.make_archive (base_name, format[,
          root_dir=None, base_dir=None, verbose=0, dry_run=0])
     Create an archive file (eg. `zip' or `tar').  _base_name_  is the
     name of the file to create, minus any format-specific extension;
     _format_ is the archive format: one of `zip', `tar',  `ztar', or
     `gztar'. _root_dir_ is a directory that will be the root directory
     of the archive; ie. we typically `chdir' into _root_dir_ before
     creating the archive.  _base_dir_ is the directory where we start
     archiving from; ie. _base_dir_ will be the common prefix of all
     files and directories in the archive.  _root_dir_ and _base_dir_
     both default to the current directory.  Returns the name of the
     archive file.

 -- Function: distutils.archive_util.make_tarball (base_name,
          base_dir[, compress='gzip', verbose=0, dry_run=0])
     'Create an (optional compressed) archive as a tar file from all
     files in and under _base_dir_. _compress_ must be `'gzip'' (the
     default),  `'compress'', `'bzip2'', or `None'.  Both *tar* and the
     compression utility named by _compress_ must be on the  default
     program search path, so this is probably Unix-specific.  The
     output tar file will be named `base_dir.tar', possibly plus the
     appropriate compression extension (`.gz', `.bz2' or `.Z').  Return
     the output filename.

 -- Function: distutils.archive_util.make_zipfile (base_name,
          base_dir[, verbose=0, dry_run=0])
     Create a zip file from all files in and under _base_dir_.  The
     output zip file will be named _base_name_ + `.zip'.  Uses either
     the  *note zipfile: 1ab. Python module (if available) or the
     InfoZIP `zip'  utility (if installed and found on the default
     search path).  If neither  tool is available, raises
     `DistutilsExecError'.   Returns the name of the output zip file.


File: python.info,  Node: distutils dep_util --- Dependency checking,  Next: distutils dir_util --- Directory tree operations,  Prev: distutils archive_util --- Archiving utilities,  Up: API Reference

8.10.9 `distutils.dep_util' -- Dependency checking
--------------------------------------------------

This module provides functions for performing simple, timestamp-based
dependency of files and groups of files; also, functions based entirely
on such timestamp dependency analysis.

 -- Function: distutils.dep_util.newer (source, target)
     Return true if _source_ exists and is more recently modified than
     _target_, or if _source_ exists and _target_ doesn't. Return false
     if both exist and _target_ is the same age or newer  than
     _source_. Raise `DistutilsFileError' if _source_ does not exist.

 -- Function: distutils.dep_util.newer_pairwise (sources, targets)
     Walk two filename lists in parallel, testing if each source is
     newer than its corresponding target.  Return a pair of lists
     (_sources_, _targets_) where source is newer than target,
     according to the semantics of *note newer(): 2f00.


 -- Function: distutils.dep_util.newer_group (sources, target[,
          missing='error'])
     Return true if _target_ is out-of-date with respect to any file
     listed in _sources_  In other words, if _target_ exists and is
     newer than every file in _sources_, return false; otherwise return
     true. _missing_ controls what we do when a source file is missing;
     the default (`'error'') is to blow up with an *note OSError: 22e.
     from  inside *note os.stat(): 3bd.; if it is `'ignore'', we
     silently drop any missing source files; if it is `'newer'', any
     missing source files make us assume that _target_ is out-of-date
     (this is handy in "dry-run" mode: it'll make you pretend to carry
     out commands that wouldn't work because inputs are missing, but
     that doesn't matter because you're not actually going to run the
     commands).


File: python.info,  Node: distutils dir_util --- Directory tree operations,  Next: distutils file_util --- Single file operations,  Prev: distutils dep_util --- Dependency checking,  Up: API Reference

8.10.10 `distutils.dir_util' -- Directory tree operations
---------------------------------------------------------

This module provides functions for operating on directories and trees of
directories.

 -- Function: distutils.dir_util.mkpath (name[, mode=0777, verbose=0,
          dry_run=0])
     Create a directory and any missing ancestor directories.  If the
     directory already exists (or if _name_ is the empty string, which
     means the current directory, which of course exists), then do
     nothing.  Raise `DistutilsFileError' if unable to create some
     directory along the way (eg.  some sub-path exists, but is a file
     rather than a directory).  If _verbose_ is true, print a one-line
     summary of each mkdir to stdout.  Return the list of directories
     actually created.

 -- Function: distutils.dir_util.create_tree (base_dir, files[,
          mode=0777, verbose=0, dry_run=0])
     Create all the empty directories under _base_dir_ needed to put
     _files_ there.  _base_dir_ is just the a name of a directory which
     doesn't necessarily exist yet; _files_ is a list of filenames to
     be interpreted relative to _base_dir_.  _base_dir_ + the directory
     portion of every file in _files_ will be created if it doesn't
     already exist.  _mode_, _verbose_ and _dry_run_ flags  are as for
     *note mkpath(): 2ef0.

 -- Function: distutils.dir_util.copy_tree (src, dst[, preserve_mode=1,
          preserve_times=1, preserve_symlinks=0, update=0, verbose=0,
          dry_run=0])
     Copy an entire directory tree _src_ to a new location _dst_.  Both
     _src_ and _dst_ must be directory names.  If _src_ is not a
     directory, raise `DistutilsFileError'.  If _dst_ does  not exist,
     it is created with *note mkpath(): 2ef0.  The end result of the
     copy is that every file in _src_ is copied to _dst_, and
     directories under _src_ are recursively copied to _dst_.  Return
     the list of files that were copied or might have been copied,
     using their output name. The return value is unaffected by
     _update_ or _dry_run_: it is simply the list of all files under
     _src_, with the names changed to be under _dst_.

     _preserve_mode_ and _preserve_times_ are the same as for
     `copy_file()' in *note distutils.file_util: aa.; note that they
     only apply to regular files, not to directories.  If
     _preserve_symlinks_ is true, symlinks will be copied as symlinks
     (on platforms that support them!); otherwise (the default), the
     destination of the symlink will be copied.  _update_ and _verbose_
     are the same as for `copy_file()'.

     Files in _src_ that begin with `.nfs' are skipped (more
     information on these files is available in answer D2 of the NFS
     FAQ page(1).

     Changed in version 2.7.4: NFS files are ignored.

 -- Function: distutils.dir_util.remove_tree (directory[, verbose=0,
          dry_run=0])
     Recursively remove _directory_ and all files and directories
     underneath it. Any errors are ignored (apart from being reported
     to `sys.stdout' if _verbose_ is true).

  ---------- Footnotes ----------

  (1) http://nfs.sourceforge.net/#section_d


File: python.info,  Node: distutils file_util --- Single file operations,  Next: distutils util --- Miscellaneous other utility functions,  Prev: distutils dir_util --- Directory tree operations,  Up: API Reference

8.10.11 `distutils.file_util' -- Single file operations
-------------------------------------------------------

This module contains some utility functions for operating on individual
files.

 -- Function: distutils.file_util.copy_file (src, dst[,
          preserve_mode=1, preserve_times=1, update=0, link=None,
          verbose=0, dry_run=0])
     Copy file _src_ to _dst_. If _dst_ is a directory, then _src_ is
     copied there with the same name; otherwise, it must be a filename.
     (If the file exists, it will be ruthlessly clobbered.) If
     _preserve_mode_ is true (the default), the file's mode (type and
     permission bits, or whatever is analogous on the current platform)
     is copied. If _preserve_times_ is true (the default), the
     last-modified and last-access times are copied as well. If
     _update_ is true, _src_ will only be copied if _dst_ does not
     exist, or if _dst_ does exist but is older than _src_.

     _link_ allows you to make hard links (using *note os.link():
     1125.) or symbolic links (using *note os.symlink(): 1134.) instead
     of copying: set it to `'hard'' or `'sym''; if it is `None' (the
     default), files are copied. Don't set _link_ on systems that don't
     support it: *note copy_file(): 2f08. doesn't check if hard or
     symbolic linking is available.  It uses `_copy_file_contents()' to
     copy file contents.

     Return a tuple `(dest_name, copied)': _dest_name_ is the actual
     name of the output file, and _copied_ is true if the file was
     copied  (or would have been copied, if _dry_run_ true).


 -- Function: distutils.file_util.move_file (src, dst[, verbose,
          dry_run])
     Move file _src_ to _dst_. If _dst_ is a directory, the file will
     be moved into it with the same name; otherwise, _src_ is just
     renamed to _dst_.  Returns the new full name of the file.

          Warning: Handles cross-device moves on Unix using *note
          copy_file(): 2f08.  What about other systems?

 -- Function: distutils.file_util.write_file (filename, contents)
     Create a file called _filename_ and write _contents_ (a sequence
     of strings without line terminators) to it.


File: python.info,  Node: distutils util --- Miscellaneous other utility functions,  Next: distutils dist --- The Distribution class,  Prev: distutils file_util --- Single file operations,  Up: API Reference

8.10.12 `distutils.util' -- Miscellaneous other utility functions
-----------------------------------------------------------------

This module contains other assorted bits and pieces that don't fit into
any other utility module.

 -- Function: distutils.util.get_platform ()
     Return a string that identifies the current platform.  This is
     used mainly to distinguish platform-specific build directories and
     platform-specific built distributions.  Typically includes the OS
     name and version and the architecture (as supplied by
     'os.uname()'), although the exact information included depends on
     the OS; eg. for IRIX the architecture isn't particularly important
     (IRIX only runs on SGI hardware), but for Linux the kernel version
     isn't particularly important.

     Examples of returned values:

        * `linux-i586'

        * `linux-alpha'

        * `solaris-2.6-sun4u'

        * `irix-5.3'

        * `irix64-6.2'

     For non-POSIX platforms, currently just returns `sys.platform'.

     For Mac OS X systems the OS version reflects the minimal version
     on which binaries will run (that is, the value of
     `MACOSX_DEPLOYMENT_TARGET' during the build of Python), not the OS
     version of the current system.

     For universal binary builds on Mac OS X the architecture value
     reflects the univeral binary status instead of the architecture of
     the current processor. For 32-bit universal binaries the
     architecture is `fat', for 64-bit universal binaries the
     architecture is `fat64', and for 4-way universal binaries the
     architecture is `universal'. Starting from Python 2.7 and Python
     3.2 the architecture `fat3' is used for a 3-way universal build
     (ppc, i386, x86_64) and `intel' is used for a univeral build with
     the i386 and x86_64 architectures

     Examples of returned values on Mac OS X:

        * `macosx-10.3-ppc'

        * `macosx-10.3-fat'

        * `macosx-10.5-universal'

        * `macosx-10.6-intel'

 -- Function: distutils.util.convert_path (pathname)
     Return 'pathname' as a name that will work on the native
     filesystem, i.e. split it on '/' and put it back together again
     using the current directory separator.  Needed because filenames
     in the setup script are always supplied in Unix style, and have to
     be converted to the local convention before we can actually use
     them in the filesystem.  Raises *note ValueError: 233. on
     non-Unix-ish systems if _pathname_ either  starts or ends with a
     slash.

 -- Function: distutils.util.change_root (new_root, pathname)
     Return _pathname_ with _new_root_ prepended.  If _pathname_ is
     relative, this is equivalent to `os.path.join(new_root,pathname)'
     Otherwise, it requires making _pathname_ relative and then joining
     the two, which is tricky on DOS/Windows.

 -- Function: distutils.util.check_environ ()
     Ensure that 'os.environ' has all the environment variables we
     guarantee that users can use in config files, command-line
     options, etc.  Currently this includes:

        * `HOME' - user's home directory (Unix only)

        * `PLAT' - description of the current platform, including
          hardware and OS (see *note get_platform(): 2f0b.)

 -- Function: distutils.util.subst_vars (s, local_vars)
     Perform shell/Perl-style variable substitution on _s_.  Every
     occurrence of `$' followed by a name is considered a variable, and
     variable is substituted by the value found in the _local_vars_
     dictionary, or in `os.environ' if it's not in _local_vars_.
     _os.environ_ is first checked/augmented to guarantee that it
     contains certain values: see *note check_environ(): 2f0e.  Raise
     *note ValueError: 233.  for any variables not found in either
     _local_vars_ or `os.environ'.

     Note that this is not a fully-fledged string interpolation
     function. A valid `$variable' can consist only of upper and lower
     case letters, numbers and an underscore. No { } or ( ) style
     quoting is available.

 -- Function: distutils.util.grok_environment_error (exc[,
          prefix='error: '])
     Generate a useful error message from an *note EnvironmentError:
     939.  (*note IOError: 1f7.  or *note OSError: 22e.) exception
     object.   Handles Python 1.5.1 and later styles, and does what it
     can to deal with  exception objects that don't have a filename
     (which happens when the error  is due to a two-file operation,
     such as `rename()' or  `link()').  Returns the error message as a
     string prefixed  with _prefix_.

 -- Function: distutils.util.split_quoted (s)
     Split a string up according to Unix shell-like rules for quotes
     and backslashes.  In short: words are delimited by spaces, as long
     as those spaces are not escaped by a backslash, or inside a quoted
     string. Single and double quotes are equivalent, and the quote
     characters can be backslash-escaped.  The backslash is stripped
     from any two-character escape sequence, leaving only the escaped
     character.  The quote characters are stripped from any quoted
     string.  Returns a list of words.


 -- Function: distutils.util.execute (func, args[, msg=None, verbose=0,
          dry_run=0])
     Perform some action that affects the outside world (for instance,
     writing to the filesystem).  Such actions are special because they
     are disabled by the _dry_run_ flag.  This method takes  care of
     all that bureaucracy for you; all you have to do is supply the
     function to call and an argument tuple for it (to embody the
     "external action" being performed), and an optional message to
     print.

 -- Function: distutils.util.strtobool (val)
     Convert a string representation of truth to true (1) or false (0).

     True values are `y', `yes', `t', `true', `on'  and `1'; false
     values are `n', `no', `f', `false',  `off' and `0'.  Raises *note
     ValueError: 233. if _val_  is anything else.

 -- Function: distutils.util.byte_compile (py_files[, optimize=0,
          force=0, prefix=None, base_dir=None, verbose=1, dry_run=0,
          direct=None])
     Byte-compile a collection of Python source files to either `.pyc'
     or `.pyo' files in the same directory.  _py_files_ is a list of
     files to compile; any files that don't end in `.py' are silently
     skipped.  _optimize_ must be one of the following:

        * `0' - don't optimize (generate `.pyc')

        * `1' - normal optimization (like `python -O')

        * `2' - extra optimization (like `python -OO')

     If _force_ is true, all files are recompiled regardless of
     timestamps.

     The source filename encoded in each *note bytecode: 57a. file
     defaults to the filenames listed in _py_files_; you can modify
     these with _prefix_ and _basedir_.  _prefix_ is a string that will
     be stripped off of each source filename, and _base_dir_ is a
     directory name that will be prepended (after _prefix_ is
     stripped).  You can supply either or both (or neither) of _prefix_
     and _base_dir_, as you wish.

     If _dry_run_ is true, doesn't actually do anything that would
     affect the filesystem.

     Byte-compilation is either done directly in this interpreter
     process with the standard *note py_compile: 13d. module, or
     indirectly by writing a temporary script and executing it.
     Normally, you should let *note byte_compile(): 2f12. figure out to
     use direct compilation or not (see the source for details).  The
     _direct_ flag is used by the script generated in indirect mode;
     unless you know what you're doing, leave it set to `None'.

 -- Function: distutils.util.rfc822_escape (header)
     Return a version of _header_ escaped for inclusion in an RFC
     822(1) header, by ensuring there are 8 spaces space after each
     newline. Note that it does no other modification of the string.


  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc822.html


File: python.info,  Node: distutils dist --- The Distribution class,  Next: distutils extension --- The Extension class,  Prev: distutils util --- Miscellaneous other utility functions,  Up: API Reference

8.10.13 `distutils.dist' -- The Distribution class
--------------------------------------------------

This module provides the `Distribution' class, which represents the
module distribution being built/installed/distributed.


File: python.info,  Node: distutils extension --- The Extension class,  Next: distutils debug --- Distutils debug mode,  Prev: distutils dist --- The Distribution class,  Up: API Reference

8.10.14 `distutils.extension' -- The Extension class
----------------------------------------------------

This module provides the `Extension' class, used to describe C/C++
extension modules in setup scripts.


File: python.info,  Node: distutils debug --- Distutils debug mode,  Next: distutils errors --- Distutils exceptions,  Prev: distutils extension --- The Extension class,  Up: API Reference

8.10.15 `distutils.debug' -- Distutils debug mode
-------------------------------------------------

This module provides the DEBUG flag.


File: python.info,  Node: distutils errors --- Distutils exceptions,  Next: distutils fancy_getopt --- Wrapper around the standard getopt module,  Prev: distutils debug --- Distutils debug mode,  Up: API Reference

8.10.16 `distutils.errors' -- Distutils exceptions
--------------------------------------------------

Provides exceptions used by the Distutils modules.  Note that Distutils
modules may raise standard exceptions; in particular, SystemExit is
usually raised for errors that are obviously the end-user's fault (eg.
bad command-line arguments).

  This module is safe to use in `from ... import *' mode; it only
exports symbols whose names start with `Distutils' and end with `Error'.


File: python.info,  Node: distutils fancy_getopt --- Wrapper around the standard getopt module,  Next: distutils filelist --- The FileList class,  Prev: distutils errors --- Distutils exceptions,  Up: API Reference

8.10.17 `distutils.fancy_getopt' -- Wrapper around the standard getopt module
-----------------------------------------------------------------------------

This module provides a wrapper around the standard *note getopt: de.
module that provides the following additional features:

   * short and long options are tied together

   * options have help strings, so *note fancy_getopt(): 2f19. could
     potentially  create a complete usage summary

   * options set attributes of a passed-in object

   * boolean options can have "negative aliases" -- eg. if `--quiet' is
     the "negative alias" of `--verbose', then `--quiet' on the command
     line sets _verbose_ to false.

 -- Function: distutils.fancy_getopt.fancy_getopt (options,
          negative_opt, object, args)
     Wrapper function. _options_ is a list of `(long_option,
     short_option, help_string)' 3-tuples as described in the
     constructor for *note FancyGetopt: 2f1a. _negative_opt_ should be
     a dictionary mapping option names to option names, both the key
     and value should be in the _options_ list.  _object_ is an object
     which will be used to store values (see the *note getopt(): de.
     method of the *note FancyGetopt: 2f1a. class). _args_ is the
     argument list. Will use `sys.argv[1:]' if you  pass `None' as
     _args_.

 -- Function: distutils.fancy_getopt.wrap_text (text, width)
     Wraps _text_ to less than _width_ wide.

 -- Class: distutils.fancy_getopt.FancyGetopt ([option_table=None])
     The option_table is a list of 3-tuples: `(long_option,
     short_option, help_string)'

     If an option takes an argument, its _long_option_ should have
     `'='' appended; _short_option_ should just be a single character,
     no `':'' in any case.  _short_option_ should be `None' if a
     _long_option_  doesn't have a corresponding _short_option_. All
     option tuples must have long options.

  The *note FancyGetopt: 2f1a. class provides the following methods:

 -- Method: FancyGetopt.getopt ([args=None, object=None])
     Parse command-line options in args. Store as attributes on
     _object_.

     If _args_ is `None' or not supplied, uses `sys.argv[1:]'.  If
     _object_ is `None' or not supplied, creates a new `OptionDummy'
     instance, stores option values there, and returns a tuple `(args,
     object)'.  If _object_ is supplied, it is modified in place and
     *note getopt(): de. just returns _args_; in both cases, the
     returned _args_ is a modified copy of the passed-in _args_ list,
     which is left untouched.


 -- Method: FancyGetopt.get_option_order ()
     Returns the list of `(option, value)' tuples processed by the
     previous run of *note getopt(): de.  Raises *note RuntimeError:
     394. if *note getopt(): de. hasn't been called yet.

 -- Method: FancyGetopt.generate_help ([header=None])
     Generate help text (a list of strings, one per suggested line of
     output) from the option table for this *note FancyGetopt: 2f1a.
     object.

     If supplied, prints the supplied _header_ at the top of the help.


File: python.info,  Node: distutils filelist --- The FileList class,  Next: distutils log --- Simple PEP 282-style logging,  Prev: distutils fancy_getopt --- Wrapper around the standard getopt module,  Up: API Reference

8.10.18 `distutils.filelist' -- The FileList class
--------------------------------------------------

This module provides the `FileList' class, used for poking about the
filesystem and building lists of files.


File: python.info,  Node: distutils log --- Simple PEP 282-style logging,  Next: distutils spawn --- Spawn a sub-process,  Prev: distutils filelist --- The FileList class,  Up: API Reference

8.10.19 `distutils.log' -- Simple PEP 282-style logging
-------------------------------------------------------


File: python.info,  Node: distutils spawn --- Spawn a sub-process,  Next: distutils sysconfig --- System configuration information,  Prev: distutils log --- Simple PEP 282-style logging,  Up: API Reference

8.10.20 `distutils.spawn' -- Spawn a sub-process
------------------------------------------------

This module provides the `spawn()' function, a front-end to  various
platform-specific functions for launching another program in a
sub-process.  Also provides `find_executable()' to search the path for
a given executable name.


File: python.info,  Node: distutils sysconfig --- System configuration information,  Next: distutils text_file --- The TextFile class,  Prev: distutils spawn --- Spawn a sub-process,  Up: API Reference

8.10.21 `distutils.sysconfig' -- System configuration information
-----------------------------------------------------------------

The *note distutils.sysconfig: af. module provides access to Python's
low-level configuration information.  The specific configuration
variables available depend heavily on the platform and configuration.
The specific variables depend on the build process for the specific
version of Python being run; the variables are those found in the
`Makefile' and configuration header that are installed with Python on
Unix systems.  The configuration header is called `pyconfig.h' for
Python versions starting with 2.2, and `config.h' for earlier versions
of Python.

  Some additional functions are provided which perform some useful
manipulations for other parts of the *note distutils: 85. package.

 -- Data: distutils.sysconfig.PREFIX
     The result of `os.path.normpath(sys.prefix)'.

 -- Data: distutils.sysconfig.EXEC_PREFIX
     The result of `os.path.normpath(sys.exec_prefix)'.

 -- Function: distutils.sysconfig.get_config_var (name)
     Return the value of a single variable.  This is equivalent to
     `get_config_vars().get(name)'.

 -- Function: distutils.sysconfig.get_config_vars (...)
     Return a set of variable definitions.  If there are no arguments,
     this returns a dictionary mapping names of configuration variables
     to values.  If arguments are provided, they should be strings, and
     the return value will be a sequence giving the associated values.
     If a given name does not have a corresponding value, `None' will
     be included for that variable.

 -- Function: distutils.sysconfig.get_config_h_filename ()
     Return the full path name of the configuration header.  For Unix,
     this will be the header generated by the *configure* script; for
     other platforms the header will have been supplied directly by the
     Python source distribution.  The file is a platform-specific text
     file.

 -- Function: distutils.sysconfig.get_makefile_filename ()
     Return the full path name of the `Makefile' used to build Python.
     For Unix, this will be a file generated by the *configure* script;
     the meaning for other platforms will vary.  The file is a
     platform-specific text file, if it exists. This function is only
     useful on POSIX platforms.

 -- Function: distutils.sysconfig.get_python_inc ([plat_specific[,
          prefix]])
     Return the directory for either the general or platform-dependent
     C include files.  If _plat_specific_ is true, the
     platform-dependent include directory is returned; if false or
     omitted, the platform-independent directory is returned.  If
     _prefix_ is given, it is used as either the prefix instead of
     *note PREFIX: 2f23, or as the exec-prefix instead of *note
     EXEC_PREFIX: 2f24. if _plat_specific_ is true.

 -- Function: distutils.sysconfig.get_python_lib ([plat_specific[,
          standard_lib[, prefix]]])
     Return the directory for either the general or platform-dependent
     library installation.  If _plat_specific_ is true, the
     platform-dependent include directory is returned; if false or
     omitted, the platform-independent directory is returned.  If
     _prefix_ is given, it is used as either the prefix instead of
     *note PREFIX: 2f23, or as the exec-prefix instead of *note
     EXEC_PREFIX: 2f24. if _plat_specific_ is true.  If _standard_lib_
     is true, the directory for the standard library is returned rather
     than the directory for the installation of third-party extensions.

  The following function is only intended for use within the *note
distutils: 85.  package.

 -- Function: distutils.sysconfig.customize_compiler (compiler)
     Do any platform-specific customization of a *note
     distutils.ccompiler.CCompiler: 2ec7. instance.

     This function is only needed on Unix at this time, but should be
     called consistently to support forward-compatibility.  It inserts
     the information that varies across Unix flavors and is stored in
     Python's `Makefile'.  This information includes the selected
     compiler, compiler and linker options, and the extension used by
     the linker for shared objects.

  This function is even more special-purpose, and should only be used
from Python's own build procedures.

 -- Function: distutils.sysconfig.set_python_build ()
     Inform the *note distutils.sysconfig: af. module that it is being
     used as part of the build process for Python.  This changes a lot
     of relative locations for files, allowing them to be located in
     the build area rather than in an installed Python.


File: python.info,  Node: distutils text_file --- The TextFile class,  Next: distutils version --- Version number classes,  Prev: distutils sysconfig --- System configuration information,  Up: API Reference

8.10.22 `distutils.text_file' -- The TextFile class
---------------------------------------------------

This module provides the *note TextFile: 2f2e. class, which gives an
interface  to text files that (optionally) takes care of stripping
comments, ignoring  blank lines, and joining lines with backslashes.

 -- Class: distutils.text_file.TextFile ([filename=None, file=None,
          **options])
     This class provides a file-like object that takes care of all  the
     things you commonly want to do when processing a text file  that
     has some line-by-line syntax: strip comments (as long as `#'  is
     your comment character), skip blank lines, join adjacent lines by
     escaping the newline (ie. backslash at end of line), strip leading
     and/or trailing whitespace.  All of these are optional and
     independently controllable.

     The class provides a *note warn(): 2f2f. method so you can
     generate  warning messages that report physical line number, even
     if the  logical line in question spans multiple physical lines.
     Also  provides *note unreadline(): 2f30. for implementing
     line-at-a-time lookahead.

     *note TextFile: 2f2e. instances are create with either _filename_,
     _file_, or both.  *note RuntimeError: 394. is raised if both are
     `None'. _filename_ should be a string, and _file_ a file object
     (or something that provides *note readline(): 144.  and *note
     close(): 2f31.  methods).  It is recommended that you supply at
     least _filename_,  so that *note TextFile: 2f2e. can include it in
     warning messages.  If _file_ is not supplied, *note TextFile:
     2f2e. creates its own using the *note open(): 2d3. built-in
     function.

     The options are all boolean, and affect the values returned by
     *note readline(): 144.

     option name            description                          default
     -------------------------------------------------------------------------- 
     _strip_comments_       strip from `'#'' to end-of- line,    true
                            as well as any whitespace leading    
                            up to the `'#''--unless it is        
                            escaped by a backslash               
     _lstrip_ws_            strip leading whitespace from each   false
                            line before returning it             
     _rstrip_ws_            strip trailing whitespace            true
                            (including line terminator!)  from   
                            each line before returning it.       
     _skip_blanks_          skip lines that are empty *after*    true
                            stripping comments and whitespace.   
                            (If both lstrip_ws and rstrip_ws are 
                            false, then some lines may consist   
                            of solely whitespace: these will     
                            *not* be skipped, even if            
                            _skip_blanks_ is true.)              
     _join_lines_           if a backslash is the last           false
                            non-newline character on a line      
                            after stripping comments and         
                            whitespace, join the following line  
                            to it to form one logical line; if N 
                            consecutive lines end with a         
                            backslash, then N+1 physical lines   
                            will be joined to form one logical   
                            line.                                
     _collapse_join_        strip leading whitespace from lines  false
                            that are joined to their             
                            predecessor; only matters if         
                            `(join_lines and not lstrip_ws)'     

     Note that since _rstrip_ws_ can strip the trailing newline, the
     semantics of *note readline(): 144. must differ from those of the
     built-in file object's *note readline(): 144. method!  In
     particular, *note readline(): 144.  returns `None' for
     end-of-file: an empty string might just be a  blank line (or an
     all-whitespace line), if _rstrip_ws_ is true  but _skip_blanks_ is
     not.

      -- Method: open (filename)
          Open a new file _filename_.  This overrides any _file_ or
          _filename_ constructor arguments.

      -- Method: close ()
          Close the current file and forget everything we know about it
          (including the filename and the current line number).

      -- Method: warn (msg[, line=None])
          Print (to stderr) a warning message tied to the current
          logical line in the current file.  If the current logical
          line in the file spans multiple physical lines, the warning
          refers to the whole range, such as `"lines 3-5"'.  If _line_
          is supplied,  it overrides the current line number; it may be
          a list or tuple  to indicate a range of physical lines, or an
          integer for a  single physical line.

      -- Method: readline ()
          Read and return a single logical line from the current file
          (or from an internal buffer if lines have previously been
          "unread" with *note unreadline(): 2f30.).  If the
          _join_lines_ option  is true, this may involve reading
          multiple physical lines concatenated into a single string.
          Updates the current line number,  so calling *note warn():
          2f2f. after *note readline(): 144. emits a warning  about the
          physical line(s) just read.  Returns `None' on end-of-file,
          since the empty string can occur if _rstrip_ws_ is true but
          _strip_blanks_ is not.

      -- Method: readlines ()
          Read and return the list of all logical lines remaining in
          the current file.  This updates the current line number to
          the last line of the file.

      -- Method: unreadline (line)
          Push _line_ (a string) onto an internal buffer that will be
          checked by future *note readline(): 144. calls.  Handy for
          implementing a parser with line-at-a-time lookahead. Note
          that lines that are "unread" with *note unreadline(): 2f30.
          are not subsequently re-cleansed (whitespace  stripped, or
          whatever) when read with *note readline(): 144. If multiple
          calls are made to *note unreadline(): 2f30. before a call to
          *note readline(): 144, the lines will be returned most in
          most recent first order.


File: python.info,  Node: distutils version --- Version number classes,  Next: distutils cmd --- Abstract base class for Distutils commands,  Prev: distutils text_file --- The TextFile class,  Up: API Reference

8.10.23 `distutils.version' -- Version number classes
-----------------------------------------------------


File: python.info,  Node: distutils cmd --- Abstract base class for Distutils commands,  Next: Creating a new Distutils command,  Prev: distutils version --- Version number classes,  Up: API Reference

8.10.24 `distutils.cmd' -- Abstract base class for Distutils commands
---------------------------------------------------------------------

This module supplies the abstract base class *note Command: 2eb2.

 -- Class: distutils.cmd.Command (dist)
     Abstract base class for defining command classes, the "worker
     bees" of the Distutils.  A useful analogy for command classes is
     to think of them as subroutines with local variables called
     _options_.  The options are declared in *note
     initialize_options(): 2f37. and defined (given their final values)
     in *note finalize_options(): 2f38, both of which must be defined
     by every command class.  The distinction between the two is
     necessary because option values might come from the outside world
     (command line, config file, ...), and any options dependent on
     other options must be computed after these outside influences have
     been processed -- hence *note finalize_options(): 2f38.  The body
     of the subroutine, where it does all its work based on the values
     of its options, is the *note run(): 2f39. method, which must also
     be implemented by every command class.

     The class constructor takes a single argument _dist_, a
     `Distribution' instance.


File: python.info,  Node: Creating a new Distutils command,  Next: distutils command --- Individual Distutils commands,  Prev: distutils cmd --- Abstract base class for Distutils commands,  Up: API Reference

8.10.25 Creating a new Distutils command
----------------------------------------

This section outlines the steps to create a new Distutils command.

  A new command lives in a module in the *note distutils.command: 8a.
package. There is a sample template in that directory called
`command_template'.  Copy this file to a new module with the same name
as the new command you're implementing.  This module should implement a
class with the same name as the module (and the command).  So, for
instance, to create the command `peel_banana' (so that users can run
`setup.py peel_banana'), you'd copy `command_template' to
`distutils/command/peel_banana.py', then edit it so that it's
implementing the class `peel_banana', a subclass of *note
distutils.cmd.Command: 2eb2.

  Subclasses of *note Command: 2eb2. must define the following methods.

 -- Method: Command.initialize_options ()
     Set default values for all the options that this command supports.
     Note that these defaults may be overridden by other commands, by
     the setup script, by config files, or by the command-line.  Thus,
     this is not the place to code dependencies between options;
     generally, *note initialize_options(): 2f37.  implementations are
     just a bunch of `self.foo = None' assignments.

 -- Method: Command.finalize_options ()
     Set final values for all the options that this command supports.
     This is always called as late as possible, ie.  after any option
     assignments from the command-line or from other commands have been
     done.  Thus, this is the place to code option dependencies: if
     _foo_ depends on _bar_, then it is safe to set _foo_ from _bar_ as
     long as _foo_ still has the same value it was assigned in *note
     initialize_options(): 2f37.

 -- Method: Command.run ()
     A command's raison d'etre: carry out the action it exists to
     perform, controlled by the options initialized in *note
     initialize_options(): 2f37, customized by other commands, the
     setup script, the command-line, and config files, and finalized in
     *note finalize_options(): 2f38.  All terminal output and
     filesystem interaction should be done by *note run(): 2f39.

 -- Attribute: Command.sub_commands
     _sub_commands_ formalizes the notion of a "family" of commands,
     e.g. `install' as the parent with sub-commands `install_lib',
     `install_headers', etc.  The parent of a family of commands defines
     _sub_commands_ as a class attribute; it's a list of 2-tuples
     `(command_name, predicate)', with _command_name_ a string and
     _predicate_ a function, a string or `None'.  _predicate_ is a
     method of the parent command that determines whether the
     corresponding command is applicable in the current situation.
     (E.g. `install_headers' is only applicable if we have any C header
     files to install.)  If _predicate_ is `None', that command is
     always applicable.

     _sub_commands_ is usually defined at the _end_ of a class, because
     predicates can be methods of the class, so they must already have
     been defined.  The canonical example is the *install* command.


File: python.info,  Node: distutils command --- Individual Distutils commands,  Next: distutils command bdist --- Build a binary installer,  Prev: Creating a new Distutils command,  Up: API Reference

8.10.26 `distutils.command' -- Individual Distutils commands
------------------------------------------------------------


File: python.info,  Node: distutils command bdist --- Build a binary installer,  Next: distutils command bdist_packager --- Abstract base class for packagers,  Prev: distutils command --- Individual Distutils commands,  Up: API Reference

8.10.27 `distutils.command.bdist' -- Build a binary installer
-------------------------------------------------------------


File: python.info,  Node: distutils command bdist_packager --- Abstract base class for packagers,  Next: distutils command bdist_dumb --- Build a "dumb" installer,  Prev: distutils command bdist --- Build a binary installer,  Up: API Reference

8.10.28 `distutils.command.bdist_packager' -- Abstract base class for packagers
-------------------------------------------------------------------------------


File: python.info,  Node: distutils command bdist_dumb --- Build a "dumb" installer,  Next: distutils command bdist_msi --- Build a Microsoft Installer binary package,  Prev: distutils command bdist_packager --- Abstract base class for packagers,  Up: API Reference

8.10.29 `distutils.command.bdist_dumb' -- Build a "dumb" installer
------------------------------------------------------------------


File: python.info,  Node: distutils command bdist_msi --- Build a Microsoft Installer binary package,  Next: distutils command bdist_rpm --- Build a binary distribution as a Redhat RPM and SRPM,  Prev: distutils command bdist_dumb --- Build a "dumb" installer,  Up: API Reference

8.10.30 `distutils.command.bdist_msi' -- Build a Microsoft Installer binary package
-----------------------------------------------------------------------------------

 -- Class: distutils.command.bdist_msi.bdist_msi
     Builds a Windows Installer(1) (.msi) binary package.

     In most cases, the `bdist_msi' installer is a better choice than
     the `bdist_wininst' installer, because it provides better support
     for Win64 platforms, allows administrators to perform
     non-interactive installations, and allows installation through
     group policies.

  ---------- Footnotes ----------

  (1) http://msdn.microsoft.com/en-us/library/cc185688(VS.85).aspx


File: python.info,  Node: distutils command bdist_rpm --- Build a binary distribution as a Redhat RPM and SRPM,  Next: distutils command bdist_wininst --- Build a Windows installer,  Prev: distutils command bdist_msi --- Build a Microsoft Installer binary package,  Up: API Reference

8.10.31 `distutils.command.bdist_rpm' -- Build a binary distribution as a Redhat RPM and SRPM
---------------------------------------------------------------------------------------------


File: python.info,  Node: distutils command bdist_wininst --- Build a Windows installer,  Next: distutils command sdist --- Build a source distribution,  Prev: distutils command bdist_rpm --- Build a binary distribution as a Redhat RPM and SRPM,  Up: API Reference

8.10.32 `distutils.command.bdist_wininst' -- Build a Windows installer
----------------------------------------------------------------------


File: python.info,  Node: distutils command sdist --- Build a source distribution,  Next: distutils command build --- Build all files of a package,  Prev: distutils command bdist_wininst --- Build a Windows installer,  Up: API Reference

8.10.33 `distutils.command.sdist' -- Build a source distribution
----------------------------------------------------------------


File: python.info,  Node: distutils command build --- Build all files of a package,  Next: distutils command build_clib --- Build any C libraries in a package,  Prev: distutils command sdist --- Build a source distribution,  Up: API Reference

8.10.34 `distutils.command.build' -- Build all files of a package
-----------------------------------------------------------------


File: python.info,  Node: distutils command build_clib --- Build any C libraries in a package,  Next: distutils command build_ext --- Build any extensions in a package,  Prev: distutils command build --- Build all files of a package,  Up: API Reference

8.10.35 `distutils.command.build_clib' -- Build any C libraries in a package
----------------------------------------------------------------------------


File: python.info,  Node: distutils command build_ext --- Build any extensions in a package,  Next: distutils command build_py --- Build the py/ pyc files of a package,  Prev: distutils command build_clib --- Build any C libraries in a package,  Up: API Reference

8.10.36 `distutils.command.build_ext' -- Build any extensions in a package
--------------------------------------------------------------------------


File: python.info,  Node: distutils command build_py --- Build the py/ pyc files of a package,  Next: distutils command build_scripts --- Build the scripts of a package,  Prev: distutils command build_ext --- Build any extensions in a package,  Up: API Reference

8.10.37 `distutils.command.build_py' -- Build the .py/.pyc files of a package
-----------------------------------------------------------------------------


File: python.info,  Node: distutils command build_scripts --- Build the scripts of a package,  Next: distutils command clean --- Clean a package build area,  Prev: distutils command build_py --- Build the py/ pyc files of a package,  Up: API Reference

8.10.38 `distutils.command.build_scripts' -- Build the scripts of a package
---------------------------------------------------------------------------


File: python.info,  Node: distutils command clean --- Clean a package build area,  Next: distutils command config --- Perform package configuration,  Prev: distutils command build_scripts --- Build the scripts of a package,  Up: API Reference

8.10.39 `distutils.command.clean' -- Clean a package build area
---------------------------------------------------------------


File: python.info,  Node: distutils command config --- Perform package configuration,  Next: distutils command install --- Install a package,  Prev: distutils command clean --- Clean a package build area,  Up: API Reference

8.10.40 `distutils.command.config' -- Perform package configuration
-------------------------------------------------------------------


File: python.info,  Node: distutils command install --- Install a package,  Next: distutils command install_data --- Install data files from a package,  Prev: distutils command config --- Perform package configuration,  Up: API Reference

8.10.41 `distutils.command.install' -- Install a package
--------------------------------------------------------


File: python.info,  Node: distutils command install_data --- Install data files from a package,  Next: distutils command install_headers --- Install C/C++ header files from a package,  Prev: distutils command install --- Install a package,  Up: API Reference

8.10.42 `distutils.command.install_data' -- Install data files from a package
-----------------------------------------------------------------------------


File: python.info,  Node: distutils command install_headers --- Install C/C++ header files from a package,  Next: distutils command install_lib --- Install library files from a package,  Prev: distutils command install_data --- Install data files from a package,  Up: API Reference

8.10.43 `distutils.command.install_headers' -- Install C/C++ header files from a package
----------------------------------------------------------------------------------------


File: python.info,  Node: distutils command install_lib --- Install library files from a package,  Next: distutils command install_scripts --- Install script files from a package,  Prev: distutils command install_headers --- Install C/C++ header files from a package,  Up: API Reference

8.10.44 `distutils.command.install_lib' -- Install library files from a package
-------------------------------------------------------------------------------


File: python.info,  Node: distutils command install_scripts --- Install script files from a package,  Next: distutils command register --- Register a module with the Python Package Index,  Prev: distutils command install_lib --- Install library files from a package,  Up: API Reference

8.10.45 `distutils.command.install_scripts' -- Install script files from a package
----------------------------------------------------------------------------------


File: python.info,  Node: distutils command register --- Register a module with the Python Package Index,  Next: distutils command check --- Check the meta-data of a package,  Prev: distutils command install_scripts --- Install script files from a package,  Up: API Reference

8.10.46 `distutils.command.register' -- Register a module with the Python Package Index
---------------------------------------------------------------------------------------

The `register' command registers the package with the Python Package
Index.  This is described in more detail in PEP 301(1).

  ---------- Footnotes ----------

  (1) http://www.python.org/dev/peps/pep-0301


File: python.info,  Node: distutils command check --- Check the meta-data of a package,  Prev: distutils command register --- Register a module with the Python Package Index,  Up: API Reference

8.10.47 `distutils.command.check' -- Check the meta-data of a package
---------------------------------------------------------------------

The `check' command performs some tests on the meta-data of a package.
For example, it verifies that all required meta-data are provided as
the arguments passed to the `setup()' function.


File: python.info,  Node: Installing Python Modules,  Next: Python HOWTOs,  Prev: Distributing Python Modules,  Up: Top

9 Installing Python Modules
***************************

     Author: Greg Ward

Abstract
--------

This document describes the Python Distribution Utilities ("Distutils")
from the end-user's point-of-view, describing how to extend the
capabilities of a standard Python installation by building and
installing third-party Python modules and extensions.

* Menu:

* Introduction: Introduction<11>.
* Standard Build and Install::
* Alternate Installation::
* Custom Installation::
* Distutils Configuration Files::
* Building Extensions; Tips and Tricks: Building Extensions Tips and Tricks.

Introduction

* Best case; trivial installation: Best case trivial installation.
* The new standard; Distutils: The new standard Distutils.

Standard Build and Install

* Platform variations::
* Splitting the job up::
* How building works::
* How installation works::

Alternate Installation

* Alternate installation; the user scheme: Alternate installation the user scheme.
* Alternate installation; the home scheme: Alternate installation the home scheme.
* Alternate installation; Unix (the prefix scheme): Alternate installation Unix the prefix scheme.
* Alternate installation; Windows (the prefix scheme): Alternate installation Windows the prefix scheme.

Custom Installation

* Modifying Python's Search Path::

Distutils Configuration Files

* Location and names of config files::
* Syntax of config files::

Building Extensions: Tips and Tricks

* Tweaking compiler/linker flags::
* Using non-Microsoft compilers on Windows::

Using non-Microsoft compilers on Windows

* Borland/CodeGear C++::
* GNU C / Cygwin / MinGW::

GNU C / Cygwin / MinGW

* Older Versions of Python and MinGW::


File: python.info,  Node: Introduction<11>,  Next: Standard Build and Install,  Up: Installing Python Modules

9.1 Introduction
================

Although Python's extensive standard library covers many programming
needs, there often comes a time when you need to add some new
functionality to your Python installation in the form of third-party
modules.  This might be necessary to support your own programming, or
to support an application that you want to use and that happens to be
written in Python.

  In the past, there has been little support for adding third-party
modules to an existing Python installation.  With the introduction of
the Python Distribution Utilities (Distutils for short) in Python 2.0,
this changed.

  This document is aimed primarily at the people who need to install
third-party Python modules: end-users and system administrators who
just need to get some Python application running, and existing Python
programmers who want to add some new goodies to their toolbox.  You
don't need to know Python to read this document; there will be some
brief forays into using Python's interactive mode to explore your
installation, but that's it.  If you're looking for information on how
to distribute your own Python modules so that others may use them, see
the *note Distributing Python Modules: 245a. manual.

* Menu:

* Best case; trivial installation: Best case trivial installation.
* The new standard; Distutils: The new standard Distutils.


File: python.info,  Node: Best case trivial installation,  Next: The new standard Distutils,  Up: Introduction<11>

9.1.1 Best case: trivial installation
-------------------------------------

In the best case, someone will have prepared a special version of the
module distribution you want to install that is targeted specifically
at your platform and is installed just like any other software on your
platform.  For example, the module developer might make an executable
installer available for Windows users, an RPM package for users of
RPM-based Linux systems (Red Hat, SuSE, Mandrake, and many others), a
Debian package for users of Debian-based Linux systems, and so forth.

  In that case, you would download the installer appropriate to your
platform and do the obvious thing with it: run it if it's an executable
installer, `rpm --install' it if it's an RPM, etc.  You don't need to
run Python or a setup script, you don't need to compile anything--you
might not even need to read any instructions (although it's always a
good idea to do so anyway).

  Of course, things will not always be that easy.  You might be
interested in a module distribution that doesn't have an easy-to-use
installer for your platform.  In that case, you'll have to start with
the source distribution released by the module's author/maintainer.
Installing from a source distribution is not too hard, as long as the
modules are packaged in the standard way.  The bulk of this document is
about building and installing modules from standard source
distributions.


File: python.info,  Node: The new standard Distutils,  Prev: Best case trivial installation,  Up: Introduction<11>

9.1.2 The new standard: Distutils
---------------------------------

If you download a module source distribution, you can tell pretty
quickly if it was packaged and distributed in the standard way, i.e.
using the Distutils.  First, the distribution's name and version number
will be featured prominently in the name of the downloaded archive,
e.g. `foo-1.0.tar.gz' or `widget-0.9.7.zip'.  Next, the archive will
unpack into a similarly-named directory: `foo-1.0' or `widget-0.9.7'.
Additionally, the distribution will contain a setup script `setup.py',
and a file named `README.txt' or possibly just `README', which should
explain that building and installing the module distribution is a
simple matter of running one command from a terminal:

    python setup.py install

For Windows, this command should be run from a command prompt window
(_Start ‣ Accessories_):

    setup.py install

If all these things are true, then you already know how to build and
install the modules you've just downloaded:  Run the command above.
Unless you need to install things in a non-standard way or customize
the build process, you don't really need this manual.  Or rather, the
above command is everything you need to get out of this manual.


File: python.info,  Node: Standard Build and Install,  Next: Alternate Installation,  Prev: Introduction<11>,  Up: Installing Python Modules

9.2 Standard Build and Install
==============================

As described in section *note The new standard; Distutils: 2f5a,
building and installing a module distribution using the Distutils is
usually one simple command to run from a terminal:

    python setup.py install


* Menu:

* Platform variations::
* Splitting the job up::
* How building works::
* How installation works::


File: python.info,  Node: Platform variations,  Next: Splitting the job up,  Up: Standard Build and Install

9.2.1 Platform variations
-------------------------

You should always run the setup command from the distribution root
directory, i.e. the top-level subdirectory that the module source
distribution unpacks into.  For example, if you've just downloaded a
module source distribution `foo-1.0.tar.gz' onto a Unix system, the
normal thing to do is:

    gunzip -c foo-1.0.tar.gz | tar xf -    # unpacks into directory foo-1.0
    cd foo-1.0
    python setup.py install

On Windows, you'd probably download `foo-1.0.zip'.  If you downloaded
the archive file to `C:\Temp', then it would unpack into
`C:\Temp\foo-1.0'; you can use either a archive manipulator with a
graphical user interface (such as WinZip) or a command-line tool (such
as *unzip* or *pkunzip*) to unpack the archive.  Then, open a command
prompt window and run:

    cd c:\Temp\foo-1.0
    python setup.py install



File: python.info,  Node: Splitting the job up,  Next: How building works,  Prev: Platform variations,  Up: Standard Build and Install

9.2.2 Splitting the job up
--------------------------

Running `setup.py install' builds and installs all modules in one run.
If you prefer to work incrementally--especially useful if you want to
customize the build process, or if things are going wrong--you can use
the setup script to do one thing at a time.  This is particularly
helpful when the build and install will be done by different users--for
example, you might want to build a module distribution and hand it off
to a system administrator for installation (or do it yourself, with
super-user privileges).

  For example, you can build everything in one step, and then install
everything in a second step, by invoking the setup script twice:

    python setup.py build
    python setup.py install

If you do this, you will notice that running the *install* command
first runs the *build* command, which--in this case--quickly notices
that it has nothing to do, since everything in the `build' directory is
up-to-date.

  You may not need this ability to break things down often if all you
do is install modules downloaded off the 'net, but it's very handy for
more advanced tasks.  If you get into distributing your own Python
modules and extensions, you'll run lots of individual Distutils
commands on their own.


File: python.info,  Node: How building works,  Next: How installation works,  Prev: Splitting the job up,  Up: Standard Build and Install

9.2.3 How building works
------------------------

As implied above, the *build* command is responsible for putting the
files to install into a _build directory_.  By default, this is `build'
under the distribution root; if you're excessively concerned with
speed, or want to keep the source tree pristine, you can change the
build directory with the `--build-base' option. For example:

    python setup.py build --build-base=/path/to/pybuild/foo-1.0

(Or you could do this permanently with a directive in your system or
personal Distutils configuration file; see section *note Distutils
Configuration Files: 2f63.)  Normally, this isn't necessary.

  The default layout for the build tree is as follows:

    --- build/ --- lib/
    or
    --- build/ --- lib.<plat>/
                   temp.<plat>/

where `<plat>' expands to a brief description of the current OS/hardware
platform and Python version.  The first form, with just a `lib'
directory, is used for "pure module distributions"--that is, module
distributions that include only pure Python modules.  If a module
distribution contains any extensions (modules written in C/C++), then
the second form, with two `<plat>' directories, is used.  In that case,
the `temp._plat_' directory holds temporary files generated by the
compile/link process that don't actually get installed.  In either
case, the `lib' (or `lib._plat_') directory contains all Python modules
(pure Python and extensions) that will be installed.

  In the future, more directories will be added to handle Python
scripts, documentation, binary executables, and whatever else is needed
to handle the job of installing Python modules and applications.


File: python.info,  Node: How installation works,  Prev: How building works,  Up: Standard Build and Install

9.2.4 How installation works
----------------------------

After the *build* command runs (whether you run it explicitly, or the
*install* command does it for you), the work of the *install* command
is relatively simple: all it has to do is copy everything under
`build/lib' (or `build/lib._plat_') to your chosen installation
directory.

  If you don't choose an installation directory--i.e., if you just run
`setup.py install'--then the *install* command installs to the standard
location for third-party Python modules.  This location varies by
platform and by how you built/installed Python itself.  On Unix (and
Mac OS X, which is also Unix-based), it also depends on whether the
module distribution being installed is pure Python or contains
extensions ("non-pure"):

Platform              Standard installation location                            Default value                                          Notes
--------------------------------------------------------------------------------------------------------------------------------------------------- 
Unix (pure)           `_prefix_/lib/python_X.Y_/site-packages'                  `/usr/local/lib/python_X.Y_/site-packages'             (1)
Unix (non-pure)       `_exec-prefix_/lib/python_X.Y_/site-packages'             `/usr/local/lib/python_X.Y_/site-packages'             (1)
Windows               `_prefix_\Lib\site-packages'                              `C:\Python_XY_\Lib\site-packages'                      (2)

  Notes:

  1. Most Linux distributions include Python as a standard part of the
     system, so `_prefix_' and `_exec-prefix_' are usually both `/usr'
     on Linux.  If you build Python yourself on Linux (or any Unix-like
     system), the default `_prefix_' and `_exec-prefix_' are
     `/usr/local'.

  2. The default installation directory on Windows was `C:\Program
     Files\Python' under Python 1.6a1, 1.5.2, and earlier.

  `_prefix_' and `_exec-prefix_' stand for the directories that Python
is installed to, and where it finds its libraries at run-time.  They
are always the same under Windows, and very often the same under Unix
and Mac OS X.  You can find out what your Python installation uses for
`_prefix_' and `_exec-prefix_' by running Python in interactive mode
and typing a few simple commands. Under Unix, just type `python' at the
shell prompt.  Under Windows, choose _Start ‣ Programs ‣ Python X.Y ‣
Python (command line)_.   Once the interpreter is started, you type
Python code at the prompt.  For example, on my Linux system, I type the
three Python statements shown below, and get the output as shown, to
find out my `_prefix_' and `_exec-prefix_':

    Python 2.4 (#26, Aug  7 2004, 17:19:02)
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import sys
    >>> sys.prefix
    '/usr'
    >>> sys.exec_prefix
    '/usr'

A few other placeholders are used in this document: `_X.Y_' stands for
the version of Python, for example `2.7'; `_distname_' will be replaced
by the name of the module distribution being installed.  Dots and
capitalization are important in the paths; for example, a value that
uses `python2.7' on UNIX will typically use `Python27' on Windows.

  If you don't want to install modules to the standard location, or if
you don't have permission to write there, then you need to read about
alternate installations in section *note Alternate Installation: 2f66.
If you want to customize your installation directories more heavily,
see section *note Custom Installation: 2f67. on custom installations.


File: python.info,  Node: Alternate Installation,  Next: Custom Installation,  Prev: Standard Build and Install,  Up: Installing Python Modules

9.3 Alternate Installation
==========================

Often, it is necessary or desirable to install modules to a location
other than the standard location for third-party Python modules.  For
example, on a Unix system you might not have permission to write to the
standard third-party module directory.  Or you might wish to try out a
module before making it a standard part of your local Python
installation.  This is especially true when upgrading a distribution
already present: you want to make sure your existing base of scripts
still works with the new version before actually upgrading.

  The Distutils *install* command is designed to make installing module
distributions to an alternate location simple and painless.  The basic
idea is that you supply a base directory for the installation, and the
*install* command picks a set of directories (called an _installation
scheme_) under this base directory in which to install files.  The
details differ across platforms, so read whichever of the following
sections applies to you.

  Note that the various alternate installation schemes are mutually
exclusive: you can pass `--user', or `--home', or `--prefix' and
`--exec-prefix', or `--install-base' and `--install-platbase', but you
can't mix from these groups.

* Menu:

* Alternate installation; the user scheme: Alternate installation the user scheme.
* Alternate installation; the home scheme: Alternate installation the home scheme.
* Alternate installation; Unix (the prefix scheme): Alternate installation Unix the prefix scheme.
* Alternate installation; Windows (the prefix scheme): Alternate installation Windows the prefix scheme.


File: python.info,  Node: Alternate installation the user scheme,  Next: Alternate installation the home scheme,  Up: Alternate Installation

9.3.1 Alternate installation: the user scheme
---------------------------------------------

This scheme is designed to be the most convenient solution for users
that don't have write permission to the global site-packages directory
or don't want to install into it.  It is enabled with a simple option:

    python setup.py install --user

Files will be installed into subdirectories of *note site.USER_BASE:
644. (written as `_userbase_' hereafter).  This scheme installs pure
Python modules and extension modules in the same location (also known
as *note site.USER_SITE: 62d.).  Here are the values for UNIX,
including Mac OS X:

Type of file        Installation directory
------------------------------------------------------------------------------------ 
modules             `_userbase_/lib/python_X.Y_/site-packages'
scripts             `_userbase_/bin'
data                `_userbase_'
C headers           `_userbase_/include/python_X.Y_/_distname_'

  And here are the values used on Windows:

Type of file        Installation directory
------------------------------------------------------------------------------------ 
modules             `_userbase_\Python_XY_\site-packages'
scripts             `_userbase_\Scripts'
data                `_userbase_'
C headers           `_userbase_\Python_XY_\Include\_distname_'

  The advantage of using this scheme compared to the other ones
described below is that the user site-packages directory is under
normal conditions always included in *note sys.path: 576. (see *note
site: 158. for more information), which means that there is no
additional step to perform after running the `setup.py' script to
finalize the installation.

  The *build_ext* command also has a `--user' option to add
`_userbase_/include' to the compiler search path for header files and
`_userbase_/lib' to the compiler search path for libraries as well as to
the runtime search path for shared C libraries (rpath).


File: python.info,  Node: Alternate installation the home scheme,  Next: Alternate installation Unix the prefix scheme,  Prev: Alternate installation the user scheme,  Up: Alternate Installation

9.3.2 Alternate installation: the home scheme
---------------------------------------------

The idea behind the "home scheme" is that you build and maintain a
personal stash of Python modules.  This scheme's name is derived from
the idea of a "home" directory on Unix, since it's not unusual for a
Unix user to make their home directory have a layout similar to `/usr/'
or `/usr/local/'.  This scheme can be used by anyone, regardless of the
operating system they are installing for.

  Installing a new module distribution is as simple as

    python setup.py install --home=<dir>

where you can supply any directory you like for the `--home' option.  On
Unix, lazy typists can just type a tilde (`~'); the *install* command
will expand this to your home directory:

    python setup.py install --home=~

To make Python find the distributions installed with this scheme, you
may have to *note modify Python's search path: 2f6c. or edit
`sitecustomize' (see *note site: 158.) to call *note site.addsitedir():
244a. or edit *note sys.path: 576.

  The `--home' option defines the installation base directory.  Files
are installed to the following directories under the installation base
as follows:

Type of file        Installation directory
------------------------------------------------------------------------------------ 
modules             `_home_/lib/python'
scripts             `_home_/bin'
data                `_home_'
C headers           `_home_/include/python/_distname_'

  (Mentally replace slashes with backslashes if you're on Windows.)

  Changed in version 2.4: The `--home' option used to be supported only
on Unix.


File: python.info,  Node: Alternate installation Unix the prefix scheme,  Next: Alternate installation Windows the prefix scheme,  Prev: Alternate installation the home scheme,  Up: Alternate Installation

9.3.3 Alternate installation: Unix (the prefix scheme)
------------------------------------------------------

The "prefix scheme" is useful when you wish to use one Python
installation to perform the build/install (i.e., to run the setup
script), but install modules into the third-party module directory of a
different Python installation (or something that looks like a different
Python installation).  If this sounds a trifle unusual, it is--that's
why the user and home schemes come before.  However, there are at least
two known cases where the prefix scheme will be useful.

  First, consider that many Linux distributions put Python in `/usr',
rather than the more traditional `/usr/local'.  This is entirely
appropriate, since in those cases Python is part of "the system" rather
than a local add-on.  However, if you are installing Python modules
from source, you probably want them to go in
`/usr/local/lib/python2._X_' rather than `/usr/lib/python2._X_'.  This
can be done with

    /usr/bin/python setup.py install --prefix=/usr/local

Another possibility is a network filesystem where the name used to
write to a remote directory is different from the name used to read it:
for example, the Python interpreter accessed as `/usr/local/bin/python'
might search for modules in `/usr/local/lib/python2._X_', but those
modules would have to be installed to, say,
`/mnt/_@server_/export/lib/python2._X_'.  This could be done with

    /usr/local/bin/python setup.py install --prefix=/mnt/@server/export

In either case, the `--prefix' option defines the installation base, and
the `--exec-prefix' option defines the platform-specific installation
base, which is used for platform-specific files.  (Currently, this just
means non-pure module distributions, but could be expanded to C
libraries, binary executables, etc.)  If `--exec-prefix' is not
supplied, it defaults to `--prefix'.  Files are installed as follows:

Type of file          Installation directory
------------------------------------------------------------------------------------- 
Python modules        `_prefix_/lib/python_X.Y_/site-packages'
extension modules     `_exec-prefix_/lib/python_X.Y_/site-packages'
scripts               `_prefix_/bin'
data                  `_prefix_'
C headers             `_prefix_/include/python_X.Y_/_distname_'

  There is no requirement that `--prefix' or `--exec-prefix' actually
point to an alternate Python installation; if the directories listed
above do not already exist, they are created at installation time.

  Incidentally, the real reason the prefix scheme is important is
simply that a standard Unix installation uses the prefix scheme, but
with `--prefix' and `--exec-prefix' supplied by Python itself as
`sys.prefix' and `sys.exec_prefix'.  Thus, you might think you'll never
use the prefix scheme, but every time you run `python setup.py install'
without any other options, you're using it.

  Note that installing extensions to an alternate Python installation
has no effect on how those extensions are built: in particular, the
Python header files (`Python.h' and friends) installed with the Python
interpreter used to run the setup script will be used in compiling
extensions.  It is your responsibility to ensure that the interpreter
used to run extensions installed in this way is compatible with the
interpreter used to build them.  The best way to do this is to ensure
that the two interpreters are the same version of Python (possibly
different builds, or possibly copies of the same build).  (Of course,
if your `--prefix' and `--exec-prefix' don't even point to an alternate
Python installation, this is immaterial.)


File: python.info,  Node: Alternate installation Windows the prefix scheme,  Prev: Alternate installation Unix the prefix scheme,  Up: Alternate Installation

9.3.4 Alternate installation: Windows (the prefix scheme)
---------------------------------------------------------

Windows has no concept of a user's home directory, and since the
standard Python installation under Windows is simpler than under Unix,
the `--prefix' option has traditionally been used to install additional
packages in separate locations on Windows.

    python setup.py install --prefix="\Temp\Python"

to install modules to the `\Temp\Python' directory on the current drive.

  The installation base is defined by the `--prefix' option; the
`--exec-prefix' option is not supported under Windows, which means that
pure Python modules and extension modules are installed into the same
location.  Files are installed as follows:

Type of file        Installation directory
----------------------------------------------------------------------------------- 
modules             `_prefix_\Lib\site-packages'
scripts             `_prefix_\Scripts'
data                `_prefix_'
C headers           `_prefix_\Include\_distname_'


File: python.info,  Node: Custom Installation,  Next: Distutils Configuration Files,  Prev: Alternate Installation,  Up: Installing Python Modules

9.4 Custom Installation
=======================

Sometimes, the alternate installation schemes described in section
*note Alternate Installation: 2f66. just don't do what you want.  You
might want to tweak just one or two directories while keeping
everything under the same base directory, or you might want to
completely redefine the installation scheme.  In either case, you're
creating a _custom installation scheme_.

  To create a custom installation scheme, you start with one of the
alternate schemes and override some of the installation directories
used for the various types of files, using these options:

Type of file               Override option
------------------------------------------------------- 
Python modules             `--install-purelib'
extension modules          `--install-platlib'
all modules                `--install-lib'
scripts                    `--install-scripts'
data                       `--install-data'
C headers                  `--install-headers'

  These override options can be relative, absolute, or explicitly
defined in terms of one of the installation base directories.  (There
are two installation base directories, and they are normally the same--
they only differ when you use the Unix "prefix scheme" and supply
different `--prefix' and `--exec-prefix' options; using `--install-lib'
will override values computed or given for `--install-purelib' and
`--install-platlib', and is recommended for schemes that don't make a
difference between Python and extension modules.)

  For example, say you're installing a module distribution to your home
directory under Unix--but you want scripts to go in `~/scripts' rather
than `~/bin'. As you might expect, you can override this directory with
the `--install-scripts' option; in this case, it makes most sense to
supply a relative path, which will be interpreted relative to the
installation base directory (your home directory, in this case):

    python setup.py install --home=~ --install-scripts=scripts

Another Unix example: suppose your Python installation was built and
installed with a prefix of `/usr/local/python', so under a standard
installation scripts will wind up in `/usr/local/python/bin'.  If you
want them in `/usr/local/bin' instead, you would supply this absolute
directory for the `--install-scripts' option:

    python setup.py install --install-scripts=/usr/local/bin

(This performs an installation using the "prefix scheme," where the
prefix is whatever your Python interpreter was installed with--
`/usr/local/python' in this case.)

  If you maintain Python on Windows, you might want third-party modules
to live in a subdirectory of `_prefix_', rather than right in `_prefix_'
itself.  This is almost as easy as customizing the script installation
directory --you just have to remember that there are two types of
modules to worry about, Python and extension modules, which can
conveniently be both controlled by one option:

    python setup.py install --install-lib=Site

The specified installation directory is relative to `_prefix_'.  Of
course, you also have to ensure that this directory is in Python's
module search path, such as by putting a `.pth' file in a site
directory (see *note site: 158.).  See section *note Modifying Python's
Search Path: 2f6c. to find out how to modify Python's search path.

  If you want to define an entire installation scheme, you just have to
supply all of the installation directory options.  The recommended way
to do this is to supply relative paths; for example, if you want to
maintain all Python module-related files under `python' in your home
directory, and you want a separate directory for each platform that you
use your home directory from, you might define the following
installation scheme:

    python setup.py install --home=~ \
                            --install-purelib=python/lib \
                            --install-platlib=python/lib.$PLAT \
                            --install-scripts=python/scripts
                            --install-data=python/data

or, equivalently,

    python setup.py install --home=~/python \
                            --install-purelib=lib \
                            --install-platlib='lib.$PLAT' \
                            --install-scripts=scripts
                            --install-data=data

`$PLAT' is not (necessarily) an environment variable--it will be
expanded by the Distutils as it parses your command line options, just
as it does when parsing your configuration file(s).

  Obviously, specifying the entire installation scheme every time you
install a new module distribution would be very tedious.  Thus, you can
put these options into your Distutils config file (see section *note
Distutils Configuration Files: 2f63.):

    [install]
    install-base=$HOME
    install-purelib=python/lib
    install-platlib=python/lib.$PLAT
    install-scripts=python/scripts
    install-data=python/data

or, equivalently,

    [install]
    install-base=$HOME/python
    install-purelib=lib
    install-platlib=lib.$PLAT
    install-scripts=scripts
    install-data=data

Note that these two are _not_ equivalent if you supply a different
installation base directory when you run the setup script.  For example,

    python setup.py install --install-base=/tmp

would install pure modules to `/tmp/python/lib' in the first case, and
to `/tmp/lib' in the second case.  (For the second case, you probably
want to supply an installation base of `/tmp/python'.)

  You probably noticed the use of `$HOME' and `$PLAT' in the sample
configuration file input.  These are Distutils configuration variables,
which bear a strong resemblance to environment variables. In fact, you
can use environment variables in config files on platforms that have
such a notion but the Distutils additionally define a few extra
variables that may not be in your environment, such as `$PLAT'.  (And
of course, on systems that don't have environment variables, such as
Mac OS 9, the configuration variables supplied by the Distutils are the
only ones you can use.) See section *note Distutils Configuration
Files: 2f63.  for details.

* Menu:

* Modifying Python's Search Path::


File: python.info,  Node: Modifying Python's Search Path,  Up: Custom Installation

9.4.1 Modifying Python's Search Path
------------------------------------

When the Python interpreter executes an *note import: 1f1. statement,
it searches for both Python code and extension modules along a search
path.  A default value for the path is configured into the Python
binary when the interpreter is built.  You can determine the path by
importing the *note sys: 16d. module and printing the value of
`sys.path'.

    $ python
    Python 2.2 (#11, Oct  3 2002, 13:31:27)
    [GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-112)] on linux2
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import sys
    >>> sys.path
    ['', '/usr/local/lib/python2.3', '/usr/local/lib/python2.3/plat-linux2',
     '/usr/local/lib/python2.3/lib-tk', '/usr/local/lib/python2.3/lib-dynload',
     '/usr/local/lib/python2.3/site-packages']
    >>>

The null string in `sys.path' represents the current working directory.

  The expected convention for locally installed packages is to put them
in the `_..._/site-packages/' directory, but you may want to install
Python modules into some arbitrary directory.  For example, your site
may have a convention of keeping all software related to the web server
under `/www'.  Add-on Python modules might then belong in
`/www/python', and in order to import them, this directory must be
added to `sys.path'.  There are several different ways to add the
directory.

  The most convenient way is to add a path configuration file to a
directory that's already on Python's path, usually to the
`.../site-packages/' directory.  Path configuration files have an
extension of `.pth', and each line must contain a single path that will
be appended to `sys.path'.  (Because the new paths are appended to
`sys.path', modules in the added directories will not override standard
modules.  This means you can't use this mechanism for installing fixed
versions of standard modules.)

  Paths can be absolute or relative, in which case they're relative to
the directory containing the `.pth' file.  See the documentation of the
*note site: 158. module for more information.

  A slightly less convenient way is to edit the `site.py' file in
Python's standard library, and modify `sys.path'.  `site.py' is
automatically imported when the Python interpreter is executed, unless
the *note -S: 62e. switch is supplied to suppress this behaviour.  So
you could simply edit `site.py' and add two lines to it:

    import sys
    sys.path.append('/www/python/')

However, if you reinstall the same major version of Python (perhaps when
upgrading from 2.2 to 2.2.2, for example) `site.py' will be overwritten
by the stock version.  You'd have to remember that it was modified and
save a copy before doing the installation.

  There are two environment variables that can modify `sys.path'.  *note
PYTHONHOME: 62a. sets an alternate value for the prefix of the Python
installation.  For example, if *note PYTHONHOME: 62a. is set to
`/www/python', the search path will be set to `['',
'/www/python/lib/pythonX.Y/', '/www/python/lib/pythonX.Y/plat-linux2',
...]'.

  The *note PYTHONPATH: 577. variable can be set to a list of paths
that will be added to the beginning of `sys.path'.  For example, if *note
PYTHONPATH: 577. is set to `/www/python:/opt/py', the search path will
begin with `['/www/python', '/opt/py']'.  (Note that directories must
exist in order to be added to `sys.path'; the *note site: 158. module
removes paths that don't exist.)

  Finally, `sys.path' is just a regular Python list, so any Python
application can modify it by adding or removing entries.


File: python.info,  Node: Distutils Configuration Files,  Next: Building Extensions Tips and Tricks,  Prev: Custom Installation,  Up: Installing Python Modules

9.5 Distutils Configuration Files
=================================

As mentioned above, you can use Distutils configuration files to record
personal or site preferences for any Distutils options.  That is, any
option to any command can be stored in one of two or three (depending
on your platform) configuration files, which will be consulted before
the command-line is parsed.  This means that configuration files will
override default values, and the command-line will in turn override
configuration files.  Furthermore, if multiple configuration files
apply, values from "earlier" files are overridden by "later" files.

* Menu:

* Location and names of config files::
* Syntax of config files::


File: python.info,  Node: Location and names of config files,  Next: Syntax of config files,  Up: Distutils Configuration Files

9.5.1 Location and names of config files
----------------------------------------

The names and locations of the configuration files vary slightly across
platforms.  On Unix and Mac OS X, the three configuration files (in the
order they are processed) are:

Type of file       Location and filename                                          Notes
---------------------------------------------------------------------------------------------- 
system             `_prefix_/lib/python_ver_/distutils/distutils.cfg'             (1)
personal           `$HOME/.pydistutils.cfg'                                       (2)
local              `setup.cfg'                                                    (3)

  And on Windows, the configuration files are:

Type of file       Location and filename                                 Notes
------------------------------------------------------------------------------------- 
system             `_prefix_\Lib\distutils\distutils.cfg'                (4)
personal           `%HOME%\pydistutils.cfg'                              (5)
local              `setup.cfg'                                           (3)

  On all platforms, the "personal" file can be temporarily disabled by
passing the `-no-user-cfg' option.

  Notes:

  1. Strictly speaking, the system-wide configuration file lives in the
     directory where the Distutils are installed; under Python 1.6 and
     later on Unix, this is as shown. For Python 1.5.2, the Distutils
     will normally be installed to
     `_prefix_/lib/python1.5/site-packages/distutils', so the system
     configuration file should be put there under Python 1.5.2.

  2. On Unix, if the `HOME' environment variable is not defined, the
     user's home directory will be determined with the `getpwuid()'
     function from the standard *note pwd: 13c. module. This is done by
     the *note os.path.expanduser(): dda.  function used by Distutils.

  3. I.e., in the current directory (usually the location of the setup
     script).

  4. (See also note (1).)  Under Python 1.6 and later, Python's default
     "installation prefix" is `C:\Python', so the system configuration
     file is normally `C:\Python\Lib\distutils\distutils.cfg'. Under
     Python 1.5.2, the default prefix was `C:\Program Files\Python',
     and the Distutils were not part of the standard library--so the
     system configuration file would be `C:\Program
     Files\Python\distutils\distutils.cfg' in a standard Python 1.5.2
     installation under Windows.

  5. On Windows, if the `HOME' environment variable is not defined, `USERPROFILE'
     then `HOMEDRIVE' and `HOMEPATH' will be tried. This is done by the
     *note os.path.expanduser(): dda. function used by Distutils.


File: python.info,  Node: Syntax of config files,  Prev: Location and names of config files,  Up: Distutils Configuration Files

9.5.2 Syntax of config files
----------------------------

The Distutils configuration files all have the same syntax.  The config
files are grouped into sections.  There is one section for each
Distutils command, plus a `global' section for global options that
affect every command.  Each section consists of one option per line,
specified as `option=value'.

  For example, the following is a complete config file that just forces
all commands to run quietly by default:

    [global]
    verbose=0

If this is installed as the system config file, it will affect all
processing of any Python module distribution by any user on the current
system.  If it is installed as your personal config file (on systems
that support them), it will affect only module distributions processed
by you.  And if it is used as the `setup.cfg' for a particular module
distribution, it affects only that distribution.

  You could override the default "build base" directory and make the
*build** commands always forcibly rebuild all files with the following:

    [build]
    build-base=blib
    force=1

which corresponds to the command-line arguments

    python setup.py build --build-base=blib --force

except that including the *build* command on the command-line means
that command will be run.  Including a particular command in config
files has no such implication; it only means that if the command is
run, the options in the config file will apply.  (Or if other commands
that derive values from it are run, they will use the values in the
config file.)

  You can find out the complete list of options for any command using
the *note -help: 1d2. option, e.g.:

    python setup.py build --help

and you can find out the complete list of global options by using *note
-help: 1d2. without a command:

    python setup.py --help

See also the "Reference" section of the "Distributing Python Modules"
manual.


File: python.info,  Node: Building Extensions Tips and Tricks,  Prev: Distutils Configuration Files,  Up: Installing Python Modules

9.6 Building Extensions: Tips and Tricks
========================================

Whenever possible, the Distutils try to use the configuration
information made available by the Python interpreter used to run the
`setup.py' script.  For example, the same compiler and linker flags
used to compile Python will also be used for compiling extensions.
Usually this will work well, but in complicated situations this might
be inappropriate.  This section discusses how to override the usual
Distutils behaviour.

* Menu:

* Tweaking compiler/linker flags::
* Using non-Microsoft compilers on Windows::


File: python.info,  Node: Tweaking compiler/linker flags,  Next: Using non-Microsoft compilers on Windows,  Up: Building Extensions Tips and Tricks

9.6.1 Tweaking compiler/linker flags
------------------------------------

Compiling a Python extension written in C or C++ will sometimes require
specifying custom flags for the compiler and linker in order to use a
particular library or produce a special kind of object code. This is
especially true if the extension hasn't been tested on your platform,
or if you're trying to cross-compile Python.

  In the most general case, the extension author might have foreseen
that compiling the extensions would be complicated, and provided a
`Setup' file for you to edit.  This will likely only be done if the
module distribution contains many separate extension modules, or if
they often require elaborate sets of compiler flags in order to work.

  A `Setup' file, if present, is parsed in order to get a list of
extensions to build.  Each line in a `Setup' describes a single module.
Lines have the following structure:

    module ... [sourcefile ...] [cpparg ...] [library ...]

Let's examine each of the fields in turn.

   * _module_ is the name of the extension module to be built, and
     should be a valid Python identifier.  You can't just change this
     in order to rename a module (edits to the source code would also
     be needed), so this should be left alone.

   * _sourcefile_ is anything that's likely to be a source code file,
     at least judging by the filename.  Filenames ending in `.c' are
     assumed to be written in C, filenames ending in `.C', `.cc', and
     `.c++' are assumed to be C++, and filenames ending in `.m' or
     `.mm' are assumed to be in Objective C.

   * _cpparg_ is an argument for the C preprocessor,  and is anything
     starting with `-I', `-D', *note -U: 63b. or `-C'.

   * _library_ is anything ending in `.a' or beginning with `-l' or
     `-L'.

  If a particular platform requires a special library on your platform,
you can add it by editing the `Setup' file and running `python setup.py
build'.  For example, if the module defined by the line

    foo foomodule.c

must be linked with the math library `libm.a' on your platform, simply
add `-lm' to the line:

    foo foomodule.c -lm

Arbitrary switches intended for the compiler or the linker can be
supplied with the `-Xcompiler' _arg_ and `-Xlinker' _arg_ options:

    foo foomodule.c -Xcompiler -o32 -Xlinker -shared -lm

The next option after `-Xcompiler' and `-Xlinker' will be appended to
the proper command line, so in the above example the compiler will be
passed the `-o32' option, and the linker will be passed `-shared'.  If
a compiler option requires an argument, you'll have to supply multiple
`-Xcompiler' options; for example, to pass `-x c++' the `Setup' file
would have to contain `-Xcompiler -x -Xcompiler c++'.

  Compiler flags can also be supplied through setting the `CFLAGS'
environment variable.  If set, the contents of `CFLAGS' will be added to
the compiler flags specified in the  `Setup' file.


File: python.info,  Node: Using non-Microsoft compilers on Windows,  Prev: Tweaking compiler/linker flags,  Up: Building Extensions Tips and Tricks

9.6.2 Using non-Microsoft compilers on Windows
----------------------------------------------

* Menu:

* Borland/CodeGear C++::
* GNU C / Cygwin / MinGW::


File: python.info,  Node: Borland/CodeGear C++,  Next: GNU C / Cygwin / MinGW,  Up: Using non-Microsoft compilers on Windows

9.6.2.1 Borland/CodeGear C++
............................

This subsection describes the necessary steps to use Distutils with the
Borland C++ compiler version 5.5.  First you have to know that
Borland's object file format (OMF) is different from the format used by
the Python version you can download from the Python or ActiveState Web
site.  (Python is built with Microsoft Visual C++, which uses COFF as
the object file format.) For this reason you have to convert Python's
library `python25.lib' into the Borland format.  You can do this as
follows:

    coff2omf python25.lib python25_bcpp.lib

The `coff2omf' program comes with the Borland compiler.  The file
`python25.lib' is in the `Libs' directory of your Python installation.
If your extension uses other libraries (zlib, ...) you have to convert
them too.

  The converted files have to reside in the same directories as the
normal libraries.

  How does Distutils manage to use these libraries with their changed
names?  If the extension needs a library (eg. `foo') Distutils checks
first if it finds a library with suffix `_bcpp' (eg. `foo_bcpp.lib')
and then uses this library.  In the case it doesn't find such a special
library it uses the default name (`foo.lib'.) (1)

  To let Distutils compile your extension with Borland C++ you now have
to type:

    python setup.py build --compiler=bcpp

If you want to use the Borland C++ compiler as the default, you could
specify this in your personal or system-wide configuration file for
Distutils (see section *note Distutils Configuration Files: 2f63.)

See also
........

C++Builder Compiler(2)
     Information about the free C++ compiler from Borland, including
     links to the download pages.

Creating Python Extensions Using Borland's Free Compiler(3)
     Document describing how to use Borland's free command-line C++
     compiler to build Python.

  ---------- Footnotes ----------

  (1) This also means you could replace all existing COFF-libraries
with OMF-libraries of the same name.

  (2) http://www.codegear.com/downloads/free/cppbuilder

  (3) http://www.cyberus.ca/~g_will/pyExtenDL.shtml


File: python.info,  Node: GNU C / Cygwin / MinGW,  Prev: Borland/CodeGear C++,  Up: Using non-Microsoft compilers on Windows

9.6.2.2 GNU C / Cygwin / MinGW
..............................

This section describes the necessary steps to use Distutils with the
GNU C/C++ compilers in their Cygwin and MinGW distributions. (1) For a
Python interpreter that was built with Cygwin, everything should work
without any of these following steps.

  Not all extensions can be built with MinGW or Cygwin, but many can.
Extensions most likely to not work are those that use C++ or depend on
Microsoft Visual C extensions.

  To let Distutils compile your extension with Cygwin you have to type:

    python setup.py build --compiler=cygwin

and for Cygwin in no-cygwin mode (2) or for MinGW type:

    python setup.py build --compiler=mingw32

If you want to use any of these options/compilers as default, you should
consider writing it in your personal or system-wide configuration file
for Distutils (see section *note Distutils Configuration Files: 2f63.)

* Menu:

* Older Versions of Python and MinGW::

  ---------- Footnotes ----------

  (1) Check <http://sources.redhat.com/cygwin/> and
<http://www.mingw.org/> for more information

  (2) Then you have no POSIX emulation available, but you also don't
need `cygwin1.dll'.


File: python.info,  Node: Older Versions of Python and MinGW,  Up: GNU C / Cygwin / MinGW

9.6.2.3 Older Versions of Python and MinGW
..........................................

The following instructions only apply if you're using a version of
Python inferior to 2.4.1 with a MinGW inferior to 3.0.0 (with
binutils-2.13.90-20030111-1).

  These compilers require some special libraries.  This task is more
complex than for Borland's C++, because there is no program to convert
the library.  First you have to create a list of symbols which the
Python DLL exports. (You can find a good program for this task at
<http://www.emmestech.com/software/pexports-0.43/download_pexports.html>).

    pexports python25.dll >python25.def

The location of an installed `python25.dll' will depend on the
installation options and the version and language of Windows.  In a
"just for me" installation, it will appear in the root of the
installation directory.  In a shared installation, it will be located
in the system directory.

  Then you can create from these information an import library for gcc.

    /cygwin/bin/dlltool --dllname python25.dll --def python25.def --output-lib libpython25.a

The resulting library has to be placed in the same directory as
`python25.lib'. (Should be the `libs' directory under your Python
installation directory.)

  If your extension uses other libraries (zlib,...) you might  have to
convert them too. The converted files have to reside in the same
directories as the normal libraries do.

See also
........

Building Python modules on MS Windows platform with MinGW(1)
     Information about building the required libraries for the MinGW
     environment.

  ---------- Footnotes ----------

  (1) http://www.zope.org/Members/als/tips/win32_mingw_modules


File: python.info,  Node: Python HOWTOs,  Next: Python Frequently Asked Questions,  Prev: Installing Python Modules,  Up: Top

10 Python HOWTOs
****************

Python HOWTOs are documents that cover a single, specific topic, and
attempt to cover it fairly completely. Modelled on the Linux
Documentation Project's HOWTO collection, this collection is an effort
to foster documentation that's more detailed than the Python Library
Reference.

  Currently, the HOWTOs are:

* Menu:

* Porting Python 2 Code to Python 3::
* Porting Extension Modules to Python 3::
* Curses Programming with Python::
* Descriptor HowTo Guide::
* Idioms and Anti-Idioms in Python::
* Functional Programming HOWTO::
* Logging HOWTO::
* Logging Cookbook::
* Regular Expression HOWTO::
* Socket Programming HOWTO::
* Sorting HOW TO::
* Unicode HOWTO::
* HOWTO Fetch Internet Resources Using urllib2::
* HOWTO Use Python in the web::
* Argparse Tutorial::

Porting Python 2 Code to Python 3

* Choosing a Strategy::
* Python 3 and 3to2::
* Python 2 and 2to3::
* Python 2/3 Compatible Source::
* Other Resources: Other Resources<2>.

Choosing a Strategy

* Universal Bits of Advice::

Python 2 and 2to3

* Support Python 2.7: Support Python 2 7.
* Try to Support Python 2.6 and Newer Only: Try to Support Python 2 6 and Newer Only.
* Supporting Python 2.5 and Newer Only: Supporting Python 2 5 and Newer Only.
* Handle Common "Gotchas"::
* Eliminate -3 Warnings::
* Run 2to3::
* Verify & Test::

Try to Support Python 2.6 and Newer Only

* from __future__ import print_function::
* from __future__ import unicode_literals::
* Bytes literals::

Supporting Python 2.5 and Newer Only

* from __future__ import absolute_import::

Handle Common "Gotchas"

* from __future__ import division::
* Specify when opening a file as binary::
* Text files::
* Subclass object::
* Deal With the Bytes/String Dichotomy::
* Indexing bytes objects::
* __str__()/__unicode__(): __str__ /__unicode__.
* Don't Index on Exceptions::
* Don't use __getslice__ & Friends::
* Updating doctests::

Deal With the Bytes/String Dichotomy

* Mark Up Python 2 String Literals::
* Decide what APIs Will Accept::
* Bytes / Unicode Comparison::

Run 2to3

* Manually::
* During Installation::

Python 2/3 Compatible Source

* Follow The Steps for Using 2to3::
* Use six::
* Capturing the Currently Raised Exception::

Porting Extension Modules to Python 3

* Conditional compilation::
* Changes to Object APIs::
* Module initialization and state::
* CObject replaced with Capsule::
* Other options: Other options<2>.

Changes to Object APIs

* str/unicode Unification::
* long/int Unification::

Curses Programming with Python

* What is curses?::
* Starting and ending a curses application::
* Windows and Pads::
* Displaying Text::
* User Input::
* For More Information::

What is curses?

* The Python curses module::

Displaying Text

* Attributes and Color::

Descriptor HowTo Guide

* Abstract::
* Definition and Introduction::
* Descriptor Protocol::
* Invoking Descriptors: Invoking Descriptors<2>.
* Descriptor Example::
* Properties::
* Functions and Methods::
* Static Methods and Class Methods::

Idioms and Anti-Idioms in Python

* Language Constructs You Should Not Use::
* Exceptions: Exceptions<8>.
* Using the Batteries::
* Using Backslash to Continue Statements::

Language Constructs You Should Not Use

* from module import *::
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends.
* from module import name1, name2: from module import name1 name2.
* except;: except.

from module import *

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::

Functional Programming HOWTO

* Introduction: Introduction<12>.
* Iterators: Iterators<2>.
* Generator expressions and list comprehensions::
* Generators: Generators<2>.
* Built-in functions::
* Small functions and the lambda expression::
* The itertools module::
* The functools module::
* Revision History and Acknowledgements::
* References::

Introduction

* Formal provability::
* Modularity::
* Ease of debugging and testing::
* Composability::

Iterators

* Data Types That Support Iterators::

Generators

* Passing values into a generator::

The itertools module

* Creating new iterators::
* Calling functions on elements::
* Selecting elements::
* Grouping elements::

The functools module

* The operator module::

References

* General::
* Python-specific::
* Python documentation::

Logging HOWTO

* Basic Logging Tutorial::
* Advanced Logging Tutorial::
* Logging Levels::
* Useful Handlers::
* Exceptions raised during logging::
* Using arbitrary objects as messages::
* Optimization::

Basic Logging Tutorial

* When to use logging::
* A simple example::
* Logging to a file::
* Logging from multiple modules::
* Logging variable data::
* Changing the format of displayed messages::
* Displaying the date/time in messages::
* Next Steps::

Advanced Logging Tutorial

* Logging Flow::
* Loggers::
* Handlers::
* Formatters::
* Configuring Logging::
* What happens if no configuration is provided::
* Configuring Logging for a Library::

Logging Levels

* Custom Levels::

Logging Cookbook

* Using logging in multiple modules::
* Multiple handlers and formatters::
* Logging to multiple destinations::
* Configuration server example::
* Sending and receiving logging events across a network::
* Adding contextual information to your logging output::
* Logging to a single file from multiple processes::
* Using file rotation::
* An example dictionary-based configuration::
* Inserting a BOM into messages sent to a SysLogHandler::
* Implementing structured logging::

Adding contextual information to your logging output

* Using LoggerAdapters to impart contextual information::
* Using Filters to impart contextual information::

Regular Expression HOWTO

* Introduction: Introduction<13>.
* Simple Patterns::
* Using Regular Expressions::
* More Pattern Power::
* Modifying Strings::
* Common Problems::
* Feedback::

Simple Patterns

* Matching Characters::
* Repeating Things::

Using Regular Expressions

* Compiling Regular Expressions::
* The Backslash Plague::
* Performing Matches::
* Module-Level Functions: Module-Level Functions<2>.
* Compilation Flags::

More Pattern Power

* More Metacharacters::
* Grouping::
* Non-capturing and Named Groups::
* Lookahead Assertions::

Modifying Strings

* Splitting Strings::
* Search and Replace::

Common Problems

* Use String Methods::
* match() versus search(): match versus search.
* Greedy versus Non-Greedy::
* Using re.VERBOSE: Using re VERBOSE.

Socket Programming HOWTO

* Sockets::
* Creating a Socket::
* Using a Socket::
* Disconnecting::
* Non-blocking Sockets::

Sockets

* History::

Creating a Socket

* IPC::

Using a Socket

* Binary Data::

Disconnecting

* When Sockets Die::

Non-blocking Sockets

* Performance: Performance<2>.

Sorting HOW TO

* Sorting Basics::
* Key Functions::
* Operator Module Functions::
* Ascending and Descending::
* Sort Stability and Complex Sorts::
* The Old Way Using Decorate-Sort-Undecorate::
* The Old Way Using the cmp Parameter::
* Odd and Ends::

Unicode HOWTO

* Introduction to Unicode::
* Python 2.x's Unicode Support: Python 2 x's Unicode Support.
* Reading and Writing Unicode Data::
* Revision History and Acknowledgements: Revision History and Acknowledgements<2>.

Introduction to Unicode

* History of Character Codes::
* Definitions::
* Encodings::
* References: References<2>.

Python 2.x's Unicode Support

* The Unicode Type::
* Unicode Literals in Python Source Code::
* Unicode Properties::
* References: References<3>.

Reading and Writing Unicode Data

* Unicode filenames::
* Tips for Writing Unicode-aware Programs::
* References: References<4>.

HOWTO Fetch Internet Resources Using urllib2

* Introduction: Introduction<14>.
* Fetching URLs::
* Handling Exceptions: Handling Exceptions<2>.
* info and geturl::
* Openers and Handlers::
* Basic Authentication::
* Proxies::
* Sockets and Layers::
* Footnotes::

Fetching URLs

* Data::
* Headers::

Handling Exceptions

* URLError::
* HTTPError::
* Wrapping it Up::

HTTPError

* Error Codes::

Wrapping it Up

* Number 1::
* Number 2::

HOWTO Use Python in the web

* The Low-Level View::
* Step back; WSGI: Step back WSGI.
* Model-View-Controller::
* Ingredients for Websites::
* Frameworks::

The Low-Level View

* Common Gateway Interface::
* mod_python::
* FastCGI and SCGI::
* mod_wsgi::

Common Gateway Interface

* Simple script for testing CGI::
* Setting up CGI on your own server::
* Common problems with CGI scripts::

FastCGI and SCGI

* Setting up FastCGI::

Step back: WSGI

* WSGI Servers::
* Case study; MoinMoin: Case study MoinMoin.

Ingredients for Websites

* Templates::
* Data persistence::

Frameworks

* Some notable frameworks::

Some notable frameworks

* Django::
* TurboGears::
* Zope::
* Other notable frameworks::

Argparse Tutorial

* Concepts::
* The basics::
* Introducing Positional arguments::
* Introducing Optional arguments::
* Combining Positional and Optional arguments::
* Getting a little more advanced::
* Conclusion::

Introducing Optional arguments

* Short options::

Getting a little more advanced

* Conflicting options::


File: python.info,  Node: Porting Python 2 Code to Python 3,  Next: Porting Extension Modules to Python 3,  Up: Python HOWTOs

10.1 Porting Python 2 Code to Python 3
======================================

     author: Brett Cannon

Abstract
........

With Python 3 being the future of Python while Python 2 is still in
active use, it is good to have your project available for both major
releases of Python. This guide is meant to help you choose which
strategy works best for your project to support both Python 2 & 3 along
with how to execute that strategy.

  If you are looking to port an extension module instead of pure Python
code, please see *note Porting Extension Modules to Python 3: 2f85.

* Menu:

* Choosing a Strategy::
* Python 3 and 3to2::
* Python 2 and 2to3::
* Python 2/3 Compatible Source::
* Other Resources: Other Resources<2>.

Choosing a Strategy

* Universal Bits of Advice::

Python 2 and 2to3

* Support Python 2.7: Support Python 2 7.
* Try to Support Python 2.6 and Newer Only: Try to Support Python 2 6 and Newer Only.
* Supporting Python 2.5 and Newer Only: Supporting Python 2 5 and Newer Only.
* Handle Common "Gotchas"::
* Eliminate -3 Warnings::
* Run 2to3::
* Verify & Test::

Try to Support Python 2.6 and Newer Only

* from __future__ import print_function::
* from __future__ import unicode_literals::
* Bytes literals::

Supporting Python 2.5 and Newer Only

* from __future__ import absolute_import::

Handle Common "Gotchas"

* from __future__ import division::
* Specify when opening a file as binary::
* Text files::
* Subclass object::
* Deal With the Bytes/String Dichotomy::
* Indexing bytes objects::
* __str__()/__unicode__(): __str__ /__unicode__.
* Don't Index on Exceptions::
* Don't use __getslice__ & Friends::
* Updating doctests::

Deal With the Bytes/String Dichotomy

* Mark Up Python 2 String Literals::
* Decide what APIs Will Accept::
* Bytes / Unicode Comparison::

Run 2to3

* Manually::
* During Installation::

Python 2/3 Compatible Source

* Follow The Steps for Using 2to3::
* Use six::
* Capturing the Currently Raised Exception::


File: python.info,  Node: Choosing a Strategy,  Next: Python 3 and 3to2,  Up: Porting Python 2 Code to Python 3

10.1.1 Choosing a Strategy
--------------------------

When a project makes the decision that it's time to support both Python
2 & 3, a decision needs to be made as to how to go about accomplishing
that goal.  The chosen strategy will depend on how large the project's
existing codebase is and how much divergence you want from your Python
2 codebase from your Python 3 one (e.g., starting a new version with
Python 3).

  If your project is brand-new or does not have a large codebase, then
you may want to consider writing/porting *note all of your code for
Python 3 and use 3to2: 2f87. to port your code for Python 2.

  If you would prefer to maintain a codebase which is semantically *and*
syntactically compatible with Python 2 & 3 simultaneously, you can write
*note Python 2/3 Compatible Source: 2f88. While this tends to lead to
somewhat non-idiomatic code, it does mean you keep a rapid development
process for you, the developer.

  Finally, you do have the option of *note using 2to3: 2f89. to
translate Python 2 code into Python 3 code (with some manual help).
This can take the form of branching your code and using 2to3 to start a
Python 3 branch. You can also have users perform the translation at
installation time automatically so that you only have to maintain a
Python 2 codebase.

  Regardless of which approach you choose, porting is not as hard or
time-consuming as you might initially think. You can also tackle the
problem piece-meal as a good portion of porting is simply updating your
code to follow current best practices in a Python 2/3 compatible way.

* Menu:

* Universal Bits of Advice::


File: python.info,  Node: Universal Bits of Advice,  Up: Choosing a Strategy

10.1.1.1 Universal Bits of Advice
.................................

Regardless of what strategy you pick, there are a few things you should
consider.

  One is make sure you have a robust test suite. You need to make sure
everything continues to work, just like when you support a new minor
version of Python.  This means making sure your test suite is thorough
and is ported properly between Python 2 & 3. You will also most likely
want to use something like tox(1) to automate testing between both a
Python 2 and Python 3 VM.

  Two, once your project has Python 3 support, make sure to add the
proper classifier on the Cheeseshop(2) (PyPI(3)). To have your project
listed as Python 3 compatible it must have the Python 3 classifier(4)
(from
<http://techspot.zzzeek.org/2011/01/24/zzzeek-s-guide-to-python-3-porting/>):

    setup(
      name='Your Library',
      version='1.0',
      classifiers=[
          # make sure to use :: Python *and* :: Python :: 3 so
          # that pypi can list the package on the python 3 page
          'Programming Language :: Python',
          'Programming Language :: Python :: 3'
      ],
      packages=['yourlibrary'],
      # make sure to add custom_fixers to the MANIFEST.in
      include_package_data=True,
      # ...
    )

Doing so will cause your project to show up in the Python 3 packages
list(5). You will know you set the classifier properly as visiting your
project page on the Cheeseshop will show a Python 3 logo in the
upper-left corner of the page.

  Three, the six(6) project provides a library which helps iron out
differences between Python 2 & 3. If you find there is a sticky point
that is a continual point of contention in your translation or
maintenance of code, consider using a source-compatible solution
relying on six. If you have to create your own Python 2/3 compatible
solution, you can use `sys.version_info[0] >= 3' as a guard.

  Four, read all the approaches. Just because some bit of advice
applies to one approach more than another doesn't mean that some advice
doesn't apply to other strategies.

  Five, drop support for older Python versions if possible. Python
2.5(7) introduced a lot of useful syntax and libraries which have
become idiomatic in Python 3. Python 2.6(8) introduced future
statements which makes compatibility much easier if you are going from
Python 2 to 3.  Python 2.7(9) continues the trend in the stdlib. So
choose the newest version of Python which you believe can be your
minimum support version and work from there.

  ---------- Footnotes ----------

  (1) http://codespeak.net/tox/

  (2) http://pypi.python.org/

  (3) http://pypi.python.org/

  (4) http://pypi.python.org/pypi?:action=browse&c=533

  (5) http://pypi.python.org/pypi?:action=browse&c=533&show=all

  (6) http://packages.python.org/six

  (7) http://www.python.org/2.5.x

  (8) http://www.python.org/2.6.x

  (9) http://www.python.org/2.7.x


File: python.info,  Node: Python 3 and 3to2,  Next: Python 2 and 2to3,  Prev: Choosing a Strategy,  Up: Porting Python 2 Code to Python 3

10.1.2 Python 3 and 3to2
------------------------

If you are starting a new project or your codebase is small enough, you
may want to consider writing your code for Python 3 and backporting to
Python 2 using 3to2(1). Thanks to Python 3 being more strict about
things than Python 2 (e.g., bytes vs. strings), the source translation
can be easier and more straightforward than from Python 2 to 3. Plus it
gives you more direct experience developing in Python 3 which, since it
is the future of Python, is a good thing long-term.

  A drawback of this approach is that 3to2 is a third-party project.
This means that the Python core developers (and thus this guide) can
make no promises about how well 3to2 works at any time. There is
nothing to suggest, though, that 3to2 is not a high-quality project.

  ---------- Footnotes ----------

  (1) https://bitbucket.org/amentajo/lib3to2/overview


File: python.info,  Node: Python 2 and 2to3,  Next: Python 2/3 Compatible Source,  Prev: Python 3 and 3to2,  Up: Porting Python 2 Code to Python 3

10.1.3 Python 2 and 2to3
------------------------

Included with Python since 2.6, the 2to3(1) tool (and *note lib2to3:
fe. module) helps with porting Python 2 to Python 3 by performing
various source translations. This is a perfect solution for projects
which wish to branch their Python 3 code from their Python 2 codebase
and maintain them as independent codebases. You can even begin
preparing to use this approach today by writing future-compatible
Python code which works cleanly in Python 2 in conjunction with 2to3;
all steps outlined below will work with Python 2 code up to the point
when the actual use of 2to3 occurs.

  Use of 2to3 as an on-demand translation step at install time is also
possible, preventing the need to maintain a separate Python 3 codebase,
but this approach does come with some drawbacks. While users will only
have to pay the translation cost once at installation, you as a
developer will need to pay the cost regularly during development. If
your codebase is sufficiently large enough then the translation step
ends up acting like a compilation step, robbing you of the rapid
development process you are used to with Python.  Obviously the time
required to translate a project will vary, so do an experimental
translation just to see how long it takes to evaluate whether you
prefer this approach compared to using *note Python 2/3 Compatible
Source: 2f88. or simply keeping a separate Python 3 codebase.

  Below are the typical steps taken by a project which uses a
2to3-based approach to supporting Python 2 & 3.

* Menu:

* Support Python 2.7: Support Python 2 7.
* Try to Support Python 2.6 and Newer Only: Try to Support Python 2 6 and Newer Only.
* Supporting Python 2.5 and Newer Only: Supporting Python 2 5 and Newer Only.
* Handle Common "Gotchas"::
* Eliminate -3 Warnings::
* Run 2to3::
* Verify & Test::

  ---------- Footnotes ----------

  (1) http://docs.python.org/py3k/library/2to3.html


File: python.info,  Node: Support Python 2 7,  Next: Try to Support Python 2 6 and Newer Only,  Up: Python 2 and 2to3

10.1.3.1 Support Python 2.7
...........................

As a first step, make sure that your project is compatible with Python
2.7(1).  This is just good to do as Python 2.7 is the last release of
Python 2 and thus will be used for a rather long time. It also allows
for use of the `-3' flag to Python to help discover places in your code
which 2to3 cannot handle but are known to cause issues.

  ---------- Footnotes ----------

  (1) http://www.python.org/2.7.x


File: python.info,  Node: Try to Support Python 2 6 and Newer Only,  Next: Supporting Python 2 5 and Newer Only,  Prev: Support Python 2 7,  Up: Python 2 and 2to3

10.1.3.2 Try to Support Python 2.6 and Newer Only
.................................................

While not possible for all projects, if you can support Python 2.6(1)
and newer *only*, your life will be much easier. Various future
statements, stdlib additions, etc. exist only in Python 2.6 and later
which greatly assist in porting to Python 3. But if you project must
keep support for Python 2.5(2) (or even Python 2.4(3)) then it is still
possible to port to Python 3.

  Below are the benefits you gain if you only have to support Python
2.6 and newer. Some of these options are personal choice while others
are *strongly* recommended (the ones that are more for personal choice
are labeled as such).  If you continue to support older versions of
Python then you at least need to watch out for situations that these
solutions fix.

* Menu:

* from __future__ import print_function::
* from __future__ import unicode_literals::
* Bytes literals::

  ---------- Footnotes ----------

  (1) http://www.python.org/2.6.x

  (2) http://www.python.org/2.5.x

  (3) http://www.python.org/2.4.x


File: python.info,  Node: from __future__ import print_function,  Next: from __future__ import unicode_literals,  Up: Try to Support Python 2 6 and Newer Only

10.1.3.3 `from __future__ import print_function'
................................................

This is a personal choice. 2to3 handles the translation from the print
statement to the print function rather well so this is an optional
step. This future statement does help, though, with getting used to
typing `print('Hello, World')' instead of `print 'Hello, World''.


File: python.info,  Node: from __future__ import unicode_literals,  Next: Bytes literals,  Prev: from __future__ import print_function,  Up: Try to Support Python 2 6 and Newer Only

10.1.3.4 `from __future__ import unicode_literals'
..................................................

Another personal choice. You can always mark what you want to be a
(unicode) string with a `u' prefix to get the same effect. But
regardless of whether you use this future statement or not, you *must*
make sure you know exactly which Python 2 strings you want to be bytes,
and which are to be strings. This means you should, *at minimum* mark
all strings that are meant to be text strings with a `u' prefix if you
do not use this future statement.


File: python.info,  Node: Bytes literals,  Prev: from __future__ import unicode_literals,  Up: Try to Support Python 2 6 and Newer Only

10.1.3.5 Bytes literals
.......................

This is a *very* important one. The ability to prefix Python 2 strings
that are meant to contain bytes with a `b' prefix help to very clearly
delineate what is and is not a Python 3 string. When you run 2to3 on
code, all Python 2 strings become Python 3 strings *unless* they are
prefixed with `b'.

  There are some differences between byte literals in Python 2 and
those in Python 3 thanks to the bytes type just being an alias to `str'
in Python 2.  Probably the biggest "gotcha" is that indexing results in
different values. In Python 2, the value of `b'py'[1]' is `'y'', while
in Python 3 it's `121'.  You can avoid this disparity by always slicing
at the size of a single element: `b'py'[1:2]' is `'y'' in Python 2 and
`b'y'' in Python 3 (i.e., close enough).

  You cannot concatenate bytes and strings in Python 3. But since Python
2 has bytes aliased to `str', it will succeed: `b'a' + u'b'' works in
Python 2, but `b'a' + 'b'' in Python 3 is a *note TypeError: 215. A
similar issue also comes about when doing comparisons between bytes and
strings.


File: python.info,  Node: Supporting Python 2 5 and Newer Only,  Next: Handle Common "Gotchas",  Prev: Try to Support Python 2 6 and Newer Only,  Up: Python 2 and 2to3

10.1.3.6 Supporting Python 2.5 and Newer Only
.............................................

If you are supporting Python 2.5(1) and newer there are still some
features of Python that you can utilize.

* Menu:

* from __future__ import absolute_import::

  ---------- Footnotes ----------

  (1) http://www.python.org/2.5.x


File: python.info,  Node: from __future__ import absolute_import,  Up: Supporting Python 2 5 and Newer Only

10.1.3.7 `from __future__ import absolute_import'
.................................................

Implicit relative imports (e.g., importing `spam.bacon' from within
`spam.eggs' with the statement `import bacon') does not work in Python
3.  This future statement moves away from that and allows the use of
explicit relative imports (e.g., `from . import bacon').

  In Python 2.5(1) you must use the __future__ statement to get to use
explicit relative imports and prevent implicit ones. In Python 2.6(2)
explicit relative imports are available without the statement, but you
still want the __future__ statement to prevent implicit relative
imports. In Python 2.7(3) the __future__ statement is not needed. In
other words, unless you are only supporting Python 2.7 or a version
earlier than Python 2.5, use the __future__ statement.

  ---------- Footnotes ----------

  (1) http://www.python.org/2.5.x

  (2) http://www.python.org/2.6.x

  (3) http://www.python.org/2.7.x


File: python.info,  Node: Handle Common "Gotchas",  Next: Eliminate -3 Warnings,  Prev: Supporting Python 2 5 and Newer Only,  Up: Python 2 and 2to3

10.1.3.8 Handle Common "Gotchas"
................................

There are a few things that just consistently come up as sticking
points for people which 2to3 cannot handle automatically or can easily
be done in Python 2 to help modernize your code.

* Menu:

* from __future__ import division::
* Specify when opening a file as binary::
* Text files::
* Subclass object::
* Deal With the Bytes/String Dichotomy::
* Indexing bytes objects::
* __str__()/__unicode__(): __str__ /__unicode__.
* Don't Index on Exceptions::
* Don't use __getslice__ & Friends::
* Updating doctests::


File: python.info,  Node: from __future__ import division,  Next: Specify when opening a file as binary,  Up: Handle Common "Gotchas"

10.1.3.9 `from __future__ import division'
..........................................

While the exact same outcome can be had by using the `-Qnew' argument to
Python, using this future statement lifts the requirement that your
users use the flag to get the expected behavior of division in Python 3
(e.g., `1/2 == 0.5; 1//2 == 0').


File: python.info,  Node: Specify when opening a file as binary,  Next: Text files,  Prev: from __future__ import division,  Up: Handle Common "Gotchas"

10.1.3.10 Specify when opening a file as binary
...............................................

Unless you have been working on Windows, there is a chance you have not
always bothered to add the `b' mode when opening a binary file (e.g.,
`rb' for binary reading).  Under Python 3, binary files and text files
are clearly distinct and mutually incompatible; see the *note io: f9.
module for details.  Therefore, you *must* make a decision of whether a
file will be used for binary access (allowing to read and/or write
bytes data) or text access (allowing to read and/or write unicode data).


File: python.info,  Node: Text files,  Next: Subclass object,  Prev: Specify when opening a file as binary,  Up: Handle Common "Gotchas"

10.1.3.11 Text files
....................

Text files created using `open()' under Python 2 return byte strings,
while under Python 3 they return unicode strings.  Depending on your
porting strategy, this can be an issue.

  If you want text files to return unicode strings in Python 2, you
have two possibilities:

   * Under Python 2.6 and higher, use *note io.open(): 118e.  Since
     *note io.open(): 118e.  is essentially the same function in both
     Python 2 and Python 3, it will help iron out any issues that might
     arise.

   * If pre-2.6 compatibility is needed, then you should use *note
     codecs.open(): a48.  instead.  This will make sure that you get
     back unicode strings in Python 2.


File: python.info,  Node: Subclass object,  Next: Deal With the Bytes/String Dichotomy,  Prev: Text files,  Up: Handle Common "Gotchas"

10.1.3.12 Subclass `object'
...........................

New-style classes have been around since Python 2.2(1). You need to
make sure you are subclassing from `object' to avoid odd edge cases
involving method resolution order, etc. This continues to be totally
valid in Python 3 (although unneeded as all classes implicitly inherit
from `object').

  ---------- Footnotes ----------

  (1) http://www.python.org/2.2.x


File: python.info,  Node: Deal With the Bytes/String Dichotomy,  Next: Indexing bytes objects,  Prev: Subclass object,  Up: Handle Common "Gotchas"

10.1.3.13 Deal With the Bytes/String Dichotomy
..............................................

One of the biggest issues people have when porting code to Python 3 is
handling the bytes/string dichotomy. Because Python 2 allowed the `str'
type to hold textual data, people have over the years been rather loose
in their delineation of what `str' instances held text compared to
bytes. In Python 3 you cannot be so care-free anymore and need to
properly handle the difference. The key handling this issue is to make
sure that *every* string literal in your Python 2 code is either
syntactically of functionally marked as either bytes or text data.
After this is done you then need to make sure your APIs are designed to
either handle a specific type or made to be properly polymorphic.

* Menu:

* Mark Up Python 2 String Literals::
* Decide what APIs Will Accept::
* Bytes / Unicode Comparison::


File: python.info,  Node: Mark Up Python 2 String Literals,  Next: Decide what APIs Will Accept,  Up: Deal With the Bytes/String Dichotomy

10.1.3.14 Mark Up Python 2 String Literals
..........................................

First thing you must do is designate every single string literal in
Python 2 as either textual or bytes data. If you are only supporting
Python 2.6 or newer, this can be accomplished by marking bytes literals
with a `b' prefix and then designating textual data with a `u' prefix
or using the `unicode_literals' future statement.

  If your project supports versions of Python predating 2.6, then you
should use the six(1) project and its `b()' function to denote bytes
literals. For text literals you can either use six's `u()' function or
use a `u' prefix.

  ---------- Footnotes ----------

  (1) http://packages.python.org/six


File: python.info,  Node: Decide what APIs Will Accept,  Next: Bytes / Unicode Comparison,  Prev: Mark Up Python 2 String Literals,  Up: Deal With the Bytes/String Dichotomy

10.1.3.15 Decide what APIs Will Accept
......................................

In Python 2 it was very easy to accidentally create an API that
accepted both bytes and textual data. But in Python 3, thanks to the
more strict handling of disparate types, this loose usage of bytes and
text together tends to fail.

  Take the dict `{b'a': 'bytes', u'a': 'text'}' in Python 2.6. It
creates the dict `{u'a': 'text'}' since `b'a' == u'a''. But in Python 3
the equivalent dict creates `{b'a': 'bytes', 'a': 'text'}', i.e., no
lost data. Similar issues can crop up when transitioning Python 2 code
to Python 3.

  This means you need to choose what an API is going to accept and
create and consistently stick to that API in both Python 2 and 3.


File: python.info,  Node: Bytes / Unicode Comparison,  Prev: Decide what APIs Will Accept,  Up: Deal With the Bytes/String Dichotomy

10.1.3.16 Bytes / Unicode Comparison
....................................

In Python 3, mixing bytes and unicode is forbidden in most situations;
it will raise a `TypeError' where Python 2 would have attempted an
implicit coercion between types.  However, there is one case where it
doesn't and it can be very misleading:

    >>> b"" == ""
    False

This is because an equality comparison is required by the language to
always succeed (and return `False' for incompatible types).  However,
this also means that code incorrectly ported to Python 3 can display
buggy behaviour if such comparisons are silently executed.  To detect
such situations, Python 3 has a `-b' flag that will display a warning:

    $ python3 -b
    >>> b"" == ""
    __main__:1: BytesWarning: Comparison between bytes and string
    False

To turn the warning into an exception, use the `-bb' flag instead:

    $ python3 -bb
    >>> b"" == ""
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    BytesWarning: Comparison between bytes and string



File: python.info,  Node: Indexing bytes objects,  Next: __str__ /__unicode__,  Prev: Deal With the Bytes/String Dichotomy,  Up: Handle Common "Gotchas"

10.1.3.17 Indexing bytes objects
................................

Another potentially surprising change is the indexing behaviour of bytes
objects in Python 3:

    >>> b"xyz"[0]
    120

Indeed, Python 3 bytes objects (as well as *note bytearray: 1f4.
objects) are sequences of integers.  But code converted from Python 2
will often assume that indexing a bytestring produces another
bytestring, not an integer.  To reconcile both behaviours, use slicing:

    >>> b"xyz"[0:1]
    b'x'
    >>> n = 1
    >>> b"xyz"[n:n+1]
    b'y'

The only remaining gotcha is that an out-of-bounds slice returns an
empty bytes object instead of raising `IndexError':

    >>> b"xyz"[3]
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    IndexError: index out of range
    >>> b"xyz"[3:4]
    b''



File: python.info,  Node: __str__ /__unicode__,  Next: Don't Index on Exceptions,  Prev: Indexing bytes objects,  Up: Handle Common "Gotchas"

10.1.3.18 `__str__()'/`__unicode__()'
.....................................

In Python 2, objects can specify both a string and unicode
representation of themselves. In Python 3, though, there is only a
string representation. This becomes an issue as people can
inadvertently do things in their `__str__()' methods which have
unpredictable results (e.g., infinite recursion if you happen to use
the `unicode(self).encode('utf8')' idiom as the body of your
`__str__()' method).

  There are two ways to solve this issue. One is to use a custom 2to3
fixer. The blog post at
<http://lucumr.pocoo.org/2011/1/22/forwards-compatible-python/>
specifies how to do this. That will allow 2to3 to change all instances
of `def __unicode(self): ...' to `def __str__(self): ...'. This does
require that you define your `__str__()' method in Python 2 before your
`__unicode__()' method.

  The other option is to use a mixin class. This allows you to only
define a `__unicode__()' method for your class and let the mixin derive
`__str__()' for you (code from
<http://lucumr.pocoo.org/2011/1/22/forwards-compatible-python/>):

    import sys

    class UnicodeMixin(object):

      """Mixin class to handle defining the proper __str__/__unicode__
      methods in Python 2 or 3."""

      if sys.version_info[0] >= 3: # Python 3
          def __str__(self):
              return self.__unicode__()
      else:  # Python 2
          def __str__(self):
              return self.__unicode__().encode('utf8')


    class Spam(UnicodeMixin):

      def __unicode__(self):
          return u'spam-spam-bacon-spam'  # 2to3 will remove the 'u' prefix



File: python.info,  Node: Don't Index on Exceptions,  Next: Don't use __getslice__ & Friends,  Prev: __str__ /__unicode__,  Up: Handle Common "Gotchas"

10.1.3.19 Don't Index on Exceptions
...................................

In Python 2, the following worked:

    >>> exc = Exception(1, 2, 3)
    >>> exc.args[1]
    2
    >>> exc[1]  # Python 2 only!
    2

But in Python 3, indexing directly on an exception is an error. You
need to make sure to only index on the `BaseException.args' attribute
which is a sequence containing all arguments passed to the *note
__init__(): 375. method.

  Even better is to use the documented attributes the exception
provides.


File: python.info,  Node: Don't use __getslice__ & Friends,  Next: Updating doctests,  Prev: Don't Index on Exceptions,  Up: Handle Common "Gotchas"

10.1.3.20 Don't use `__getslice__' & Friends
............................................

Been deprecated for a while, but Python 3 finally drops support for
`__getslice__()', etc. Move completely over to *note __getitem__():
448. and friends.


File: python.info,  Node: Updating doctests,  Prev: Don't use __getslice__ & Friends,  Up: Handle Common "Gotchas"

10.1.3.21 Updating doctests
...........................

2to3(1) will attempt to generate fixes for doctests that it comes
across. It's not perfect, though. If you wrote a monolithic set of
doctests (e.g., a single docstring containing all of your doctests),
you should at least consider breaking the doctests up into smaller
pieces to make it more manageable to fix.  Otherwise it might very well
be worth your time and effort to port your tests to *note unittest: 187.

  ---------- Footnotes ----------

  (1) http://docs.python.org/py3k/library/2to3.html


File: python.info,  Node: Eliminate -3 Warnings,  Next: Run 2to3,  Prev: Handle Common "Gotchas",  Up: Python 2 and 2to3

10.1.3.22 Eliminate `-3' Warnings
.................................

When you run your application's test suite, run it using the `-3' flag
passed to Python. This will cause various warnings to be raised during
execution about things that 2to3 cannot handle automatically (e.g.,
modules that have been removed). Try to eliminate those warnings to
make your code even more portable to Python 3.


File: python.info,  Node: Run 2to3,  Next: Verify & Test,  Prev: Eliminate -3 Warnings,  Up: Python 2 and 2to3

10.1.3.23 Run 2to3
..................

Once you have made your Python 2 code future-compatible with Python 3,
it's time to use 2to3(1) to actually port your code.

* Menu:

* Manually::
* During Installation::

  ---------- Footnotes ----------

  (1) http://docs.python.org/py3k/library/2to3.html


File: python.info,  Node: Manually,  Next: During Installation,  Up: Run 2to3

10.1.3.24 Manually
..................

To manually convert source code using 2to3(1), you use the `2to3'
script that is installed with Python 2.6 and later.:

    2to3 <directory or file to convert>

This will cause 2to3 to write out a diff with all of the fixers applied
for the converted source code. If you would like 2to3 to go ahead and
apply the changes you can pass it the `-w' flag:

    2to3 -w <stuff to convert>

There are other flags available to control exactly which fixers are
applied, etc.

  ---------- Footnotes ----------

  (1) http://docs.python.org/py3k/library/2to3.html


File: python.info,  Node: During Installation,  Prev: Manually,  Up: Run 2to3

10.1.3.25 During Installation
.............................

When a user installs your project for Python 3, you can have either
*note distutils: 85. or Distribute(1) run 2to3(2) on your behalf.  For
distutils, use the following idiom:

    try:  # Python 3
      from distutils.command.build_py import build_py_2to3 as build_py
    except ImportError:  # Python 2
      from distutils.command.build_py import build_py

    setup(cmdclass = {'build_py': build_py},
      # ...
    )

For Distribute:

    setup(use_2to3=True,
      # ...
    )

This will allow you to not have to distribute a separate Python 3
version of your project. It does require, though, that when you perform
development that you at least build your project and use the built
Python 3 source for testing.

  ---------- Footnotes ----------

  (1) http://packages.python.org/distribute/

  (2) http://docs.python.org/py3k/library/2to3.html


File: python.info,  Node: Verify & Test,  Prev: Run 2to3,  Up: Python 2 and 2to3

10.1.3.26 Verify & Test
.......................

At this point you should (hopefully) have your project converted in
such a way that it works in Python 3. Verify it by running your unit
tests and making sure nothing has gone awry. If you miss something then
figure out how to fix it in Python 3, backport to your Python 2 code,
and run your code through 2to3 again to verify the fix transforms
properly.


File: python.info,  Node: Python 2/3 Compatible Source,  Next: Other Resources<2>,  Prev: Python 2 and 2to3,  Up: Porting Python 2 Code to Python 3

10.1.4 Python 2/3 Compatible Source
-----------------------------------

While it may seem counter-intuitive, you can write Python code which is
source-compatible between Python 2 & 3. It does lead to code that is not
entirely idiomatic Python (e.g., having to extract the currently raised
exception from `sys.exc_info()[1]'), but it can be run under Python 2
*and* Python 3 without using 2to3(1) as a translation step (although
the tool should be used to help find potential portability problems).
This allows you to continue to have a rapid development process
regardless of whether you are developing under Python 2 or Python 3.
Whether this approach or using *note Python 2 and 2to3: 2f89. works
best for you will be a per-project decision.

  To get a complete idea of what issues you will need to deal with, see
the What's New in Python 3.0(2). Others have reorganized the data in
other formats such as
<http://docs.pythonsprints.com/python3_porting/py-porting.html> .

  The following are some steps to take to try to support both Python 2
& 3 from the same source code.

* Menu:

* Follow The Steps for Using 2to3::
* Use six::
* Capturing the Currently Raised Exception::

  ---------- Footnotes ----------

  (1) http://docs.python.org/py3k/library/2to3.html

  (2) http://docs.python.org/release/3.0/whatsnew/3.0.html


File: python.info,  Node: Follow The Steps for Using 2to3,  Next: Use six,  Up: Python 2/3 Compatible Source

10.1.4.1 Follow The Steps for Using 2to3
........................................

All of the steps outlined in how to *note port Python 2 code with 2to3:
2f89. apply to creating a Python 2/3 codebase. This includes trying
only support Python 2.6 or newer (the *note __future__: 1. statements
work in Python 3 without issue), eliminating warnings that are
triggered by `-3', etc.

  You should even consider running 2to3(1) over your code (without
committing the changes). This will let you know where potential pain
points are within your code so that you can fix them properly before
they become an issue.

  ---------- Footnotes ----------

  (1) http://docs.python.org/py3k/library/2to3.html


File: python.info,  Node: Use six,  Next: Capturing the Currently Raised Exception,  Prev: Follow The Steps for Using 2to3,  Up: Python 2/3 Compatible Source

10.1.4.2 Use six
................

The six(1) project contains many things to help you write portable
Python code.  You should make sure to read its documentation from
beginning to end and use any and all features it provides. That way you
will minimize any mistakes you might make in writing cross-version code.

  ---------- Footnotes ----------

  (1) http://packages.python.org/six


File: python.info,  Node: Capturing the Currently Raised Exception,  Prev: Use six,  Up: Python 2/3 Compatible Source

10.1.4.3 Capturing the Currently Raised Exception
.................................................

One change between Python 2 and 3 that will require changing how you
code (if you support Python 2.5(1) and earlier) is accessing the
currently raised exception.  In Python 2.5 and earlier the syntax to
access the current exception is:

    try:
      raise Exception()
    except Exception, exc:
      # Current exception is 'exc'
      pass

This syntax changed in Python 3 (and backported to Python 2.6(2) and
later) to:

    try:
      raise Exception()
    except Exception as exc:
      # Current exception is 'exc'
      # In Python 3, 'exc' is restricted to the block; Python 2.6 will "leak"
      pass

Because of this syntax change you must change to capturing the current
exception to:

    try:
      raise Exception()
    except Exception:
      import sys
      exc = sys.exc_info()[1]
      # Current exception is 'exc'
      pass

You can get more information about the raised exception from *note
sys.exc_info(): 2ec. than simply the current exception instance, but
you most likely don't need it.

     Note: In Python 3, the traceback is attached to the exception
     instance through the `__traceback__' attribute. If the instance is
     saved in a local variable that persists outside of the `except'
     block, the traceback will create a reference cycle with the
     current frame and its dictionary of local variables.  This will
     delay reclaiming dead resources until the next cyclic *note
     garbage collection: 5fc. pass.

     In Python 2, this problem only occurs if you save the traceback
     itself (e.g. the third element of the tuple returned by *note
     sys.exc_info(): 2ec.)  in a variable.

  ---------- Footnotes ----------

  (1) http://www.python.org/2.5.x

  (2) http://www.python.org/2.6.x


File: python.info,  Node: Other Resources<2>,  Prev: Python 2/3 Compatible Source,  Up: Porting Python 2 Code to Python 3

10.1.5 Other Resources
----------------------

The authors of the following blog posts, wiki pages, and books deserve
special thanks for making public their tips for porting Python 2 code
to Python 3 (and thus helping provide information for this document):

   * <http://python3porting.com/>

   * <http://docs.pythonsprints.com/python3_porting/py-porting.html>

   *
     <http://techspot.zzzeek.org/2011/01/24/zzzeek-s-guide-to-python-3-porting/>

   *
     <http://dabeaz.blogspot.com/2011/01/porting-py65-and-my-superboard-to.html>

   * <http://lucumr.pocoo.org/2011/1/22/forwards-compatible-python/>

   * <http://lucumr.pocoo.org/2010/2/11/porting-to-python-3-a-guide/>

   * <http://wiki.python.org/moin/PortingPythonToPy3k>

  If you feel there is something missing from this document that should
be added, please email the python-porting(1) mailing list.

  ---------- Footnotes ----------

  (1) http://mail.python.org/mailman/listinfo/python-porting


File: python.info,  Node: Porting Extension Modules to Python 3,  Next: Curses Programming with Python,  Prev: Porting Python 2 Code to Python 3,  Up: Python HOWTOs

10.2 Porting Extension Modules to Python 3
==========================================

     author: Benjamin Peterson

Abstract
........

Although changing the C-API was not one of Python 3's objectives, the
many Python-level changes made leaving Python 2's API intact
impossible.  In fact, some changes such as *note int(): 1ef. and *note
long(): 1f0. unification are more obvious on the C level.  This
document endeavors to document incompatibilities and how they can be
worked around.

* Menu:

* Conditional compilation::
* Changes to Object APIs::
* Module initialization and state::
* CObject replaced with Capsule::
* Other options: Other options<2>.


File: python.info,  Node: Conditional compilation,  Next: Changes to Object APIs,  Up: Porting Extension Modules to Python 3

10.2.1 Conditional compilation
------------------------------

The easiest way to compile only some code for Python 3 is to check if
`PY_MAJOR_VERSION' is greater than or equal to 3.

    #if PY_MAJOR_VERSION >= 3
    #define IS_PY3K
    #endif

API functions that are not present can be aliased to their equivalents
within conditional blocks.


File: python.info,  Node: Changes to Object APIs,  Next: Module initialization and state,  Prev: Conditional compilation,  Up: Porting Extension Modules to Python 3

10.2.2 Changes to Object APIs
-----------------------------

Python 3 merged together some types with similar functions while cleanly
separating others.

* Menu:

* str/unicode Unification::
* long/int Unification::


File: python.info,  Node: str/unicode Unification,  Next: long/int Unification,  Up: Changes to Object APIs

10.2.2.1 str/unicode Unification
................................

Python 3's *note str(): 1e7. (`PyString_*' functions in C) type is
equivalent to Python 2's *note unicode(): 1f2. (`PyUnicode_*').  The
old 8-bit string type has become `bytes()'.  Python 2.6 and later
provide a compatibility header, `bytesobject.h', mapping `PyBytes'
names to `PyString' ones.  For best compatibility with Python 3,
`PyUnicode' should be used for textual data and `PyBytes' for binary
data.  It's also important to remember that `PyBytes' and `PyUnicode'
in Python 3 are not interchangeable like `PyString' and `PyUnicode' are
in Python 2.  The following example shows best practices with regards
to `PyUnicode', `PyString', and `PyBytes'.

    #include "stdlib.h"
    #include "Python.h"
    #include "bytesobject.h"

    /* text example */
    static PyObject *
    say_hello(PyObject *self, PyObject *args) {
        PyObject *name, *result;

        if (!PyArg_ParseTuple(args, "U:say_hello", &name))
            return NULL;

        result = PyUnicode_FromFormat("Hello, %S!", name);
        return result;
    }

    /* just a forward */
    static char * do_encode(PyObject *);

    /* bytes example */
    static PyObject *
    encode_object(PyObject *self, PyObject *args) {
        char *encoded;
        PyObject *result, *myobj;

        if (!PyArg_ParseTuple(args, "O:encode_object", &myobj))
            return NULL;

        encoded = do_encode(myobj);
        if (encoded == NULL)
            return NULL;
        result = PyBytes_FromString(encoded);
        free(encoded);
        return result;
    }



File: python.info,  Node: long/int Unification,  Prev: str/unicode Unification,  Up: Changes to Object APIs

10.2.2.2 long/int Unification
.............................

Python 3 has only one integer type, *note int(): 1ef.  But it actually
corresponds to Python 2's *note long(): 1f0. type-the *note int(): 1ef.
type used in Python 2 was removed.  In the C-API, `PyInt_*' functions
are replaced by their `PyLong_*' equivalents.


File: python.info,  Node: Module initialization and state,  Next: CObject replaced with Capsule,  Prev: Changes to Object APIs,  Up: Porting Extension Modules to Python 3

10.2.3 Module initialization and state
--------------------------------------

Python 3 has a revamped extension module initialization system.  (See PEP
3121(1).)  Instead of storing module state in globals, they should be
stored in an interpreter specific structure.  Creating modules that act
correctly in both Python 2 and Python 3 is tricky.  The following
simple example demonstrates how.

    #include "Python.h"

    struct module_state {
        PyObject *error;
    };

    #if PY_MAJOR_VERSION >= 3
    #define GETSTATE(m) ((struct module_state*)PyModule_GetState(m))
    #else
    #define GETSTATE(m) (&_state)
    static struct module_state _state;
    #endif

    static PyObject *
    error_out(PyObject *m) {
        struct module_state *st = GETSTATE(m);
        PyErr_SetString(st->error, "something bad happened");
        return NULL;
    }

    static PyMethodDef myextension_methods[] = {
        {"error_out", (PyCFunction)error_out, METH_NOARGS, NULL},
        {NULL, NULL}
    };

    #if PY_MAJOR_VERSION >= 3

    static int myextension_traverse(PyObject *m, visitproc visit, void *arg) {
        Py_VISIT(GETSTATE(m)->error);
        return 0;
    }

    static int myextension_clear(PyObject *m) {
        Py_CLEAR(GETSTATE(m)->error);
        return 0;
    }


    static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "myextension",
            NULL,
            sizeof(struct module_state),
            myextension_methods,
            NULL,
            myextension_traverse,
            myextension_clear,
            NULL
    };

    #define INITERROR return NULL

    PyObject *
    PyInit_myextension(void)

    #else
    #define INITERROR return

    void
    initmyextension(void)
    #endif
    {
    #if PY_MAJOR_VERSION >= 3
        PyObject *module = PyModule_Create(&moduledef);
    #else
        PyObject *module = Py_InitModule("myextension", myextension_methods);
    #endif

        if (module == NULL)
            INITERROR;
        struct module_state *st = GETSTATE(module);

        st->error = PyErr_NewException("myextension.Error", NULL, NULL);
        if (st->error == NULL) {
            Py_DECREF(module);
            INITERROR;
        }

    #if PY_MAJOR_VERSION >= 3
        return module;
    #endif
    }


  ---------- Footnotes ----------

  (1) http://www.python.org/dev/peps/pep-3121


File: python.info,  Node: CObject replaced with Capsule,  Next: Other options<2>,  Prev: Module initialization and state,  Up: Porting Extension Modules to Python 3

10.2.4 CObject replaced with Capsule
------------------------------------

The `Capsule' object was introduced in Python 3.1 and 2.7 to replace
`CObject'.  CObjects were useful, but the `CObject' API was
problematic: it didn't permit distinguishing between valid CObjects,
which allowed mismatched CObjects to crash the interpreter, and some of
its APIs relied on undefined behavior in C.  (For further reading on
the rationale behind Capsules, please see issue 5630(1).)

  If you're currently using CObjects, and you want to migrate to 3.1 or
newer, you'll need to switch to Capsules.  `CObject' was deprecated in
3.1 and 2.7 and completely removed in Python 3.2.  If you only support
2.7, or 3.1 and above, you can simply switch to `Capsule'.  If you need
to support Python 3.0, or versions of Python earlier than 2.7, you'll
have to support both CObjects and Capsules.  (Note that Python 3.0 is
no longer supported, and it is not recommended for production use.)

  The following example header file `capsulethunk.h' may solve the
problem for you.  Simply write your code against the `Capsule' API and
include this header file after `Python.h'.  Your code will
automatically use Capsules in versions of Python with Capsules, and
switch to CObjects when Capsules are unavailable.

  `capsulethunk.h' simulates Capsules using CObjects.  However,
`CObject' provides no place to store the capsule's "name".  As a result
the simulated `Capsule' objects created by `capsulethunk.h' behave
slightly differently from real Capsules.  Specifically:

        * The name parameter passed in to *note PyCapsule_New(): 29b1.
          is ignored.

        * The name parameter passed in to *note PyCapsule_IsValid():
          2c5. and *note PyCapsule_GetPointer(): 2d3a. is ignored, and
          no error checking of the name is performed.

        * *note PyCapsule_GetName(): 2d3d. always returns NULL.

        * *note PyCapsule_SetName(): 2d40. always raises an exception
          and returns failure.  (Since there's no way to store a name
          in a CObject, noisy failure of *note PyCapsule_SetName():
          2d40.  was deemed preferable to silent failure here.  If this
          is inconvenient, feel free to modify your local copy as you
          see fit.)

  You can find `capsulethunk.h' in the Python source distribution as
Doc/includes/capsulethunk.h(2).  We also include it here for your
convenience:

    #ifndef __CAPSULETHUNK_H
    #define __CAPSULETHUNK_H

    #if (    (PY_VERSION_HEX <  0x02070000) \
         || ((PY_VERSION_HEX >= 0x03000000) \
          && (PY_VERSION_HEX <  0x03010000)) )

    #define __PyCapsule_GetField(capsule, field, default_value) \
        ( PyCapsule_CheckExact(capsule) \
            ? (((PyCObject *)capsule)->field) \
            : (default_value) \
        ) \

    #define __PyCapsule_SetField(capsule, field, value) \
        ( PyCapsule_CheckExact(capsule) \
            ? (((PyCObject *)capsule)->field = value), 1 \
            : 0 \
        ) \


    #define PyCapsule_Type PyCObject_Type

    #define PyCapsule_CheckExact(capsule) (PyCObject_Check(capsule))
    #define PyCapsule_IsValid(capsule, name) (PyCObject_Check(capsule))


    #define PyCapsule_New(pointer, name, destructor) \
        (PyCObject_FromVoidPtr(pointer, destructor))


    #define PyCapsule_GetPointer(capsule, name) \
        (PyCObject_AsVoidPtr(capsule))

    /* Don't call PyCObject_SetPointer here, it fails if there's a destructor */
    #define PyCapsule_SetPointer(capsule, pointer) \
        __PyCapsule_SetField(capsule, cobject, pointer)


    #define PyCapsule_GetDestructor(capsule) \
        __PyCapsule_GetField(capsule, destructor)

    #define PyCapsule_SetDestructor(capsule, dtor) \
        __PyCapsule_SetField(capsule, destructor, dtor)


    /*
     * Sorry, there's simply no place
     * to store a Capsule "name" in a CObject.
     */
    #define PyCapsule_GetName(capsule) NULL

    static int
    PyCapsule_SetName(PyObject *capsule, const char *unused)
    {
        unused = unused;
        PyErr_SetString(PyExc_NotImplementedError,
            "can't use PyCapsule_SetName with CObjects");
        return 1;
    }



    #define PyCapsule_GetContext(capsule) \
        __PyCapsule_GetField(capsule, descr)

    #define PyCapsule_SetContext(capsule, context) \
        __PyCapsule_SetField(capsule, descr, context)


    static void *
    PyCapsule_Import(const char *name, int no_block)
    {
        PyObject *object = NULL;
        void *return_value = NULL;
        char *trace;
        size_t name_length = (strlen(name) + 1) * sizeof(char);
        char *name_dup = (char *)PyMem_MALLOC(name_length);

        if (!name_dup) {
            return NULL;
        }

        memcpy(name_dup, name, name_length);

        trace = name_dup;
        while (trace) {
            char *dot = strchr(trace, '.');
            if (dot) {
                *dot++ = '\0';
            }

            if (object == NULL) {
                if (no_block) {
                    object = PyImport_ImportModuleNoBlock(trace);
                } else {
                    object = PyImport_ImportModule(trace);
                    if (!object) {
                        PyErr_Format(PyExc_ImportError,
                            "PyCapsule_Import could not "
                            "import module \"%s\"", trace);
                    }
                }
            } else {
                PyObject *object2 = PyObject_GetAttrString(object, trace);
                Py_DECREF(object);
                object = object2;
            }
            if (!object) {
                goto EXIT;
            }

            trace = dot;
        }

        if (PyCObject_Check(object)) {
            PyCObject *cobject = (PyCObject *)object;
            return_value = cobject->cobject;
        } else {
            PyErr_Format(PyExc_AttributeError,
                "PyCapsule_Import \"%s\" is not valid",
                name);
        }

    EXIT:
        Py_XDECREF(object);
        if (name_dup) {
            PyMem_FREE(name_dup);
        }
        return return_value;
    }

    #endif /* #if PY_VERSION_HEX < 0x02070000 */

    #endif /* __CAPSULETHUNK_H */


  ---------- Footnotes ----------

  (1) http://bugs.python.org/issue5630

  (2) http://hg.python.org/cpython/file/2.7/Doc/includes/capsulethunk.h


File: python.info,  Node: Other options<2>,  Prev: CObject replaced with Capsule,  Up: Porting Extension Modules to Python 3

10.2.5 Other options
--------------------

If you are writing a new extension module, you might consider
Cython(1).  It translates a Python-like language to C.  The extension
modules it creates are compatible with Python 3 and Python 2.

  ---------- Footnotes ----------

  (1) http://www.cython.org


File: python.info,  Node: Curses Programming with Python,  Next: Descriptor HowTo Guide,  Prev: Porting Extension Modules to Python 3,  Up: Python HOWTOs

10.3 Curses Programming with Python
===================================

     Author: A.M. Kuchling, Eric S. Raymond

     Release: 2.03

Abstract
........

This document describes how to write text-mode programs with Python
2.x, using the *note curses: 79. extension module to control the
display.

* Menu:

* What is curses?::
* Starting and ending a curses application::
* Windows and Pads::
* Displaying Text::
* User Input::
* For More Information::

What is curses?

* The Python curses module::

Displaying Text

* Attributes and Color::


File: python.info,  Node: What is curses?,  Next: Starting and ending a curses application,  Up: Curses Programming with Python

10.3.1 What is curses?
----------------------

The curses library supplies a terminal-independent screen-painting and
keyboard-handling facility for text-based terminals; such terminals
include VT100s, the Linux console, and the simulated terminal provided
by X11 programs such as xterm and rxvt.  Display terminals support
various control codes to perform common operations such as moving the
cursor, scrolling the screen, and erasing areas.  Different terminals
use widely differing codes, and often have their own minor quirks.

  In a world of X displays, one might ask "why bother"?  It's true that
character-cell display terminals are an obsolete technology, but there
are niches in which being able to do fancy things with them are still
valuable.  One is on small-footprint or embedded Unixes that don't
carry an X server.  Another is for tools like OS installers and kernel
configurators that may have to run before X is available.

  The curses library hides all the details of different terminals, and
provides the programmer with an abstraction of a display, containing
multiple non-overlapping windows.  The contents of a window can be
changed in various ways- adding text, erasing it, changing its
appearance-and the curses library will automagically figure out what
control codes need to be sent to the terminal to produce the right
output.

  The curses library was originally written for BSD Unix; the later
System V versions of Unix from AT&T added many enhancements and new
functions. BSD curses is no longer maintained, having been replaced by
ncurses, which is an open-source implementation of the AT&T interface.
If you're using an open-source Unix such as Linux or FreeBSD, your
system almost certainly uses ncurses.  Since most current commercial
Unix versions are based on System V code, all the functions described
here will probably be available.  The older versions of curses carried
by some proprietary Unixes may not support everything, though.

  No one has made a Windows port of the curses module.  On a Windows
platform, try the Console module written by Fredrik Lundh.  The Console
module provides cursor-addressable text output, plus full support for
mouse and keyboard input, and is available from
<http://effbot.org/zone/console-index.htm>.

* Menu:

* The Python curses module::


File: python.info,  Node: The Python curses module,  Up: What is curses?

10.3.1.1 The Python curses module
.................................

Thy Python module is a fairly simple wrapper over the C functions
provided by curses; if you're already familiar with curses programming
in C, it's really easy to transfer that knowledge to Python.  The
biggest difference is that the Python interface makes things simpler,
by merging different C functions such as `addstr()', `mvaddstr()',
`mvwaddstr()', into a single `addstr()' method.  You'll see this
covered in more detail later.

  This HOWTO is simply an introduction to writing text-mode programs
with curses and Python. It doesn't attempt to be a complete guide to
the curses API; for that, see the Python library guide's section on
ncurses, and the C manual pages for ncurses.  It will, however, give
you the basic ideas.


File: python.info,  Node: Starting and ending a curses application,  Next: Windows and Pads,  Prev: What is curses?,  Up: Curses Programming with Python

10.3.2 Starting and ending a curses application
-----------------------------------------------

Before doing anything, curses must be initialized.  This is done by
calling the `initscr()' function, which will determine the terminal
type, send any required setup codes to the terminal, and create various
internal data structures.  If successful, `initscr()' returns a window
object representing the entire screen; this is usually called `stdscr',
after the name of the corresponding C variable.

    import curses
    stdscr = curses.initscr()

Usually curses applications turn off automatic echoing of keys to the
screen, in order to be able to read keys and only display them under
certain circumstances.  This requires calling the `noecho()' function.

    curses.noecho()

Applications will also commonly need to react to keys instantly, without
requiring the Enter key to be pressed; this is called cbreak mode, as
opposed to the usual buffered input mode.

    curses.cbreak()

Terminals usually return special keys, such as the cursor keys or
navigation keys such as Page Up and Home, as a multibyte escape
sequence.  While you could write your application to expect such
sequences and process them accordingly, curses can do it for you,
returning a special value such as `curses.KEY_LEFT'.  To get curses to
do the job, you'll have to enable keypad mode.

    stdscr.keypad(1)

Terminating a curses application is much easier than starting one.
You'll need to call

    curses.nocbreak(); stdscr.keypad(0); curses.echo()

to reverse the curses-friendly terminal settings. Then call the
`endwin()' function to restore the terminal to its original operating
mode.

    curses.endwin()

A common problem when debugging a curses application is to get your
terminal messed up when the application dies without restoring the
terminal to its previous state.  In Python this commonly happens when
your code is buggy and raises an uncaught exception.  Keys are no
longer echoed to the screen when you type them, for example, which
makes using the shell difficult.

  In Python you can avoid these complications and make debugging much
easier by importing the module *note curses.wrapper: 13ac.  It supplies
a `wrapper()' function that takes a callable.  It does the
initializations described above, and also initializes colors if color
support is present.  It then runs your provided callable and finally
deinitializes appropriately.  The callable is called inside a try-catch
clause which catches exceptions, performs curses deinitialization, and
then passes the exception upwards.  Thus, your terminal won't be left
in a funny state on exception.


File: python.info,  Node: Windows and Pads,  Next: Displaying Text,  Prev: Starting and ending a curses application,  Up: Curses Programming with Python

10.3.3 Windows and Pads
-----------------------

Windows are the basic abstraction in curses.  A window object
represents a rectangular area of the screen, and supports various
methods to display text, erase it, allow the user to input strings, and
so forth.

  The `stdscr' object returned by the `initscr()' function is a window
object that covers the entire screen.  Many programs may need only this
single window, but you might wish to divide the screen into smaller
windows, in order to redraw or clear them separately. The `newwin()'
function creates a new window of a given size, returning the new window
object.

    begin_x = 20 ; begin_y = 7
    height = 5 ; width = 40
    win = curses.newwin(height, width, begin_y, begin_x)

A word about the coordinate system used in curses: coordinates are
always passed in the order _y,x_, and the top-left corner of a window
is coordinate (0,0).  This breaks a common convention for handling
coordinates, where the _x_ coordinate usually comes first.  This is an
unfortunate difference from most other computer applications, but it's
been part of curses since it was first written, and it's too late to
change things now.

  When you call a method to display or erase text, the effect doesn't
immediately show up on the display.  This is because curses was
originally written with slow 300-baud terminal connections in mind;
with these terminals, minimizing the time required to redraw the screen
is very important.  This lets curses accumulate changes to the screen,
and display them in the most efficient manner.  For example, if your
program displays some characters in a window, and then clears the
window, there's no need to send the original characters because they'd
never be visible.

  Accordingly, curses requires that you explicitly tell it to redraw
windows, using the `refresh()' method of window objects.  In practice,
this doesn't really complicate programming with curses much. Most
programs go into a flurry of activity, and then pause waiting for a
keypress or some other action on the part of the user.  All you have to
do is to be sure that the screen has been redrawn before pausing to
wait for user input, by simply calling `stdscr.refresh()' or the
`refresh()' method of some other relevant window.

  A pad is a special case of a window; it can be larger than the actual
display screen, and only a portion of it displayed at a time. Creating
a pad simply requires the pad's height and width, while refreshing a
pad requires giving the coordinates of the on-screen area where a
subsection of the pad will be displayed.

    pad = curses.newpad(100, 100)
    #  These loops fill the pad with letters; this is
    # explained in the next section
    for y in range(0, 100):
        for x in range(0, 100):
            try: pad.addch(y,x, ord('a') + (x*x+y*y) % 26 )
            except curses.error: pass

    #  Displays a section of the pad in the middle of the screen
    pad.refresh( 0,0, 5,5, 20,75)

The `refresh()' call displays a section of the pad in the rectangle
extending from coordinate (5,5) to coordinate (20,75) on the screen;
the upper left corner of the displayed section is coordinate (0,0) on
the pad.  Beyond that difference, pads are exactly like ordinary
windows and support the same methods.

  If you have multiple windows and pads on screen there is a more
efficient way to go, which will prevent annoying screen flicker at
refresh time.  Use the `noutrefresh()' method of each window to update
the data structure representing the desired state of the screen; then
change the physical screen to match the desired state in one go with
the function `doupdate()'.  The normal `refresh()' method calls
`doupdate()' as its last act.


File: python.info,  Node: Displaying Text,  Next: User Input,  Prev: Windows and Pads,  Up: Curses Programming with Python

10.3.4 Displaying Text
----------------------

From a C programmer's point of view, curses may sometimes look like a
twisty maze of functions, all subtly different.  For example,
`addstr()' displays a string at the current cursor location in the
`stdscr' window, while `mvaddstr()' moves to a given y,x coordinate
first before displaying the string. `waddstr()' is just like
`addstr()', but allows specifying a window to use, instead of using
`stdscr' by default. `mvwaddstr()' follows similarly.

  Fortunately the Python interface hides all these details; `stdscr' is
a window object like any other, and methods like `addstr()' accept
multiple argument forms.  Usually there are four different forms.

Form                                  Description
------------------------------------------------------------------------------------------ 
_str_ or _ch_                         Display the string _str_ or character _ch_ at the
                                      current position
_str_ or _ch_, _attr_                 Display the string _str_ or character _ch_, using
                                      attribute _attr_ at the current position
_y_, _x_, _str_ or _ch_               Move to position _y,x_ within the window, and
                                      display _str_ or _ch_
_y_, _x_, _str_ or _ch_, _attr_       Move to position _y,x_ within the window, and
                                      display _str_ or _ch_, using attribute _attr_

  Attributes allow displaying text in highlighted forms, such as in
boldface, underline, reverse code, or in color.  They'll be explained
in more detail in the next subsection.

  The `addstr()' function takes a Python string as the value to be
displayed, while the `addch()' functions take a character, which can be
either a Python string of length 1 or an integer.  If it's a string,
you're limited to displaying characters between 0 and 255.  SVr4 curses
provides constants for extension characters; these constants are
integers greater than 255.  For example, `ACS_PLMINUS' is a +/- symbol,
and `ACS_ULCORNER' is the upper left corner of a box (handy for drawing
borders).

  Windows remember where the cursor was left after the last operation,
so if you leave out the _y,x_ coordinates, the string or character will
be displayed wherever the last operation left off.  You can also move
the cursor with the `move(y,x)' method.  Because some terminals always
display a flashing cursor, you may want to ensure that the cursor is
positioned in some location where it won't be distracting; it can be
confusing to have the cursor blinking at some apparently random
location.

  If your application doesn't need a blinking cursor at all, you can
call `curs_set(0)' to make it invisible.  Equivalently, and for
compatibility with older curses versions, there's a `leaveok(bool)'
function.  When _bool_ is true, the curses library will attempt to
suppress the flashing cursor, and you won't need to worry about leaving
it in odd locations.

* Menu:

* Attributes and Color::


File: python.info,  Node: Attributes and Color,  Up: Displaying Text

10.3.4.1 Attributes and Color
.............................

Characters can be displayed in different ways.  Status lines in a
text-based application are commonly shown in reverse video; a text
viewer may need to highlight certain words.  curses supports this by
allowing you to specify an attribute for each cell on the screen.

  An attribute is an integer, each bit representing a different
attribute.  You can try to display text with multiple attribute bits
set, but curses doesn't guarantee that all the possible combinations
are available, or that they're all visually distinct.  That depends on
the ability of the terminal being used, so it's safest to stick to the
most commonly available attributes, listed here.

Attribute                  Description
---------------------------------------------------------------------- 
`A_BLINK'                  Blinking text
`A_BOLD'                   Extra bright or bold text
`A_DIM'                    Half bright text
`A_REVERSE'                Reverse-video text
`A_STANDOUT'               The best highlighting mode available
`A_UNDERLINE'              Underlined text

  So, to display a reverse-video status line on the top line of the
screen, you could code:

    stdscr.addstr(0, 0, "Current mode: Typing mode",
                  curses.A_REVERSE)
    stdscr.refresh()

The curses library also supports color on those terminals that provide
it. The most common such terminal is probably the Linux console,
followed by color xterms.

  To use color, you must call the `start_color()' function soon after
calling `initscr()', to initialize the default color set (the
`curses.wrapper.wrapper()' function does this automatically).  Once
that's done, the `has_colors()' function returns TRUE if the terminal
in use can actually display color.  (Note: curses uses the American
spelling 'color', instead of the Canadian/British spelling 'colour'.
If you're used to the British spelling, you'll have to resign yourself
to misspelling it for the sake of these functions.)

  The curses library maintains a finite number of color pairs,
containing a foreground (or text) color and a background color.  You
can get the attribute value corresponding to a color pair with the
`color_pair()' function; this can be bitwise-OR'ed with other
attributes such as `A_REVERSE', but again, such combinations are not
guaranteed to work on all terminals.

  An example, which displays a line of text using color pair 1:

    stdscr.addstr( "Pretty text", curses.color_pair(1) )
    stdscr.refresh()

As I said before, a color pair consists of a foreground and background
color.  `start_color()' initializes 8 basic colors when it activates
color mode.  They are: 0:black, 1:red, 2:green, 3:yellow, 4:blue,
5:magenta, 6:cyan, and 7:white.  The curses module defines named
constants for each of these colors: `curses.COLOR_BLACK',
`curses.COLOR_RED', and so forth.

  The `init_pair(n, f, b)' function changes the definition of color
pair _n_, to foreground color f and background color b.  Color pair 0
is hard-wired to white on black, and cannot be changed.

  Let's put all this together. To change color 1 to red text on a white
background, you would call:

    curses.init_pair(1, curses.COLOR_RED, curses.COLOR_WHITE)

When you change a color pair, any text already displayed using that
color pair will change to the new colors.  You can also display new
text in this color with:

    stdscr.addstr(0,0, "RED ALERT!", curses.color_pair(1) )

Very fancy terminals can change the definitions of the actual colors to
a given RGB value.  This lets you change color 1, which is usually red,
to purple or blue or any other color you like.  Unfortunately, the
Linux console doesn't support this, so I'm unable to try it out, and
can't provide any examples.  You can check if your terminal can do this
by calling `can_change_color()', which returns TRUE if the capability
is there.  If you're lucky enough to have such a talented terminal,
consult your system's man pages for more information.


File: python.info,  Node: User Input,  Next: For More Information,  Prev: Displaying Text,  Up: Curses Programming with Python

10.3.5 User Input
-----------------

The curses library itself offers only very simple input mechanisms.
Python's support adds a text-input widget that makes up some of the
lack.

  The most common way to get input to a window is to use its `getch()'
method.  `getch()' pauses and waits for the user to hit a key,
displaying it if `echo()' has been called earlier.  You can optionally
specify a coordinate to which the cursor should be moved before pausing.

  It's possible to change this behavior with the method `nodelay()'.
After `nodelay(1)', `getch()' for the window becomes non-blocking and
returns `curses.ERR' (a value of -1) when no input is ready.  There's
also a `halfdelay()' function, which can be used to (in effect) set a
timer on each `getch()'; if no input becomes available within a
specified delay (measured in tenths of a second), curses raises an
exception.

  The `getch()' method returns an integer; if it's between 0 and 255, it
represents the ASCII code of the key pressed.  Values greater than 255
are special keys such as Page Up, Home, or the cursor keys. You can
compare the value returned to constants such as `curses.KEY_PPAGE',
`curses.KEY_HOME', or `curses.KEY_LEFT'.  Usually the main loop of your
program will look something like this:

    while 1:
        c = stdscr.getch()
        if c == ord('p'): PrintDocument()
        elif c == ord('q'): break  # Exit the while()
        elif c == curses.KEY_HOME: x = y = 0

The *note curses.ascii: 7a. module supplies ASCII class membership
functions that take either integer or 1-character-string arguments;
these may be useful in writing more readable tests for your command
interpreters.  It also supplies conversion functions  that take either
integer or 1-character-string arguments and return the same type.  For
example, *note curses.ascii.ctrl(): 1417. returns the control character
corresponding to its argument.

  There's also a method to retrieve an entire string, `getstr()'.  It
isn't used very often, because its functionality is quite limited; the
only editing keys available are the backspace key and the Enter key,
which terminates the string.  It can optionally be limited to a fixed
number of characters.

    curses.echo()            # Enable echoing of characters

    # Get a 15-character string, with the cursor on the top line
    s = stdscr.getstr(0,0, 15)

The Python *note curses.textpad: 7c. module supplies something better.
With it, you can turn a window into a text box that supports an
Emacs-like set of keybindings.  Various methods of `Textbox' class
support editing with input validation and gathering the edit results
either with or without trailing spaces.   See the library documentation
on *note curses.textpad: 7c. for the details.


File: python.info,  Node: For More Information,  Prev: User Input,  Up: Curses Programming with Python

10.3.6 For More Information
---------------------------

This HOWTO didn't cover some advanced topics, such as screen-scraping or
capturing mouse events from an xterm instance.  But the Python library
page for the curses modules is now pretty complete.  You should browse
it next.

  If you're in doubt about the detailed behavior of any of the ncurses
entry points, consult the manual pages for your curses implementation,
whether it's ncurses or a proprietary Unix vendor's.  The manual pages
will document any quirks, and provide complete lists of all the
functions, attributes, and `ACS_*' characters available to you.

  Because the curses API is so large, some functions aren't supported
in the Python interface, not because they're difficult to implement,
but because no one has needed them yet.  Feel free to add them and then
submit a patch.  Also, we don't yet have support for the menu library
associated with ncurses; feel free to add that.

  If you write an interesting little program, feel free to contribute
it as another demo.  We can always use more of them!

  The ncurses FAQ:
<http://invisible-island.net/ncurses/ncurses.faq.html>


File: python.info,  Node: Descriptor HowTo Guide,  Next: Idioms and Anti-Idioms in Python,  Prev: Curses Programming with Python,  Up: Python HOWTOs

10.4 Descriptor HowTo Guide
===========================

     Author: Raymond Hettinger

     Contact: <python at rcn dot com>

* Menu:

* Abstract::
* Definition and Introduction::
* Descriptor Protocol::
* Invoking Descriptors: Invoking Descriptors<2>.
* Descriptor Example::
* Properties::
* Functions and Methods::
* Static Methods and Class Methods::


File: python.info,  Node: Abstract,  Next: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.1 Abstract
---------------

Defines descriptors, summarizes the protocol, and shows how descriptors
are called.  Examines a custom descriptor and several built-in python
descriptors including functions, properties, static methods, and class
methods.  Shows how each works by giving a pure Python equivalent and a
sample application.

  Learning about descriptors not only provides access to a larger
toolset, it creates a deeper understanding of how Python works and an
appreciation for the elegance of its design.


File: python.info,  Node: Definition and Introduction,  Next: Descriptor Protocol,  Prev: Abstract,  Up: Descriptor HowTo Guide

10.4.2 Definition and Introduction
----------------------------------

In general, a descriptor is an object attribute with "binding
behavior", one whose attribute access has been overridden by methods in
the descriptor protocol.  Those methods are *note __get__(): 700, *note
__set__(): 701, and *note __delete__(): 702.  If any of those methods
are defined for an object, it is said to be a descriptor.

  The default behavior for attribute access is to get, set, or delete
the attribute from an object's dictionary.  For instance, `a.x' has a
lookup chain starting with `a.__dict__['x']', then
`type(a).__dict__['x']', and continuing through the base classes of
`type(a)' excluding metaclasses. If the looked-up value is an object
defining one of the descriptor methods, then Python may override the
default behavior and invoke the descriptor method instead.  Where this
occurs in the precedence chain depends on which descriptor methods were
defined.  Note that descriptors are only invoked for new style objects
or classes (a class is new style if it inherits from *note object: 1ee.
or *note type: 487.).

  Descriptors are a powerful, general purpose protocol.  They are the
mechanism behind properties, methods, static methods, class methods,
and *note super(): 376.  They are used throughout Python itself to
implement the new style classes introduced in version 2.2.  Descriptors
simplify the underlying C-code and offer a flexible set of new tools
for everyday Python programs.


File: python.info,  Node: Descriptor Protocol,  Next: Invoking Descriptors<2>,  Prev: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.3 Descriptor Protocol
--------------------------

`descr.__get__(self, obj, type=None) --> value'

  `descr.__set__(self, obj, value) --> None'

  `descr.__delete__(self, obj) --> None'

  That is all there is to it.  Define any of these methods and an
object is considered a descriptor and can override default behavior
upon being looked up as an attribute.

  If an object defines both *note __get__(): 700. and *note __set__():
701, it is considered a data descriptor.  Descriptors that only define
*note __get__(): 700. are called non-data descriptors (they are
typically used for methods but other uses are possible).

  Data and non-data descriptors differ in how overrides are calculated
with respect to entries in an instance's dictionary.  If an instance's
dictionary has an entry with the same name as a data descriptor, the
data descriptor takes precedence.  If an instance's dictionary has an
entry with the same name as a non-data descriptor, the dictionary entry
takes precedence.

  To make a read-only data descriptor, define both *note __get__():
700. and *note __set__(): 701. with the *note __set__(): 701. raising
an *note AttributeError: 1f5. when called.  Defining the *note
__set__(): 701. method with an exception raising placeholder is enough
to make it a data descriptor.


File: python.info,  Node: Invoking Descriptors<2>,  Next: Descriptor Example,  Prev: Descriptor Protocol,  Up: Descriptor HowTo Guide

10.4.4 Invoking Descriptors
---------------------------

A descriptor can be called directly by its method name.  For example,
`d.__get__(obj)'.

  Alternatively, it is more common for a descriptor to be invoked
automatically upon attribute access.  For example, `obj.d' looks up `d'
in the dictionary of `obj'.  If `d' defines the method *note __get__():
700, then `d.__get__(obj)' is invoked according to the precedence rules
listed below.

  The details of invocation depend on whether `obj' is an object or a
class.  Either way, descriptors only work for new style objects and
classes.  A class is new style if it is a subclass of *note object: 1ee.

  For objects, the machinery is in *note object.__getattribute__():
334. which transforms `b.x' into `type(b).__dict__['x'].__get__(b,
type(b))'.  The implementation works through a precedence chain that
gives data descriptors priority over instance variables, instance
variables priority over non-data descriptors, and assigns lowest
priority to *note __getattr__(): 32a. if provided.  The full C
implementation can be found in *note PyObject_GenericGetAttr(): 2aee. in
Objects/object.c(1).

  For classes, the machinery is in `type.__getattribute__()' which
transforms `B.x' into `B.__dict__['x'].__get__(None, B)'.  In pure
Python, it looks like:

    def __getattribute__(self, key):
        "Emulate type_getattro() in Objects/typeobject.c"
        v = object.__getattribute__(self, key)
        if hasattr(v, '__get__'):
           return v.__get__(None, self)
        return v

The important points to remember are:

   * descriptors are invoked by the *note __getattribute__(): 334.
     method

   * overriding *note __getattribute__(): 334. prevents automatic
     descriptor calls

   * *note __getattribute__(): 334. is only available with new style
     classes and objects

   * *note object.__getattribute__(): 334. and
     `type.__getattribute__()' make different calls to *note __get__():
     700.

   * data descriptors always override instance dictionaries.

   * non-data descriptors may be overridden by instance dictionaries.

  The object returned by `super()' also has a custom *note
__getattribute__(): 334.  method for invoking descriptors.  The call
`super(B, obj).m()' searches `obj.__class__.__mro__' for the base class
`A' immediately following `B' and then returns
`A.__dict__['m'].__get__(obj, A)'.  If not a descriptor, `m' is
returned unchanged.  If not in the dictionary, `m' reverts to a search
using *note object.__getattribute__(): 334.

  Note, in Python 2.2, `super(B, obj).m()' would only invoke *note
__get__(): 700. if `m' was a data descriptor.  In Python 2.3, non-data
descriptors also get invoked unless an old-style class is involved.
The implementation details are in `super_getattro()' in
Objects/typeobject.c(2) and a pure Python equivalent can be found in
Guido's Tutorial(3).

  The details above show that the mechanism for descriptors is embedded
in the *note __getattribute__(): 334. methods for *note object: 1ee,
*note type: 487, and *note super(): 376.  Classes inherit this
machinery when they derive from *note object: 1ee. or if they have a
meta-class providing similar functionality.  Likewise, classes can
turn-off descriptor invocation by overriding *note __getattribute__():
334.

  ---------- Footnotes ----------

  (1)
http://svn.python.org/view/python/trunk/Objects/object.c?view=markup

  (2)
http://svn.python.org/view/python/trunk/Objects/typeobject.c?view=markup

  (3) http://www.python.org/2.2.3/descrintro.html#cooperation


File: python.info,  Node: Descriptor Example,  Next: Properties,  Prev: Invoking Descriptors<2>,  Up: Descriptor HowTo Guide

10.4.5 Descriptor Example
-------------------------

The following code creates a class whose objects are data descriptors
which print a message for each get or set.  Overriding *note
__getattribute__(): 334. is alternate approach that could do this for
every attribute.  However, this descriptor is useful for monitoring
just a few chosen attributes:

    class RevealAccess(object):
        """A data descriptor that sets and returns values
           normally and prints a message logging their access.
        """

        def __init__(self, initval=None, name='var'):
            self.val = initval
            self.name = name

        def __get__(self, obj, objtype):
            print 'Retrieving', self.name
            return self.val

        def __set__(self, obj, val):
            print 'Updating' , self.name
            self.val = val

    >>> class MyClass(object):
        x = RevealAccess(10, 'var "x"')
        y = 5

    >>> m = MyClass()
    >>> m.x
    Retrieving var "x"
    10
    >>> m.x = 20
    Updating var "x"
    >>> m.x
    Retrieving var "x"
    20
    >>> m.y
    5

The protocol is simple and offers exciting possibilities.  Several use
cases are so common that they have been packaged into individual
function calls.  Properties, bound and unbound methods, static methods,
and class methods are all based on the descriptor protocol.


File: python.info,  Node: Properties,  Next: Functions and Methods,  Prev: Descriptor Example,  Up: Descriptor HowTo Guide

10.4.6 Properties
-----------------

Calling *note property(): 480. is a succinct way of building a data
descriptor that triggers function calls upon access to an attribute.
Its signature is:

    property(fget=None, fset=None, fdel=None, doc=None) -> property attribute

The documentation shows a typical use to define a managed attribute `x':

    class C(object):
        def getx(self): return self.__x
        def setx(self, value): self.__x = value
        def delx(self): del self.__x
        x = property(getx, setx, delx, "I'm the 'x' property.")

To see how *note property(): 480. is implemented in terms of the
descriptor protocol, here is a pure Python equivalent:

    class Property(object):
        "Emulate PyProperty_Type() in Objects/descrobject.c"

        def __init__(self, fget=None, fset=None, fdel=None, doc=None):
            self.fget = fget
            self.fset = fset
            self.fdel = fdel
            if doc is None and fget is not None:
                doc = fget.__doc__
            self.__doc__ = doc

        def __get__(self, obj, objtype=None):
            if obj is None:
                return self
            if self.fget is None:
                raise AttributeError("unreadable attribute")
            return self.fget(obj)

        def __set__(self, obj, value):
            if self.fset is None:
                raise AttributeError("can't set attribute")
            self.fset(obj, value)

        def __delete__(self, obj):
            if self.fdel is None:
                raise AttributeError("can't delete attribute")
            self.fdel(obj)

        def getter(self, fget):
            return type(self)(fget, self.fset, self.fdel, self.__doc__)

        def setter(self, fset):
            return type(self)(self.fget, fset, self.fdel, self.__doc__)

        def deleter(self, fdel):
            return type(self)(self.fget, self.fset, fdel, self.__doc__)

The *note property(): 480. builtin helps whenever a user interface has
granted attribute access and then subsequent changes require the
intervention of a method.

  For instance, a spreadsheet class may grant access to a cell value
through `Cell('b10').value'. Subsequent improvements to the program
require the cell to be recalculated on every access; however, the
programmer does not want to affect existing client code accessing the
attribute directly.  The solution is to wrap access to the value
attribute in a property data descriptor:

    class Cell(object):
        . . .
        def getvalue(self, obj):
            "Recalculate cell before returning value"
            self.recalc()
            return obj._value
        value = property(getvalue)



File: python.info,  Node: Functions and Methods,  Next: Static Methods and Class Methods,  Prev: Properties,  Up: Descriptor HowTo Guide

10.4.7 Functions and Methods
----------------------------

Python's object oriented features are built upon a function based
environment.  Using non-data descriptors, the two are merged seamlessly.

  Class dictionaries store methods as functions.  In a class
definition, methods are written using *note def: 3ed. and *note lambda:
3fc, the usual tools for creating functions.  The only difference from
regular functions is that the first argument is reserved for the object
instance.  By Python convention, the instance reference is called
_self_ but may be called _this_ or any other variable name.

  To support method calls, functions include the *note __get__(): 700.
method for binding methods during attribute access.  This means that
all functions are non-data descriptors which return bound or unbound
methods depending whether they are invoked from an object or a class.
In pure python, it works like this:

    class Function(object):
        . . .
        def __get__(self, obj, objtype=None):
            "Simulate func_descr_get() in Objects/funcobject.c"
            return types.MethodType(self, obj, objtype)

Running the interpreter shows how the function descriptor works in
practice:

    >>> class D(object):
         def f(self, x):
              return x

    >>> d = D()
    >>> D.__dict__['f'] # Stored internally as a function
    <function f at 0x00C45070>
    >>> D.f             # Get from a class becomes an unbound method
    <unbound method D.f>
    >>> d.f             # Get from an instance becomes a bound method
    <bound method D.f of <__main__.D object at 0x00B18C90>>

The output suggests that bound and unbound methods are two different
types.  While they could have been implemented that way, the actual C
implementation of *note PyMethod_Type: 2ce5. in Objects/classobject.c(1)
is a single object with two different representations depending on
whether the `im_self' field is set or is _NULL_ (the C equivalent of
_None_).

  Likewise, the effects of calling a method object depend on the
`im_self' field. If set (meaning bound), the original function (stored
in the `im_func' field) is called as expected with the first argument
set to the instance.  If unbound, all of the arguments are passed
unchanged to the original function. The actual C implementation of
`instancemethod_call()' is only slightly more complex in that it
includes some type checking.

  ---------- Footnotes ----------

  (1)
http://svn.python.org/view/python/trunk/Objects/classobject.c?view=markup


File: python.info,  Node: Static Methods and Class Methods,  Prev: Functions and Methods,  Up: Descriptor HowTo Guide

10.4.8 Static Methods and Class Methods
---------------------------------------

Non-data descriptors provide a simple mechanism for variations on the
usual patterns of binding functions into methods.

  To recap, functions have a *note __get__(): 700. method so that they
can be converted to a method when accessed as attributes.  The non-data
descriptor transforms a `obj.f(*args)' call into `f(obj, *args)'.
Calling `klass.f(*args)' becomes `f(*args)'.

  This chart summarizes the binding and its two most useful variants:

      Transformation        Called from an Object      Called from a Class
     ------------------------------------------------------------------------ 
     function              f(obj, *args)              f(*args)
     staticmethod          f(*args)                   f(*args)
     classmethod           f(type(obj), *args)        f(klass, *args)


  Static methods return the underlying function without changes.
Calling either `c.f' or `C.f' is the equivalent of a direct lookup into
`object.__getattribute__(c, "f")' or `object.__getattribute__(C, "f")'.
As a result, the function becomes identically accessible from either an
object or a class.

  Good candidates for static methods are methods that do not reference
the `self' variable.

  For instance, a statistics package may include a container class for
experimental data.  The class provides normal methods for computing the
average, mean, median, and other descriptive statistics that depend on
the data. However, there may be useful functions which are conceptually
related but do not depend on the data.  For instance, `erf(x)' is handy
conversion routine that comes up in statistical work but does not
directly depend on a particular dataset.  It can be called either from
an object or the class:  `s.erf(1.5) --> .9332' or `Sample.erf(1.5) -->
.9332'.

  Since staticmethods return the underlying function with no changes,
the example calls are unexciting:

    >>> class E(object):
         def f(x):
              print x
         f = staticmethod(f)

    >>> print E.f(3)
    3
    >>> print E().f(3)
    3

Using the non-data descriptor protocol, a pure Python version of *note
staticmethod(): 3ee. would look like this:

    class StaticMethod(object):
     "Emulate PyStaticMethod_Type() in Objects/funcobject.c"

     def __init__(self, f):
          self.f = f

     def __get__(self, obj, objtype=None):
          return self.f

Unlike static methods, class methods prepend the class reference to the
argument list before calling the function.  This format is the same for
whether the caller is an object or a class:

    >>> class E(object):
         def f(klass, x):
              return klass.__name__, x
         f = classmethod(f)

    >>> print E.f(3)
    ('E', 3)
    >>> print E().f(3)
    ('E', 3)

This behavior is useful whenever the function only needs to have a class
reference and does not care about any underlying data.  One use for
classmethods is to create alternate class constructors.  In Python 2.3,
the classmethod *note dict.fromkeys(): 8e5. creates a new dictionary
from a list of keys.  The pure Python equivalent is:

    class Dict(object):
        . . .
        def fromkeys(klass, iterable, value=None):
            "Emulate dict_fromkeys() in Objects/dictobject.c"
            d = klass()
            for key in iterable:
                d[key] = value
            return d
        fromkeys = classmethod(fromkeys)

Now a new dictionary of unique keys can be constructed like this:

    >>> Dict.fromkeys('abracadabra')
    {'a': None, 'r': None, 'b': None, 'c': None, 'd': None}

Using the non-data descriptor protocol, a pure Python version of *note
classmethod(): 3ef. would look like this:

    class ClassMethod(object):
         "Emulate PyClassMethod_Type() in Objects/funcobject.c"

         def __init__(self, f):
              self.f = f

         def __get__(self, obj, klass=None):
              if klass is None:
                   klass = type(obj)
              def newfunc(*args):
                   return self.f(klass, *args)
              return newfunc



File: python.info,  Node: Idioms and Anti-Idioms in Python,  Next: Functional Programming HOWTO,  Prev: Descriptor HowTo Guide,  Up: Python HOWTOs

10.5 Idioms and Anti-Idioms in Python
=====================================

     Author: Moshe Zadka

  This document is placed in the public domain.

Abstract
........

This document can be considered a companion to the tutorial. It shows
how to use Python, and even more importantly, how _not_ to use Python.

* Menu:

* Language Constructs You Should Not Use::
* Exceptions: Exceptions<8>.
* Using the Batteries::
* Using Backslash to Continue Statements::

Language Constructs You Should Not Use

* from module import *::
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends.
* from module import name1, name2: from module import name1 name2.
* except;: except.

from module import *

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::


File: python.info,  Node: Language Constructs You Should Not Use,  Next: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

10.5.1 Language Constructs You Should Not Use
---------------------------------------------

While Python has relatively few gotchas compared to other languages, it
still has some constructs which are only useful in corner cases, or are
plain dangerous.

* Menu:

* from module import *::
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends.
* from module import name1, name2: from module import name1 name2.
* except;: except.

from module import *

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::


File: python.info,  Node: from module import *,  Next: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

10.5.1.1 from module import *
.............................

* Menu:

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::


File: python.info,  Node: Inside Function Definitions,  Next: At Module Level,  Up: from module import *

10.5.1.2 Inside Function Definitions
....................................

`from module import *' is _invalid_ inside function definitions. While
many versions of Python do not check for the invalidity, it does not
make it more valid, no more than having a smart lawyer makes a man
innocent. Do not use it like that ever. Even in versions where it was
accepted, it made the function execution slower, because the compiler
could not be certain which names were local and which were global. In
Python 2.1 this construct causes warnings, and sometimes even errors.


File: python.info,  Node: At Module Level,  Next: When It Is Just Fine,  Prev: Inside Function Definitions,  Up: from module import *

10.5.1.3 At Module Level
........................

While it is valid to use `from module import *' at module level it is
usually a bad idea. For one, this loses an important property Python
otherwise has -- you can know where each toplevel name is defined by a
simple "search" function in your favourite editor. You also open
yourself to trouble in the future, if some module grows additional
functions or classes.

  One of the most awful questions asked on the newsgroup is why this
code:

    f = open("www")
    f.read()

does not work. Of course, it works just fine (assuming you have a file
called "www".) But it does not work if somewhere in the module, the
statement `from os import *' is present. The *note os: 128. module has
a function called *note open(): 2d3. which returns an integer. While it
is very useful, shadowing a builtin is one of its least useful
properties.

  Remember, you can never know for sure what names a module exports, so
either take what you need -- `from module import name1, name2', or keep
them in the module and access on a per-need basis --  `import
module;print module.name'.


File: python.info,  Node: When It Is Just Fine,  Prev: At Module Level,  Up: from module import *

10.5.1.4 When It Is Just Fine
.............................

There are situations in which `from module import *' is just fine:

   * The interactive prompt. For example, `from math import *' makes
     Python an amazing scientific calculator.

   * When extending a module in C with a module in Python.

   * When the module advertises itself as `from import *' safe.


File: python.info,  Node: Unadorned exec execfile and friends,  Next: from module import name1 name2,  Prev: from module import *,  Up: Language Constructs You Should Not Use

10.5.1.5 Unadorned `exec', `execfile()' and friends
...................................................

The word "unadorned" refers to the use without an explicit dictionary,
in which case those constructs evaluate code in the _current_
environment. This is dangerous for the same reasons `from import *' is
dangerous -- it might step over variables you are counting on and mess
up things for the rest of your code.  Simply do not do that.

  Bad examples:

    >>> for name in sys.argv[1:]:
    >>>     exec "%s=1" % name
    >>> def func(s, **kw):
    >>>     for var, val in kw.items():
    >>>         exec "s.%s=val" % var  # invalid!
    >>> execfile("handler.py")
    >>> handle()

Good examples:

    >>> d = {}
    >>> for name in sys.argv[1:]:
    >>>     d[name] = 1
    >>> def func(s, **kw):
    >>>     for var, val in kw.items():
    >>>         setattr(s, var, val)
    >>> d={}
    >>> execfile("handle.py", d, d)
    >>> handle = d['handle']
    >>> handle()



File: python.info,  Node: from module import name1 name2,  Next: except,  Prev: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

10.5.1.6 from module import name1, name2
........................................

This is a "don't" which is much weaker than the previous "don't"s but
is still something you should not do if you don't have good reasons to
do that. The reason it is usually a bad idea is because you suddenly
have an object which lives in two separate namespaces. When the binding
in one namespace changes, the binding in the other will not, so there
will be a discrepancy between them. This happens when, for example, one
module is reloaded, or changes the definition of a function at runtime.

  Bad example:

    # foo.py
    a = 1

    # bar.py
    from foo import a
    if something():
        a = 2 # danger: foo.a != a

Good example:

    # foo.py
    a = 1

    # bar.py
    import foo
    if something():
        foo.a = 2



File: python.info,  Node: except,  Prev: from module import name1 name2,  Up: Language Constructs You Should Not Use

10.5.1.7 except:
................

Python has the `except:' clause, which catches all exceptions. Since
_every_ error in Python raises an exception, using `except:' can make
many programming errors look like runtime problems, which hinders the
debugging process.

  The following code shows a great example of why this is bad:

    try:
        foo = opne("file") # misspelled "open"
    except:
        sys.exit("could not open file!")

The second line triggers a *note NameError: 39c, which is caught by the
except clause. The program will exit, and the error message the program
prints will make you think the problem is the readability of `"file"'
when in fact the real error has nothing to do with `"file"'.

  A better way to write the above is

    try:
        foo = opne("file")
    except IOError:
        sys.exit("could not open file")

When this is run, Python will produce a traceback showing the *note
NameError: 39c, and it will be immediately apparent what needs to be
fixed.

  Because `except:' catches _all_ exceptions, including *note
SystemExit: 32b, *note KeyboardInterrupt: 24e, and *note GeneratorExit:
330. (which is not an error and should not normally be caught by user
code), using a bare `except:' is almost never a good idea.  In
situations where you need to catch all "normal" errors, such as in a
framework that runs callbacks, you can catch the base class for all
normal exceptions, *note Exception: 332.  Unfortunately in Python 2.x
it is possible for third-party code to raise exceptions that do not
inherit from *note Exception: 332, so in Python 2.x there are some
cases where you may have to use a bare `except:' and manually re-raise
the exceptions you don't want to catch.


File: python.info,  Node: Exceptions<8>,  Next: Using the Batteries,  Prev: Language Constructs You Should Not Use,  Up: Idioms and Anti-Idioms in Python

10.5.2 Exceptions
-----------------

Exceptions are a useful feature of Python. You should learn to raise
them whenever something unexpected occurs, and catch them only where
you can do something about them.

  The following is a very popular anti-idiom

    def get_status(file):
        if not os.path.exists(file):
            print "file not found"
            sys.exit(1)
        return open(file).readline()

Consider the case where the file gets deleted between the time the call
to *note os.path.exists(): ddf. is made and the time *note open(): 2d3.
is called. In that case the last line will raise an *note IOError: 1f7.
The same thing would happen if _file_ exists but has no read
permission.  Since testing this on a normal machine on existent and
non-existent files makes it seem bugless, the test results will seem
fine, and the code will get shipped.  Later an unhandled *note IOError:
1f7. (or perhaps some other *note EnvironmentError: 939.) escapes to the
user, who gets to watch the ugly traceback.

  Here is a somewhat better way to do it.

    def get_status(file):
        try:
            return open(file).readline()
        except EnvironmentError as err:
            print "Unable to open file: {}".format(err)
            sys.exit(1)

In this version, _either_ the file gets opened and the line is read (so
it works even on flaky NFS or SMB connections), or an error message is
printed that provides all the available information on why the open
failed, and the application is aborted.

  However, even this version of `get_status()' makes too many
assumptions -- that it will only be used in a short running script, and
not, say, in a long running server. Sure, the caller could do something
like

    try:
        status = get_status(log)
    except SystemExit:
        status = None

But there is a better way.  You should try to use as few `except'
clauses in your code as you can -- the ones you do use will usually be
inside calls which should always succeed, or a catch-all in a main
function.

  So, an even better version of `get_status()' is probably

    def get_status(file):
        return open(file).readline()

The caller can deal with the exception if it wants (for example, if it
tries several files in a loop), or just let the exception filter
upwards to _its_ caller.

  But the last version still has a serious problem -- due to
implementation details in CPython, the file would not be closed when an
exception is raised until the exception handler finishes; and, worse,
in other implementations (e.g., Jython) it might not be closed at all
regardless of whether or not an exception is raised.

  The best version of this function uses the `open()' call as a context
manager, which will ensure that the file gets closed as soon as the
function returns:

    def get_status(file):
        with open(file) as fp:
            return fp.readline()



File: python.info,  Node: Using the Batteries,  Next: Using Backslash to Continue Statements,  Prev: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

10.5.3 Using the Batteries
--------------------------

Every so often, people seem to be writing stuff in the Python library
again, usually poorly. While the occasional module has a poor
interface, it is usually much better to use the rich standard library
and data types that come with Python than inventing your own.

  A useful module very few people know about is *note os.path: 129. It
always has the correct path arithmetic for your operating system, and
will usually be much better than whatever you come up with yourself.

  Compare:

    # ugh!
    return dir+"/"+file
    # better
    return os.path.join(dir, file)

More useful functions in *note os.path: 129.: `basename()',
`dirname()' and `splitext()'.

  There are also many useful built-in functions people seem not to be
aware of for some reason: *note min(): 221. and *note max(): 222. can
find the minimum/maximum of any sequence with comparable semantics, for
example, yet many people write their own *note max(): 222./*note min():
221. Another highly useful function is *note reduce(): 2e2. which can
be used to repeatly apply a binary operation to a sequence, reducing it
to a single value.  For example, compute a factorial with a series of
multiply operations:

    >>> n = 4
    >>> import operator
    >>> reduce(operator.mul, range(1, n+1))
    24

When it comes to parsing numbers, note that *note float(): 1e8, *note
int(): 1ef. and *note long(): 1f0. all accept string arguments and will
reject ill-formed strings by raising an *note ValueError: 233.


File: python.info,  Node: Using Backslash to Continue Statements,  Prev: Using the Batteries,  Up: Idioms and Anti-Idioms in Python

10.5.4 Using Backslash to Continue Statements
---------------------------------------------

Since Python treats a newline as a statement terminator, and since
statements are often more than is comfortable to put in one line, many
people do:

    if foo.bar()['first'][0] == baz.quux(1, 2)[5:9] and \
       calculate_number(10, 20) != forbulate(500, 360):
          pass

You should realize that this is dangerous: a stray space after the `\'
would make this line wrong, and stray spaces are notoriously hard to
see in editors.  In this case, at least it would be a syntax error, but
if the code was:

    value = foo.bar()['first'][0]*baz.quux(1, 2)[5:9] \
            + calculate_number(10, 20)*forbulate(500, 360)

then it would just be subtly wrong.

  It is usually much better to use the implicit continuation inside
parenthesis:

  This version is bulletproof:

    value = (foo.bar()['first'][0]*baz.quux(1, 2)[5:9]
            + calculate_number(10, 20)*forbulate(500, 360))



File: python.info,  Node: Functional Programming HOWTO,  Next: Logging HOWTO,  Prev: Idioms and Anti-Idioms in Python,  Up: Python HOWTOs

10.6 Functional Programming HOWTO
=================================

     Author: A. M. Kuchling

     Release: 0.31

  In this document, we'll take a tour of Python's features suitable for
implementing programs in a functional style.  After an introduction to
the concepts of functional programming, we'll look at language features
such as *note iterator: 869.s and *note generator: 5cd.s and relevant
library modules such as *note itertools: fa. and *note functools: d9.

* Menu:

* Introduction: Introduction<12>.
* Iterators: Iterators<2>.
* Generator expressions and list comprehensions::
* Generators: Generators<2>.
* Built-in functions::
* Small functions and the lambda expression::
* The itertools module::
* The functools module::
* Revision History and Acknowledgements::
* References::

Introduction

* Formal provability::
* Modularity::
* Ease of debugging and testing::
* Composability::

Iterators

* Data Types That Support Iterators::

Generators

* Passing values into a generator::

The itertools module

* Creating new iterators::
* Calling functions on elements::
* Selecting elements::
* Grouping elements::

The functools module

* The operator module::

References

* General::
* Python-specific::
* Python documentation::


File: python.info,  Node: Introduction<12>,  Next: Iterators<2>,  Up: Functional Programming HOWTO

10.6.1 Introduction
-------------------

This section explains the basic concept of functional programming; if
you're just interested in learning about Python language features, skip
to the next section.

  Programming languages support decomposing problems in several
different ways:

   * Most programming languages are *procedural*: programs are lists of
     instructions that tell the computer what to do with the program's
     input.  C, Pascal, and even Unix shells are procedural languages.

   * In *declarative* languages, you write a specification that
     describes the problem to be solved, and the language
     implementation figures out how to perform the computation
     efficiently.  SQL is the declarative language you're most likely
     to be familiar with; a SQL query describes the data set you want
     to retrieve, and the SQL engine decides whether to scan tables or
     use indexes, which subclauses should be performed first, etc.

   * *Object-oriented* programs manipulate collections of objects.
     Objects have internal state and support methods that query or
     modify this internal state in some way. Smalltalk and Java are
     object-oriented languages.  C++ and Python are languages that
     support object-oriented programming, but don't force the use of
     object-oriented features.

   * *Functional* programming decomposes a problem into a set of
     functions.  Ideally, functions only take inputs and produce
     outputs, and don't have any internal state that affects the output
     produced for a given input.  Well-known functional languages
     include the ML family (Standard ML, OCaml, and other variants) and
     Haskell.

  The designers of some computer languages choose to emphasize one
particular approach to programming.  This often makes it difficult to
write programs that use a different approach.  Other languages are
multi-paradigm languages that support several different approaches.
Lisp, C++, and Python are multi-paradigm; you can write programs or
libraries that are largely procedural, object-oriented, or functional
in all of these languages.  In a large program, different sections
might be written using different approaches; the GUI might be
object-oriented while the processing logic is procedural or functional,
for example.

  In a functional program, input flows through a set of functions. Each
function operates on its input and produces some output.  Functional
style discourages functions with side effects that modify internal
state or make other changes that aren't visible in the function's
return value.  Functions that have no side effects at all are called
*purely functional*.  Avoiding side effects means not using data
structures that get updated as a program runs; every function's output
must only depend on its input.

  Some languages are very strict about purity and don't even have
assignment statements such as `a=3' or `c = a + b', but it's difficult
to avoid all side effects.  Printing to the screen or writing to a disk
file are side effects, for example.  For example, in Python a `print'
statement or a `time.sleep(1)' both return no useful value; they're
only called for their side effects of sending some text to the screen
or pausing execution for a second.

  Python programs written in functional style usually won't go to the
extreme of avoiding all I/O or all assignments; instead, they'll
provide a functional-appearing interface but will use non-functional
features internally.  For example, the implementation of a function
will still use assignments to local variables, but won't modify global
variables or have other side effects.

  Functional programming can be considered the opposite of
object-oriented programming.  Objects are little capsules containing
some internal state along with a collection of method calls that let
you modify this state, and programs consist of making the right set of
state changes.  Functional programming wants to avoid state changes as
much as possible and works with data flowing between functions.  In
Python you might combine the two approaches by writing functions that
take and return instances representing objects in your application
(e-mail messages, transactions, etc.).

  Functional design may seem like an odd constraint to work under.  Why
should you avoid objects and side effects?  There are theoretical and
practical advantages to the functional style:

   * Formal provability.

   * Modularity.

   * Composability.

   * Ease of debugging and testing.

* Menu:

* Formal provability::
* Modularity::
* Ease of debugging and testing::
* Composability::


File: python.info,  Node: Formal provability,  Next: Modularity,  Up: Introduction<12>

10.6.1.1 Formal provability
...........................

A theoretical benefit is that it's easier to construct a mathematical
proof that a functional program is correct.

  For a long time researchers have been interested in finding ways to
mathematically prove programs correct.  This is different from testing
a program on numerous inputs and concluding that its output is usually
correct, or reading a program's source code and concluding that the
code looks right; the goal is instead a rigorous proof that a program
produces the right result for all possible inputs.

  The technique used to prove programs correct is to write down
*invariants*, properties of the input data and of the program's
variables that are always true.  For each line of code, you then show
that if invariants X and Y are true *before* the line is executed, the
slightly different invariants X' and Y' are true *after* the line is
executed.  This continues until you reach the end of the program, at
which point the invariants should match the desired conditions on the
program's output.

  Functional programming's avoidance of assignments arose because
assignments are difficult to handle with this technique; assignments
can break invariants that were true before the assignment without
producing any new invariants that can be propagated onward.

  Unfortunately, proving programs correct is largely impractical and
not relevant to Python software. Even trivial programs require proofs
that are several pages long; the proof of correctness for a moderately
complicated program would be enormous, and few or none of the programs
you use daily (the Python interpreter, your XML parser, your web
browser) could be proven correct.  Even if you wrote down or generated
a proof, there would then be the question of verifying the proof; maybe
there's an error in it, and you wrongly believe you've proved the
program correct.


File: python.info,  Node: Modularity,  Next: Ease of debugging and testing,  Prev: Formal provability,  Up: Introduction<12>

10.6.1.2 Modularity
...................

A more practical benefit of functional programming is that it forces
you to break apart your problem into small pieces.  Programs are more
modular as a result.  It's easier to specify and write a small function
that does one thing than a large function that performs a complicated
transformation.  Small functions are also easier to read and to check
for errors.


File: python.info,  Node: Ease of debugging and testing,  Next: Composability,  Prev: Modularity,  Up: Introduction<12>

10.6.1.3 Ease of debugging and testing
......................................

Testing and debugging a functional-style program is easier.

  Debugging is simplified because functions are generally small and
clearly specified.  When a program doesn't work, each function is an
interface point where you can check that the data are correct.  You can
look at the intermediate inputs and outputs to quickly isolate the
function that's responsible for a bug.

  Testing is easier because each function is a potential subject for a
unit test.  Functions don't depend on system state that needs to be
replicated before running a test; instead you only have to synthesize
the right input and then check that the output matches expectations.


File: python.info,  Node: Composability,  Prev: Ease of debugging and testing,  Up: Introduction<12>

10.6.1.4 Composability
......................

As you work on a functional-style program, you'll write a number of
functions with varying inputs and outputs.  Some of these functions
will be unavoidably specialized to a particular application, but others
will be useful in a wide variety of programs.  For example, a function
that takes a directory path and returns all the XML files in the
directory, or a function that takes a filename and returns its
contents, can be applied to many different situations.

  Over time you'll form a personal library of utilities.  Often you'll
assemble new programs by arranging existing functions in a new
configuration and writing a few functions specialized for the current
task.


File: python.info,  Node: Iterators<2>,  Next: Generator expressions and list comprehensions,  Prev: Introduction<12>,  Up: Functional Programming HOWTO

10.6.2 Iterators
----------------

I'll start by looking at a Python language feature that's an important
foundation for writing functional-style programs: iterators.

  An iterator is an object representing a stream of data; this object
returns the data one element at a time.  A Python iterator must support
a method called `next()' that takes no arguments and always returns the
next element of the stream.  If there are no more elements in the
stream, `next()' must raise the `StopIteration' exception.  Iterators
don't have to be finite, though; it's perfectly reasonable to write an
iterator that produces an infinite stream of data.

  The built-in *note iter(): 319. function takes an arbitrary object
and tries to return an iterator that will return the object's contents
or elements, raising *note TypeError: 215. if the object doesn't
support iteration.  Several of Python's built-in data types support
iteration, the most common being lists and dictionaries.  An object is
called an *iterable* object if you can get an iterator for it.

  You can experiment with the iteration interface manually:

    >>> L = [1,2,3]
    >>> it = iter(L)
    >>> print it
    <...iterator object at ...>
    >>> it.next()
    1
    >>> it.next()
    2
    >>> it.next()
    3
    >>> it.next()
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    StopIteration
    >>>

Python expects iterable objects in several different contexts, the most
important being the `for' statement.  In the statement `for X in Y', Y
must be an iterator or some object for which `iter()' can create an
iterator.  These two statements are equivalent:

    for i in iter(obj):
        print i

    for i in obj:
        print i

Iterators can be materialized as lists or tuples by using the *note
list(): 3b5. or *note tuple(): 401. constructor functions:

    >>> L = [1,2,3]
    >>> iterator = iter(L)
    >>> t = tuple(iterator)
    >>> t
    (1, 2, 3)

Sequence unpacking also supports iterators: if you know an iterator
will return N elements, you can unpack them into an N-tuple:

    >>> L = [1,2,3]
    >>> iterator = iter(L)
    >>> a,b,c = iterator
    >>> a,b,c
    (1, 2, 3)

Built-in functions such as *note max(): 222. and *note min(): 221. can
take a single iterator argument and will return the largest or smallest
element.  The `"in"' and `"not in"' operators also support iterators:
`X in iterator' is true if X is found in the stream returned by the
iterator.  You'll run into obvious problems if the iterator is
infinite; `max()', `min()' will never return, and if the element X
never appears in the stream, the `"in"' and `"not in"' operators won't
return either.

  Note that you can only go forward in an iterator; there's no way to
get the previous element, reset the iterator, or make a copy of it.
Iterator objects can optionally provide these additional capabilities,
but the iterator protocol only specifies the `next()' method.
Functions may therefore consume all of the iterator's output, and if
you need to do something different with the same stream, you'll have to
create a new iterator.

* Menu:

* Data Types That Support Iterators::


File: python.info,  Node: Data Types That Support Iterators,  Up: Iterators<2>

10.6.2.1 Data Types That Support Iterators
..........................................

We've already seen how lists and tuples support iterators.  In fact,
any Python sequence type, such as strings, will automatically support
creation of an iterator.

  Calling *note iter(): 319. on a dictionary returns an iterator that
will loop over the dictionary's keys:

    >>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
    ...      'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
    >>> for key in m:
    ...     print key, m[key]
    Mar 3
    Feb 2
    Aug 8
    Sep 9
    Apr 4
    Jun 6
    Jul 7
    Jan 1
    May 5
    Nov 11
    Dec 12
    Oct 10

Note that the order is essentially random, because it's based on the
hash ordering of the objects in the dictionary.

  Applying `iter()' to a dictionary always loops over the keys, but
dictionaries have methods that return other iterators.  If you want to
iterate over keys, values, or key/value pairs, you can explicitly call
the `iterkeys()', `itervalues()', or `iteritems()' methods to get an
appropriate iterator.

  The *note dict(): 2fe. constructor can accept an iterator that
returns a finite stream of `(key, value)' tuples:

    >>> L = [('Italy', 'Rome'), ('France', 'Paris'), ('US', 'Washington DC')]
    >>> dict(iter(L))
    {'Italy': 'Rome', 'US': 'Washington DC', 'France': 'Paris'}

Files also support iteration by calling the `readline()' method until
there are no more lines in the file.  This means you can read each line
of a file like this:

    for line in file:
        # do something for each line
        ...

Sets can take their contents from an iterable and let you iterate over
the set's elements:

    S = set((2, 3, 5, 7, 11, 13))
    for i in S:
        print i



File: python.info,  Node: Generator expressions and list comprehensions,  Next: Generators<2>,  Prev: Iterators<2>,  Up: Functional Programming HOWTO

10.6.3 Generator expressions and list comprehensions
----------------------------------------------------

Two common operations on an iterator's output are 1) performing some
operation for every element, 2) selecting a subset of elements that
meet some condition.  For example, given a list of strings, you might
want to strip off trailing whitespace from each line or extract all the
strings containing a given substring.

  List comprehensions and generator expressions (short form:
"listcomps" and "genexps") are a concise notation for such operations,
borrowed from the functional programming language Haskell
(<http://www.haskell.org/>).  You can strip all the whitespace from a
stream of strings with the following code:

    line_list = ['  line 1\n', 'line 2  \n', ...]

    # Generator expression -- returns iterator
    stripped_iter = (line.strip() for line in line_list)

    # List comprehension -- returns list
    stripped_list = [line.strip() for line in line_list]

You can select only certain elements by adding an `"if"' condition:

    stripped_list = [line.strip() for line in line_list
                     if line != ""]

With a list comprehension, you get back a Python list; `stripped_list'
is a list containing the resulting lines, not an iterator.  Generator
expressions return an iterator that computes the values as necessary,
not needing to materialize all the values at once.  This means that
list comprehensions aren't useful if you're working with iterators that
return an infinite stream or a very large amount of data.  Generator
expressions are preferable in these situations.

  Generator expressions are surrounded by parentheses ("()") and list
comprehensions are surrounded by square brackets ("[]").  Generator
expressions have the form:

    ( expression for expr in sequence1
                 if condition1
                 for expr2 in sequence2
                 if condition2
                 for expr3 in sequence3 ...
                 if condition3
                 for exprN in sequenceN
                 if conditionN )

Again, for a list comprehension only the outside brackets are different
(square brackets instead of parentheses).

  The elements of the generated output will be the successive values of
`expression'.  The `if' clauses are all optional; if present,
`expression' is only evaluated and added to the result when `condition'
is true.

  Generator expressions always have to be written inside parentheses,
but the parentheses signalling a function call also count.  If you want
to create an iterator that will be immediately passed to a function you
can write:

    obj_total = sum(obj.count for obj in list_all_objects())

The `for...in' clauses contain the sequences to be iterated over.  The
sequences do not have to be the same length, because they are iterated
over from left to right, *not* in parallel.  For each element in
`sequence1', `sequence2' is looped over from the beginning.
`sequence3' is then looped over for each resulting pair of elements
from `sequence1' and `sequence2'.

  To put it another way, a list comprehension or generator expression is
equivalent to the following Python code:

    for expr1 in sequence1:
        if not (condition1):
            continue   # Skip this element
        for expr2 in sequence2:
            if not (condition2):
                continue    # Skip this element
            ...
            for exprN in sequenceN:
                 if not (conditionN):
                     continue   # Skip this element

                 # Output the value of
                 # the expression.

This means that when there are multiple `for...in' clauses but no `if'
clauses, the length of the resulting output will be equal to the
product of the lengths of all the sequences.  If you have two lists of
length 3, the output list is 9 elements long:

    >>> seq1 = 'abc'
    >>> seq2 = (1,2,3)
    >>> [(x,y) for x in seq1 for y in seq2]
    [('a', 1), ('a', 2), ('a', 3),
     ('b', 1), ('b', 2), ('b', 3),
     ('c', 1), ('c', 2), ('c', 3)]

To avoid introducing an ambiguity into Python's grammar, if
`expression' is creating a tuple, it must be surrounded with
parentheses.  The first list comprehension below is a syntax error,
while the second one is correct:

    # Syntax error
    [ x,y for x in seq1 for y in seq2]
    # Correct
    [ (x,y) for x in seq1 for y in seq2]



File: python.info,  Node: Generators<2>,  Next: Built-in functions,  Prev: Generator expressions and list comprehensions,  Up: Functional Programming HOWTO

10.6.4 Generators
-----------------

Generators are a special class of functions that simplify the task of
writing iterators.  Regular functions compute a value and return it,
but generators return an iterator that returns a stream of values.

  You're doubtless familiar with how regular function calls work in
Python or C.  When you call a function, it gets a private namespace
where its local variables are created.  When the function reaches a
`return' statement, the local variables are destroyed and the value is
returned to the caller.  A later call to the same function creates a
new private namespace and a fresh set of local variables. But, what if
the local variables weren't thrown away on exiting a function?  What if
you could later resume the function where it left off?  This is what
generators provide; they can be thought of as resumable functions.

  Here's the simplest example of a generator function:

    def generate_ints(N):
        for i in range(N):
            yield i

Any function containing a `yield' keyword is a generator function; this
is detected by Python's *note bytecode: 57a. compiler which compiles
the function specially as a result.

  When you call a generator function, it doesn't return a single value;
instead it returns a generator object that supports the iterator
protocol.  On executing the `yield' expression, the generator outputs
the value of `i', similar to a `return' statement.  The big difference
between `yield' and a `return' statement is that on reaching a `yield'
the generator's state of execution is suspended and local variables are
preserved.  On the next call to the generator's `.next()' method, the
function will resume executing.

  Here's a sample usage of the `generate_ints()' generator:

    >>> gen = generate_ints(3)
    >>> gen
    <generator object generate_ints at ...>
    >>> gen.next()
    0
    >>> gen.next()
    1
    >>> gen.next()
    2
    >>> gen.next()
    Traceback (most recent call last):
      File "stdin", line 1, in ?
      File "stdin", line 2, in generate_ints
    StopIteration

You could equally write `for i in generate_ints(5)', or `a,b,c =
generate_ints(3)'.

  Inside a generator function, the `return' statement can only be used
without a value, and signals the end of the procession of values; after
executing a `return' the generator cannot return any further values.
`return' with a value, such as `return 5', is a syntax error inside a
generator function.  The end of the generator's results can also be
indicated by raising `StopIteration' manually, or by just letting the
flow of execution fall off the bottom of the function.

  You could achieve the effect of generators manually by writing your
own class and storing all the local variables of the generator as
instance variables.  For example, returning a list of integers could be
done by setting `self.count' to 0, and having the `next()' method
increment `self.count' and return it.  However, for a moderately
complicated generator, writing a corresponding class can be much
messier.

  The test suite included with Python's library, `test_generators.py',
contains a number of more interesting examples.  Here's one generator
that implements an in-order traversal of a tree using generators
recursively.

    # A recursive generator that generates Tree leaves in in-order.
    def inorder(t):
        if t:
            for x in inorder(t.left):
                yield x

            yield t.label

            for x in inorder(t.right):
                yield x

Two other examples in `test_generators.py' produce solutions for the
N-Queens problem (placing N queens on an NxN chess board so that no
queen threatens another) and the Knight's Tour (finding a route that
takes a knight to every square of an NxN chessboard without visiting
any square twice).

* Menu:

* Passing values into a generator::


File: python.info,  Node: Passing values into a generator,  Up: Generators<2>

10.6.4.1 Passing values into a generator
........................................

In Python 2.4 and earlier, generators only produced output.  Once a
generator's code was invoked to create an iterator, there was no way to
pass any new information into the function when its execution is
resumed.  You could hack together this ability by making the generator
look at a global variable or by passing in some mutable object that
callers then modify, but these approaches are messy.

  In Python 2.5 there's a simple way to pass values into a generator.
*note yield: 2f0. became an expression, returning a value that can be
assigned to a variable or otherwise operated on:

    val = (yield i)

I recommend that you *always* put parentheses around a `yield'
expression when you're doing something with the returned value, as in
the above example.  The parentheses aren't always necessary, but it's
easier to always add them instead of having to remember when they're
needed.

  (PEP 342 explains the exact rules, which are that a
`yield'-expression must always be parenthesized except when it occurs
at the top-level expression on the right-hand side of an assignment.
This means you can write `val = yield i' but have to use parentheses
when there's an operation, as in `val = (yield i) + 12'.)

  Values are sent into a generator by calling its `send(value)' method.
This method resumes the generator's code and the `yield' expression
returns the specified value.  If the regular `next()' method is called,
the `yield' returns `None'.

  Here's a simple counter that increments by 1 and allows changing the
value of the internal counter.

    def counter (maximum):
        i = 0
        while i < maximum:
            val = (yield i)
            # If value provided, change counter
            if val is not None:
                i = val
            else:
                i += 1

And here's an example of changing the counter:

    >>> it = counter(10)
    >>> print it.next()
    0
    >>> print it.next()
    1
    >>> print it.send(8)
    8
    >>> print it.next()
    9
    >>> print it.next()
    Traceback (most recent call last):
      File "t.py", line 15, in ?
        print it.next()
    StopIteration

Because `yield' will often be returning `None', you should always check
for this case.  Don't just use its value in expressions unless you're
sure that the `send()' method will be the only method used resume your
generator function.

  In addition to `send()', there are two other new methods on
generators:

   * `throw(type, value=None, traceback=None)' is used to raise an
     exception inside the generator; the exception is raised by the
     `yield' expression where the generator's execution is paused.

   * `close()' raises a *note GeneratorExit: 330. exception inside the
     generator to terminate the iteration.  On receiving this
     exception, the generator's code must either raise *note
     GeneratorExit: 330. or *note StopIteration: 32c.; catching the
     exception and doing anything else is illegal and will trigger a
     *note RuntimeError: 394.  `close()' will also be called by
     Python's garbage collector when the generator is garbage-collected.

     If you need to run cleanup code when a *note GeneratorExit: 330.
     occurs, I suggest using a `try: ... finally:' suite instead of
     catching *note GeneratorExit: 330.

  The cumulative effect of these changes is to turn generators from
one-way producers of information into both producers and consumers.

  Generators also become *coroutines*, a more generalized form of
subroutines.  Subroutines are entered at one point and exited at
another point (the top of the function, and a `return' statement), but
coroutines can be entered, exited, and resumed at many different points
(the `yield' statements).


File: python.info,  Node: Built-in functions,  Next: Small functions and the lambda expression,  Prev: Generators<2>,  Up: Functional Programming HOWTO

10.6.5 Built-in functions
-------------------------

Let's look in more detail at built-in functions often used with
iterators.

  Two of Python's built-in functions, *note map(): 2fd. and *note
filter(): 402, are somewhat obsolete; they duplicate the features of
list comprehensions but return actual lists instead of iterators.

  `map(f, iterA, iterB, ...)' returns a list containing `f(iterA[0],
iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]), ...'.

    >>> def upper(s):
    ...     return s.upper()


    >>> map(upper, ['sentence', 'fragment'])
    ['SENTENCE', 'FRAGMENT']


    >>> [upper(s) for s in ['sentence', 'fragment']]
    ['SENTENCE', 'FRAGMENT']

As shown above, you can achieve the same effect with a list
comprehension.  The *note itertools.imap(): d47. function does the same
thing but can handle infinite iterators; it'll be discussed later, in
the section on the *note itertools: fa. module.

  `filter(predicate, iter)' returns a list that contains all the
sequence elements that meet a certain condition, and is similarly
duplicated by list comprehensions.  A *predicate* is a function that
returns the truth value of some condition; for use with *note filter():
402, the predicate must take a single value.

    >>> def is_even(x):
    ...     return (x % 2) == 0


    >>> filter(is_even, range(10))
    [0, 2, 4, 6, 8]

This can also be written as a list comprehension:

    >>> [x for x in range(10) if is_even(x)]
    [0, 2, 4, 6, 8]

*note filter(): 402. also has a counterpart in the *note itertools: fa.
module, *note itertools.ifilter(): 86b, that returns an iterator and
can therefore handle infinite sequences just as *note itertools.imap():
d47. can.

  `reduce(func, iter, [initial_value])' doesn't have a counterpart in
the *note itertools: fa. module because it cumulatively performs an
operation on all the iterable's elements and therefore can't be applied
to infinite iterables.  `func' must be a function that takes two
elements and returns a single value.  *note reduce(): 2e2. takes the
first two elements A and B returned by the iterator and calculates
`func(A, B)'.  It then requests the third element, C, calculates
`func(func(A, B), C)', combines this result with the fourth element
returned, and continues until the iterable is exhausted.  If the
iterable returns no values at all, a *note TypeError: 215. exception is
raised.  If the initial value is supplied, it's used as a starting
point and `func(initial_value, A)' is the first calculation.

    >>> import operator
    >>> reduce(operator.concat, ['A', 'BB', 'C'])
    'ABBC'
    >>> reduce(operator.concat, [])
    Traceback (most recent call last):
      ...
    TypeError: reduce() of empty sequence with no initial value
    >>> reduce(operator.mul, [1,2,3], 1)
    6
    >>> reduce(operator.mul, [], 1)
    1

If you use *note operator.add(): d75. with *note reduce(): 2e2, you'll
add up all the elements of the iterable.  This case is so common that
there's a special built-in called *note sum(): 41f. to compute it:

    >>> reduce(operator.add, [1,2,3,4], 0)
    10
    >>> sum([1,2,3,4])
    10
    >>> sum([])
    0

For many uses of *note reduce(): 2e2, though, it can be clearer to just
write the obvious *note for: 2e9. loop:

    # Instead of:
    product = reduce(operator.mul, [1,2,3], 1)

    # You can write:
    product = 1
    for i in [1,2,3]:
        product *= i

`enumerate(iter)' counts off the elements in the iterable, returning
2-tuples containing the count and each element.

    >>> for item in enumerate(['subject', 'verb', 'object']):
    ...     print item
    (0, 'subject')
    (1, 'verb')
    (2, 'object')

*note enumerate(): 420. is often used when looping through a list and
recording the indexes at which certain conditions are met:

    f = open('data.txt', 'r')
    for i, line in enumerate(f):
        if line.strip() == '':
            print 'Blank line at line #%i' % i

`sorted(iterable, [cmp=None], [key=None], [reverse=False])' collects
all the elements of the iterable into a list, sorts the list, and
returns the sorted result.  The `cmp', `key', and `reverse' arguments
are passed through to the constructed list's `.sort()' method.

    >>> import random
    >>> # Generate 8 random numbers between [0, 10000)
    >>> rand_list = random.sample(range(10000), 8)
    >>> rand_list
    [769, 7953, 9828, 6431, 8442, 9878, 6213, 2207]
    >>> sorted(rand_list)
    [769, 2207, 6213, 6431, 7953, 8442, 9828, 9878]
    >>> sorted(rand_list, reverse=True)
    [9878, 9828, 8442, 7953, 6431, 6213, 2207, 769]

(For a more detailed discussion of sorting, see the Sorting mini-HOWTO
in the Python wiki at <http://wiki.python.org/moin/HowTo/Sorting>.)

  The `any(iter)' and `all(iter)' built-ins look at the truth values of
an iterable's contents.  *note any(): 3a7. returns True if any element
in the iterable is a true value, and *note all(): 3a8. returns True if
all of the elements are true values:

    >>> any([0,1,0])
    True
    >>> any([0,0,0])
    False
    >>> any([1,1,1])
    True
    >>> all([0,1,0])
    False
    >>> all([0,0,0])
    False
    >>> all([1,1,1])
    True



File: python.info,  Node: Small functions and the lambda expression,  Next: The itertools module,  Prev: Built-in functions,  Up: Functional Programming HOWTO

10.6.6 Small functions and the lambda expression
------------------------------------------------

When writing functional-style programs, you'll often need little
functions that act as predicates or that combine elements in some way.

  If there's a Python built-in or a module function that's suitable,
you don't need to define a new function at all:

    stripped_lines = [line.strip() for line in lines]
    existing_files = filter(os.path.exists, file_list)

If the function you need doesn't exist, you need to write it.  One way
to write small functions is to use the `lambda' statement.  `lambda'
takes a number of parameters and an expression combining these
parameters, and creates a small function that returns the value of the
expression:

    lowercase = lambda x: x.lower()

    print_assign = lambda name, value: name + '=' + str(value)

    adder = lambda x, y: x+y

An alternative is to just use the `def' statement and define a function
in the usual way:

    def lowercase(x):
        return x.lower()

    def print_assign(name, value):
        return name + '=' + str(value)

    def adder(x,y):
        return x + y

Which alternative is preferable?  That's a style question; my usual
course is to avoid using `lambda'.

  One reason for my preference is that `lambda' is quite limited in the
functions it can define.  The result has to be computable as a single
expression, which means you can't have multiway `if... elif... else'
comparisons or `try... except' statements.  If you try to do too much
in a `lambda' statement, you'll end up with an overly complicated
expression that's hard to read.  Quick, what's the following code doing?

    total = reduce(lambda a, b: (0, a[1] + b[1]), items)[1]

You can figure it out, but it takes time to disentangle the expression
to figure out what's going on.  Using a short nested `def' statements
makes things a little bit better:

    def combine (a, b):
        return 0, a[1] + b[1]

    total = reduce(combine, items)[1]

But it would be best of all if I had simply used a `for' loop:

    total = 0
    for a, b in items:
        total += b

Or the *note sum(): 41f. built-in and a generator expression:

    total = sum(b for a,b in items)

Many uses of *note reduce(): 2e2. are clearer when written as `for'
loops.

  Fredrik Lundh once suggested the following set of rules for
refactoring uses of `lambda':

  1. Write a lambda function.

  2. Write a comment explaining what the heck that lambda does.

  3. Study the comment for a while, and think of a name that captures
     the essence of the comment.

  4. Convert the lambda to a def statement, using that name.

  5. Remove the comment.

  I really like these rules, but you're free to disagree about whether
this lambda-free style is better.


File: python.info,  Node: The itertools module,  Next: The functools module,  Prev: Small functions and the lambda expression,  Up: Functional Programming HOWTO

10.6.7 The itertools module
---------------------------

The *note itertools: fa. module contains a number of commonly-used
iterators as well as functions for combining several iterators.  This
section will introduce the module's contents by showing small examples.

  The module's functions fall into a few broad classes:

   * Functions that create a new iterator based on an existing iterator.

   * Functions for treating an iterator's elements as function
     arguments.

   * Functions for selecting portions of an iterator's output.

   * A function for grouping an iterator's output.

* Menu:

* Creating new iterators::
* Calling functions on elements::
* Selecting elements::
* Grouping elements::


File: python.info,  Node: Creating new iterators,  Next: Calling functions on elements,  Up: The itertools module

10.6.7.1 Creating new iterators
...............................

`itertools.count(n)' returns an infinite stream of integers, increasing
by 1 each time.  You can optionally supply the starting number, which
defaults to 0:

    itertools.count() =>
      0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
    itertools.count(10) =>
      10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

`itertools.cycle(iter)' saves a copy of the contents of a provided
iterable and returns a new iterator that returns its elements from
first to last.  The new iterator will repeat these elements infinitely.

    itertools.cycle([1,2,3,4,5]) =>
      1, 2, 3, 4, 5, 1, 2, 3, 4, 5, ...

`itertools.repeat(elem, [n])' returns the provided element `n' times, or
returns the element endlessly if `n' is not provided.

    itertools.repeat('abc') =>
      abc, abc, abc, abc, abc, abc, abc, abc, abc, abc, ...
    itertools.repeat('abc', 5) =>
      abc, abc, abc, abc, abc

`itertools.chain(iterA, iterB, ...)' takes an arbitrary number of
iterables as input, and returns all the elements of the first iterator,
then all the elements of the second, and so on, until all of the
iterables have been exhausted.

    itertools.chain(['a', 'b', 'c'], (1, 2, 3)) =>
      a, b, c, 1, 2, 3

`itertools.izip(iterA, iterB, ...)' takes one element from each
iterable and returns them in a tuple:

    itertools.izip(['a', 'b', 'c'], (1, 2, 3)) =>
      ('a', 1), ('b', 2), ('c', 3)

It's similar to the built-in *note zip(): 3fe. function, but doesn't
construct an in-memory list and exhaust all the input iterators before
returning; instead tuples are constructed and returned only if they're
requested.  (The technical term for this behaviour is lazy
evaluation(1).)

  This iterator is intended to be used with iterables that are all of
the same length.  If the iterables are of different lengths, the
resulting stream will be the same length as the shortest iterable.

    itertools.izip(['a', 'b'], (1, 2, 3)) =>
      ('a', 1), ('b', 2)

You should avoid doing this, though, because an element may be taken
from the longer iterators and discarded.  This means you can't go on to
use the iterators further because you risk skipping a discarded element.

  `itertools.islice(iter, [start], stop, [step])' returns a stream
that's a slice of the iterator.  With a single `stop' argument, it will
return the first `stop' elements.  If you supply a starting index,
you'll get `stop-start' elements, and if you supply a value for `step',
elements will be skipped accordingly.  Unlike Python's string and list
slicing, you can't use negative values for `start', `stop', or `step'.

    itertools.islice(range(10), 8) =>
      0, 1, 2, 3, 4, 5, 6, 7
    itertools.islice(range(10), 2, 8) =>
      2, 3, 4, 5, 6, 7
    itertools.islice(range(10), 2, 8, 2) =>
      2, 4, 6

`itertools.tee(iter, [n])' replicates an iterator; it returns `n'
independent iterators that will all return the contents of the source
iterator.  If you don't supply a value for `n', the default is 2.
Replicating iterators requires saving some of the contents of the
source iterator, so this can consume significant memory if the iterator
is large and one of the new iterators is consumed more than the others.

    itertools.tee( itertools.count() ) =>
       iterA, iterB

    where iterA ->
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

    and   iterB ->
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...


  ---------- Footnotes ----------

  (1) http://en.wikipedia.org/wiki/Lazy_evaluation


File: python.info,  Node: Calling functions on elements,  Next: Selecting elements,  Prev: Creating new iterators,  Up: The itertools module

10.6.7.2 Calling functions on elements
......................................

Two functions are used for calling other functions on the contents of an
iterable.

  `itertools.imap(f, iterA, iterB, ...)' returns a stream containing
`f(iterA[0], iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]),
...':

    itertools.imap(operator.add, [5, 6, 5], [1, 2, 3]) =>
      6, 8, 8

The `operator' module contains a set of functions corresponding to
Python's operators.  Some examples are `operator.add(a, b)' (adds two
values), `operator.ne(a, b)' (same as `a!=b'), and
`operator.attrgetter('id')' (returns a callable that fetches the `"id"'
attribute).

  `itertools.starmap(func, iter)' assumes that the iterable will return
a stream of tuples, and calls `f()' using these tuples as the arguments:

    itertools.starmap(os.path.join,
                      [('/usr', 'bin', 'java'), ('/bin', 'python'),
                       ('/usr', 'bin', 'perl'),('/usr', 'bin', 'ruby')])
    =>
      /usr/bin/java, /bin/python, /usr/bin/perl, /usr/bin/ruby



File: python.info,  Node: Selecting elements,  Next: Grouping elements,  Prev: Calling functions on elements,  Up: The itertools module

10.6.7.3 Selecting elements
...........................

Another group of functions chooses a subset of an iterator's elements
based on a predicate.

  `itertools.ifilter(predicate, iter)' returns all the elements for
which the predicate returns true:

    def is_even(x):
        return (x % 2) == 0

    itertools.ifilter(is_even, itertools.count()) =>
      0, 2, 4, 6, 8, 10, 12, 14, ...

`itertools.ifilterfalse(predicate, iter)' is the opposite, returning all
elements for which the predicate returns false:

    itertools.ifilterfalse(is_even, itertools.count()) =>
      1, 3, 5, 7, 9, 11, 13, 15, ...

`itertools.takewhile(predicate, iter)' returns elements for as long as
the predicate returns true.  Once the predicate returns false, the
iterator will signal the end of its results.

    def less_than_10(x):
        return (x < 10)

    itertools.takewhile(less_than_10, itertools.count()) =>
      0, 1, 2, 3, 4, 5, 6, 7, 8, 9

    itertools.takewhile(is_even, itertools.count()) =>
      0

`itertools.dropwhile(predicate, iter)' discards elements while the
predicate returns true, and then returns the rest of the iterable's
results.

    itertools.dropwhile(less_than_10, itertools.count()) =>
      10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

    itertools.dropwhile(is_even, itertools.count()) =>
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...



File: python.info,  Node: Grouping elements,  Prev: Selecting elements,  Up: The itertools module

10.6.7.4 Grouping elements
..........................

The last function I'll discuss, `itertools.groupby(iter,
key_func=None)', is the most complicated.  `key_func(elem)' is a
function that can compute a key value for each element returned by the
iterable.  If you don't supply a key function, the key is simply each
element itself.

  `groupby()' collects all the consecutive elements from the underlying
iterable that have the same key value, and returns a stream of 2-tuples
containing a key value and an iterator for the elements with that key.

    city_list = [('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL'),
                 ('Anchorage', 'AK'), ('Nome', 'AK'),
                 ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ'),
                 ...
                ]

    def get_state ((city, state)):
        return state

    itertools.groupby(city_list, get_state) =>
      ('AL', iterator-1),
      ('AK', iterator-2),
      ('AZ', iterator-3), ...

    where
    iterator-1 =>
      ('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL')
    iterator-2 =>
      ('Anchorage', 'AK'), ('Nome', 'AK')
    iterator-3 =>
      ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ')

`groupby()' assumes that the underlying iterable's contents will
already be sorted based on the key.  Note that the returned iterators
also use the underlying iterable, so you have to consume the results of
iterator-1 before requesting iterator-2 and its corresponding key.


File: python.info,  Node: The functools module,  Next: Revision History and Acknowledgements,  Prev: The itertools module,  Up: Functional Programming HOWTO

10.6.8 The functools module
---------------------------

The *note functools: d9. module in Python 2.5 contains some
higher-order functions.  A *higher-order function* takes one or more
functions as input and returns a new function.  The most useful tool in
this module is the *note functools.partial(): d58. function.

  For programs written in a functional style, you'll sometimes want to
construct variants of existing functions that have some of the
parameters filled in.  Consider a Python function `f(a, b, c)'; you may
wish to create a new function `g(b, c)' that's equivalent to `f(1, b,
c)'; you're filling in a value for one of `f()''s parameters.  This is
called "partial function application".

  The constructor for `partial' takes the arguments `(function, arg1,
arg2, ... kwarg1=value1, kwarg2=value2)'.  The resulting object is
callable, so you can just call it to invoke `function' with the
filled-in arguments.

  Here's a small but realistic example:

    import functools

    def log (message, subsystem):
        "Write the contents of 'message' to the specified subsystem."
        print '%s: %s' % (subsystem, message)
        ...

    server_log = functools.partial(log, subsystem='server')
    server_log('Unable to open socket')


* Menu:

* The operator module::


File: python.info,  Node: The operator module,  Up: The functools module

10.6.8.1 The operator module
............................

The *note operator: 126. module was mentioned earlier.  It contains a
set of functions corresponding to Python's operators.  These functions
are often useful in functional-style code because they save you from
writing trivial functions that perform a single operation.

  Some of the functions in this module are:

   * Math operations: `add()', `sub()', `mul()', `div()', `floordiv()',
     `abs()', ...

   * Logical operations: `not_()', `truth()'.

   * Bitwise operations: `and_()', `or_()', `invert()'.

   * Comparisons: `eq()', `ne()', `lt()', `le()', `gt()', and `ge()'.

   * Object identity: `is_()', `is_not()'.

  Consult the operator module's documentation for a complete list.


File: python.info,  Node: Revision History and Acknowledgements,  Next: References,  Prev: The functools module,  Up: Functional Programming HOWTO

10.6.9 Revision History and Acknowledgements
--------------------------------------------

The author would like to thank the following people for offering
suggestions, corrections and assistance with various drafts of this
article: Ian Bicking, Nick Coghlan, Nick Efford, Raymond Hettinger, Jim
Jewett, Mike Krell, Leandro Lameiro, Jussi Salmela, Collin Winter,
Blake Winton.

  Version 0.1: posted June 30 2006.

  Version 0.11: posted July 1 2006.  Typo fixes.

  Version 0.2: posted July 10 2006.  Merged genexp and listcomp
sections into one.  Typo fixes.

  Version 0.21: Added more references suggested on the tutor mailing
list.

  Version 0.30: Adds a section on the `functional' module written by
Collin Winter; adds short section on the operator module; a few other
edits.


File: python.info,  Node: References,  Prev: Revision History and Acknowledgements,  Up: Functional Programming HOWTO

10.6.10 References
------------------

* Menu:

* General::
* Python-specific::
* Python documentation::


File: python.info,  Node: General,  Next: Python-specific,  Up: References

10.6.10.1 General
.................

*Structure and Interpretation of Computer Programs*, by Harold Abelson
and Gerald Jay Sussman with Julie Sussman.  Full text at
<http://mitpress.mit.edu/sicp/>.  In this classic textbook of computer
science, chapters 2 and 3 discuss the use of sequences and streams to
organize the data flow inside a program.  The book uses Scheme for its
examples, but many of the design approaches described in these chapters
are applicable to functional-style Python code.

  <http://www.defmacro.org/ramblings/fp.html>: A general introduction
to functional programming that uses Java examples and has a lengthy
historical introduction.

  <http://en.wikipedia.org/wiki/Functional_programming>: General
Wikipedia entry describing functional programming.

  <http://en.wikipedia.org/wiki/Coroutine>: Entry for coroutines.

  <http://en.wikipedia.org/wiki/Currying>: Entry for the concept of
currying.


File: python.info,  Node: Python-specific,  Next: Python documentation,  Prev: General,  Up: References

10.6.10.2 Python-specific
.........................

<http://gnosis.cx/TPiP/>: The first chapter of David Mertz's book `Text
Processing in Python' discusses functional programming for text
processing, in the section titled "Utilizing Higher-Order Functions in
Text Processing".

  Mertz also wrote a 3-part series of articles on functional programming
for IBM's DeveloperWorks site; see

  part 1(1), part 2(2), and part 3(3),

  ---------- Footnotes ----------

  (1) http://www.ibm.com/developerworks/linux/library/l-prog/index.html

  (2) http://www.ibm.com/developerworks/linux/library/l-prog2/index.html

  (3) http://www.ibm.com/developerworks/linux/library/l-prog3/index.html


File: python.info,  Node: Python documentation,  Prev: Python-specific,  Up: References

10.6.10.3 Python documentation
..............................

Documentation for the *note itertools: fa. module.

  Documentation for the *note operator: 126. module.

  PEP 289(1): "Generator Expressions"

  PEP 342(2): "Coroutines via Enhanced Generators" describes the new
generator features in Python 2.5.

  ---------- Footnotes ----------

  (1) http://www.python.org/dev/peps/pep-0289

  (2) http://www.python.org/dev/peps/pep-0342


File: python.info,  Node: Logging HOWTO,  Next: Logging Cookbook,  Prev: Functional Programming HOWTO,  Up: Python HOWTOs

10.7 Logging HOWTO
==================

     Author: Vinay Sajip <vinay_sajip at red-dove dot com>

* Menu:

* Basic Logging Tutorial::
* Advanced Logging Tutorial::
* Logging Levels::
* Useful Handlers::
* Exceptions raised during logging::
* Using arbitrary objects as messages::
* Optimization::

Basic Logging Tutorial

* When to use logging::
* A simple example::
* Logging to a file::
* Logging from multiple modules::
* Logging variable data::
* Changing the format of displayed messages::
* Displaying the date/time in messages::
* Next Steps::

Advanced Logging Tutorial

* Logging Flow::
* Loggers::
* Handlers::
* Formatters::
* Configuring Logging::
* What happens if no configuration is provided::
* Configuring Logging for a Library::

Logging Levels

* Custom Levels::


File: python.info,  Node: Basic Logging Tutorial,  Next: Advanced Logging Tutorial,  Up: Logging HOWTO

10.7.1 Basic Logging Tutorial
-----------------------------

Logging is a means of tracking events that happen when some software
runs. The software's developer adds logging calls to their code to
indicate that certain events have occurred. An event is described by a
descriptive message which can optionally contain variable data (i.e.
data that is potentially different for each occurrence of the event).
Events also have an importance which the developer ascribes to the
event; the importance can also be called the _level_ or _severity_.

* Menu:

* When to use logging::
* A simple example::
* Logging to a file::
* Logging from multiple modules::
* Logging variable data::
* Changing the format of displayed messages::
* Displaying the date/time in messages::
* Next Steps::


File: python.info,  Node: When to use logging,  Next: A simple example,  Up: Basic Logging Tutorial

10.7.1.1 When to use logging
............................

Logging provides a set of convenience functions for simple logging
usage. These are *note debug(): 12a5, *note info(): 12d1, *note
warning(): 12dd, *note error(): 12de. and *note critical(): 12e0. To
determine when to use logging, see the table below, which states, for
each of a set of common tasks, the best tool to use for it.

Task you want to perform                  The best tool for the task
------------------------------------------------------------------------------------- 
Display console output for ordinary       *note print(): 304.
usage of a command line script or program 
Report events that occur during normal    *note logging.info(): 12d1. (or *note
operation of a program (e.g.  for status  logging.debug(): 12a5. for very detailed
monitoring or fault investigation)        output for diagnostic purposes)
Issue a warning regarding a particular    *note warnings.warn(): 4b7. in library
runtime event                             code if the issue is avoidable and the
                                          client application should be modified to
                                          eliminate the warning
                                          
                                            *note logging.warning(): 12dd. if there
                                          is nothing the client application can do
                                          about the situation, but the event should
                                          still be noted
Report an error regarding a particular    Raise an exception
runtime event                             
Report suppression of an error without    *note logging.error(): 12de, *note
raising an exception (e.g.  error         logging.exception(): 12df. or *note
handler in a long-running server process) logging.critical(): 12e0. as appropriate
                                          for the specific error and application
                                          domain

  The logging functions are named after the level or severity of the
events they are used to track. The standard levels and their
applicability are described below (in increasing order of severity):

Level              When it's used
--------------------------------------------------------------------- 
`DEBUG'            Detailed information, typically of interest only
                   when diagnosing problems.
`INFO'             Confirmation that things are working as expected.
`WARNING'          An indication that something unexpected
                   happened, or indicative of some problem in the
                   near future (e.g. 'disk space low').  The
                   software is still working as expected.
`ERROR'            Due to a more serious problem, the software has
                   not been able to perform some function.
`CRITICAL'         A serious error, indicating that the program
                   itself may be unable to continue running.

  The default level is `WARNING', which means that only events of this
level and above will be tracked, unless the logging package is
configured to do otherwise.

  Events that are tracked can be handled in different ways. The
simplest way of handling tracked events is to print them to the
console. Another common way is to write them to a disk file.


File: python.info,  Node: A simple example,  Next: Logging to a file,  Prev: When to use logging,  Up: Basic Logging Tutorial

10.7.1.2 A simple example
.........................

A very simple example is:

    import logging
    logging.warning('Watch out!') # will print a message to the console
    logging.info('I told you so') # will not print anything

If you type these lines into a script and run it, you'll see:

    WARNING:root:Watch out!

printed out on the console. The `INFO' message doesn't appear because
the default level is `WARNING'. The printed message includes the
indication of the level and the description of the event provided in
the logging call, i.e.  'Watch out!'. Don't worry about the 'root' part
for now: it will be explained later. The actual output can be formatted
quite flexibly if you need that; formatting options will also be
explained later.


File: python.info,  Node: Logging to a file,  Next: Logging from multiple modules,  Prev: A simple example,  Up: Basic Logging Tutorial

10.7.1.3 Logging to a file
..........................

A very common situation is that of recording logging events in a file,
so let's look at that next:

    import logging
    logging.basicConfig(filename='example.log',level=logging.DEBUG)
    logging.debug('This message should go to the log file')
    logging.info('So should this')
    logging.warning('And this, too')

And now if we open the file and look at what we have, we should find
the log messages:

    DEBUG:root:This message should go to the log file
    INFO:root:So should this
    WARNING:root:And this, too

This example also shows how you can set the logging level which acts as
the threshold for tracking. In this case, because we set the threshold
to `DEBUG', all of the messages were printed.

  If you want to set the logging level from a command-line option such
as:

    --log=INFO

and you have the value of the parameter passed for `--log' in some
variable _loglevel_, you can use:

    getattr(logging, loglevel.upper())

to get the value which you'll pass to *note basicConfig(): 12e6. via
the _level_ argument. You may want to error check any user input value,
perhaps as in the following example:

    # assuming loglevel is bound to the string value obtained from the
    # command line argument. Convert to upper case to allow the user to
    # specify --log=DEBUG or --log=debug
    numeric_level = getattr(logging, loglevel.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: %s' % loglevel)
    logging.basicConfig(level=numeric_level, ...)

The call to *note basicConfig(): 12e6. should come _before_ any calls
to *note debug(): 12a5, *note info(): 12d1. etc. As it's intended as a
one-off simple configuration facility, only the first call will
actually do anything: subsequent calls are effectively no-ops.

  If you run the above script several times, the messages from
successive runs are appended to the file _example.log_. If you want
each run to start afresh, not remembering the messages from earlier
runs, you can specify the _filemode_ argument, by changing the call in
the above example to:

    logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)

The output will be the same as before, but the log file is no longer
appended to, so the messages from earlier runs are lost.


File: python.info,  Node: Logging from multiple modules,  Next: Logging variable data,  Prev: Logging to a file,  Up: Basic Logging Tutorial

10.7.1.4 Logging from multiple modules
......................................

If your program consists of multiple modules, here's an example of how
you could organize logging in it:

    # myapp.py
    import logging
    import mylib

    def main():
        logging.basicConfig(filename='myapp.log', level=logging.INFO)
        logging.info('Started')
        mylib.do_something()
        logging.info('Finished')

    if __name__ == '__main__':
        main()


    # mylib.py
    import logging

    def do_something():
        logging.info('Doing something')

If you run _myapp.py_, you should see this in _myapp.log_:

    INFO:root:Started
    INFO:root:Doing something
    INFO:root:Finished

which is hopefully what you were expecting to see. You can generalize
this to multiple modules, using the pattern in _mylib.py_. Note that
for this simple usage pattern, you won't know, by looking in the log
file, _where_ in your application your messages came from, apart from
looking at the event description. If you want to track the location of
your messages, you'll need to refer to the documentation beyond the
tutorial level - see *note Advanced Logging Tutorial: 1299.


File: python.info,  Node: Logging variable data,  Next: Changing the format of displayed messages,  Prev: Logging from multiple modules,  Up: Basic Logging Tutorial

10.7.1.5 Logging variable data
..............................

To log variable data, use a format string for the event description
message and append the variable data as arguments. For example:

    import logging
    logging.warning('%s before you %s', 'Look', 'leap!')

will display:

    WARNING:root:Look before you leap!

As you can see, merging of variable data into the event description
message uses the old, %-style of string formatting. This is for
backwards compatibility: the logging package pre-dates newer formatting
options such as *note str.format(): 1cf. and *note string.Template:
58a. These newer formatting options _are_ supported, but exploring them
is outside the scope of this tutorial.


File: python.info,  Node: Changing the format of displayed messages,  Next: Displaying the date/time in messages,  Prev: Logging variable data,  Up: Basic Logging Tutorial

10.7.1.6 Changing the format of displayed messages
..................................................

To change the format which is used to display messages, you need to
specify the format you want to use:

    import logging
    logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
    logging.debug('This message should appear on the console')
    logging.info('So should this')
    logging.warning('And this, too')

which would print:

    DEBUG:This message should appear on the console
    INFO:So should this
    WARNING:And this, too

Notice that the 'root' which appeared in earlier examples has
disappeared. For a full set of things that can appear in format
strings, you can refer to the documentation for *note LogRecord
attributes: 12c8, but for simple usage, you just need the _levelname_
(severity), _message_ (event description, including variable data) and
perhaps to display when the event occurred. This is described in the
next section.


File: python.info,  Node: Displaying the date/time in messages,  Next: Next Steps,  Prev: Changing the format of displayed messages,  Up: Basic Logging Tutorial

10.7.1.7 Displaying the date/time in messages
.............................................

To display the date and time of an event, you would place '%(asctime)s'
in your format string:

    import logging
    logging.basicConfig(format='%(asctime)s %(message)s')
    logging.warning('is when this event was logged.')

which should print something like this:

    2010-12-12 11:41:42,612 is when this event was logged.

The default format for date/time display (shown above) is ISO8601. If
you need more control over the formatting of the date/time, provide a
_datefmt_ argument to `basicConfig', as in this example:

    import logging
    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
    logging.warning('is when this event was logged.')

which would display something like this:

    12/12/2010 11:46:36 AM is when this event was logged.

The format of the _datefmt_ argument is the same as supported by *note
time.strftime(): 3b7.


File: python.info,  Node: Next Steps,  Prev: Displaying the date/time in messages,  Up: Basic Logging Tutorial

10.7.1.8 Next Steps
...................

That concludes the basic tutorial. It should be enough to get you up and
running with logging. There's a lot more that the logging package
offers, but to get the best out of it, you'll need to invest a little
more of your time in reading the following sections. If you're ready
for that, grab some of your favourite beverage and carry on.

  If your logging needs are simple, then use the above examples to
incorporate logging into your own scripts, and if you run into problems
or don't understand something, please post a question on the
comp.lang.python Usenet group (available at
<http://groups.google.com/group/comp.lang.python>) and you should
receive help before too long.

  Still here? You can carry on reading the next few sections, which
provide a slightly more advanced/in-depth tutorial than the basic one
above. After that, you can take a look at the *note Logging Cookbook:
129a.


File: python.info,  Node: Advanced Logging Tutorial,  Next: Logging Levels,  Prev: Basic Logging Tutorial,  Up: Logging HOWTO

10.7.2 Advanced Logging Tutorial
--------------------------------

The logging library takes a modular approach and offers several
categories of components: loggers, handlers, filters, and formatters.

   * Loggers expose the interface that application code directly uses.

   * Handlers send the log records (created by loggers) to the
     appropriate destination.

   * Filters provide a finer grained facility for determining which log
     records to output.

   * Formatters specify the layout of log records in the final output.

  Log event information is passed between loggers, handlers, filters and
formatters in a *note LogRecord: 12b3. instance.

  Logging is performed by calling methods on instances of the *note
Logger: 1da.  class (hereafter called _loggers_). Each instance has a
name, and they are conceptually arranged in a namespace hierarchy using
dots (periods) as separators. For example, a logger named 'scan' is the
parent of loggers 'scan.text', 'scan.html' and 'scan.pdf'. Logger names
can be anything you want, and indicate the area of an application in
which a logged message originates.

  A good convention to use when naming loggers is to use a module-level
logger, in each module which uses logging, named as follows:

    logger = logging.getLogger(__name__)

This means that logger names track the package/module hierarchy, and
it's intuitively obvious where events are logged just from the logger
name.

  The root of the hierarchy of loggers is called the root logger.
That's the logger used by the functions *note debug(): 12a5, *note
info(): 12d1, *note warning(): 12dd, *note error(): 12de. and *note
critical(): 12e0, which just call the same-named method of the root
logger. The functions and the methods have the same signatures. The
root logger's name is printed as 'root' in the logged output.

  It is, of course, possible to log messages to different destinations.
Support is included in the package for writing log messages to files,
HTTP GET/POST locations, email via SMTP, generic sockets, or
OS-specific logging mechanisms such as syslog or the Windows NT event
log. Destinations are served by _handler_ classes. You can create your
own log destination class if you have special requirements not met by
any of the built-in handler classes.

  By default, no destination is set for any logging messages. You can
specify a destination (such as console or file) by using *note
basicConfig(): 12e6. as in the tutorial examples. If you call the
functions  *note debug(): 12a5, *note info(): 12d1, *note warning():
12dd, *note error(): 12de. and *note critical(): 12e0, they will check
to see if no destination is set; and if one is not set, they will set a
destination of the console (`sys.stderr') and a default format for the
displayed message before delegating to the root logger to do the actual
message output.

  The default format set by *note basicConfig(): 12e6. for messages is:

    severity:logger name:message

You can change this by passing a format string to *note basicConfig():
12e6. with the _format_ keyword argument. For all options regarding how
a format string is constructed, see *note Formatter Objects: 12c6.

* Menu:

* Logging Flow::
* Loggers::
* Handlers::
* Formatters::
* Configuring Logging::
* What happens if no configuration is provided::
* Configuring Logging for a Library::


File: python.info,  Node: Logging Flow,  Next: Loggers,  Up: Advanced Logging Tutorial

10.7.2.1 Logging Flow
.....................

The flow of log event information in loggers and handlers is
illustrated in the following diagram.


File: python.info,  Node: Loggers,  Next: Handlers,  Prev: Logging Flow,  Up: Advanced Logging Tutorial

10.7.2.2 Loggers
................

*note Logger: 1da. objects have a threefold job.  First, they expose
several methods to application code so that applications can log
messages at runtime.  Second, logger objects determine which log
messages to act upon based upon severity (the default filtering
facility) or filter objects.  Third, logger objects pass along relevant
log messages to all interested log handlers.

  The most widely used methods on logger objects fall into two
categories: configuration and message sending.

  These are the most common configuration methods:

   * *note Logger.setLevel(): 129f. specifies the lowest-severity log
     message a logger will handle, where debug is the lowest built-in
     severity level and critical is the highest built-in severity.  For
     example, if the severity level is INFO, the logger will handle
     only INFO, WARNING, ERROR, and CRITICAL messages and will ignore
     DEBUG messages.

   * *note Logger.addHandler(): 12ae. and *note Logger.removeHandler():
     12af. add and remove handler objects from the logger object.
     Handlers are covered in more detail in *note Handlers: 3002.

   * *note Logger.addFilter(): 12ab. and *note Logger.removeFilter():
     12ac. add and remove filter objects from the logger object.
     Filters are covered in more detail in *note Filter Objects: 12cd.

  You don't need to always call these methods on every logger you
create. See the last two paragraphs in this section.

  With the logger object configured, the following methods create log
messages:

   * *note Logger.debug(): 12a2, *note Logger.info(): 12a4, *note
     Logger.warning(): 12a6, *note Logger.error(): 12a7, and *note
     Logger.critical(): 12a8. all create log records with a message and
     a level that corresponds to their respective method names. The
     message is actually a format string, which may contain the
     standard string substitution syntax of `%s', `%d', `%f', and so
     on.  The rest of their arguments is a list of objects that
     correspond with the substitution fields in the message.  With
     regard to `**kwargs', the logging methods care only about a
     keyword of `exc_info' and use it to determine whether to log
     exception information.

   * *note Logger.exception(): 12aa. creates a log message similar to
     *note Logger.error(): 12a7.  The difference is that *note
     Logger.exception(): 12aa. dumps a stack trace along with it.  Call
     this method only from an exception handler.

   * *note Logger.log(): 12a9. takes a log level as an explicit
     argument.  This is a little more verbose for logging messages than
     using the log level convenience methods listed above, but this is
     how to log at custom log levels.

  *note getLogger(): 129d. returns a reference to a logger instance
with the specified name if it is provided, or `root' if not.  The names
are period-separated hierarchical structures.  Multiple calls to *note
getLogger(): 129d. with the same name will return a reference to the
same logger object.  Loggers that are further down in the hierarchical
list are children of loggers higher up in the list.  For example, given
a logger with a name of `foo', loggers with names of `foo.bar',
`foo.bar.baz', and `foo.bam' are all descendants of `foo'.

  Loggers have a concept of _effective level_. If a level is not
explicitly set on a logger, the level of its parent is used instead as
its effective level.  If the parent has no explicit level set, _its_
parent is examined, and so on - all ancestors are searched until an
explicitly set level is found. The root logger always has an explicit
level set (`WARNING' by default). When deciding whether to process an
event, the effective level of the logger is used to determine whether
the event is passed to the logger's handlers.

  Child loggers propagate messages up to the handlers associated with
their ancestor loggers. Because of this, it is unnecessary to define
and configure handlers for all the loggers an application uses. It is
sufficient to configure handlers for a top-level logger and create
child loggers as needed.  (You can, however, turn off propagation by
setting the _propagate_ attribute of a logger to _False_.)


File: python.info,  Node: Handlers,  Next: Formatters,  Prev: Loggers,  Up: Advanced Logging Tutorial

10.7.2.3 Handlers
.................

`Handler' objects are responsible for dispatching the appropriate log
messages (based on the log messages' severity) to the handler's
specified destination.  Logger objects can add zero or more handler
objects to themselves with an `addHandler()' method.  As an example
scenario, an application may want to send all log messages to a log
file, all log messages of error or higher to stdout, and all messages
of critical to an email address.  This scenario requires three
individual handlers where each handler is responsible for sending
messages of a specific severity to a specific location.

  The standard library includes quite a few handler types (see *note
Useful Handlers: 3004.); the tutorials use mainly *note StreamHandler:
12eb. and *note FileHandler: 130b. in its examples.

  There are very few methods in a handler for application developers to
concern themselves with.  The only handler methods that seem relevant
for application developers who are using the built-in handler objects
(that is, not creating custom handlers) are the following configuration
methods:

   * The *note Handler.setLevel(): 12ba. method, just as in logger
     objects, specifies the lowest severity that will be dispatched to
     the appropriate destination.  Why are there two `setLevel()'
     methods?  The level set in the logger determines which severity of
     messages it will pass to its handlers.  The level set in each
     handler determines which messages that handler will send on.

   * `setFormatter()' selects a Formatter object for this handler to
     use.

   * `addFilter()' and `removeFilter()' respectively configure and
     deconfigure filter objects on handlers.

  Application code should not directly instantiate and use instances of
`Handler'.  Instead, the `Handler' class is a base class that defines
the interface that all handlers should have and establishes some
default behavior that child classes can use (or override).


File: python.info,  Node: Formatters,  Next: Configuring Logging,  Prev: Handlers,  Up: Advanced Logging Tutorial

10.7.2.4 Formatters
...................

Formatter objects configure the final order, structure, and contents of
the log message.  Unlike the base `logging.Handler' class, application
code may instantiate formatter classes, although you could likely
subclass the formatter if your application needs special behavior.  The
constructor takes two optional arguments - a message format string and
a date format string.

 -- Method: logging.Formatter.__init__ (fmt=None, datefmt=None)

  If there is no message format string, the default is to use the raw
message.  If there is no date format string, the default date format is:

    %Y-%m-%d %H:%M:%S

with the milliseconds tacked on at the end.

  The message format string uses `%(<dictionary key>)s' styled string
substitution; the possible keys are documented in *note LogRecord
attributes: 12c8.

  The following message format string will log the time in a
human-readable format, the severity of the message, and the contents of
the message, in that order:

    '%(asctime)s - %(levelname)s - %(message)s'

Formatters use a user-configurable function to convert the creation
time of a record to a tuple. By default, *note time.localtime(): ac2.
is used; to change this for a particular formatter instance, set the
`converter' attribute of the instance to a function with the same
signature as *note time.localtime(): ac2. or *note time.gmtime(): b34.
To change it for all formatters, for example if you want all logging
times to be shown in GMT, set the `converter' attribute in the
Formatter class (to `time.gmtime' for GMT display).


File: python.info,  Node: Configuring Logging,  Next: What happens if no configuration is provided,  Prev: Formatters,  Up: Advanced Logging Tutorial

10.7.2.5 Configuring Logging
............................

Programmers can configure logging in three ways:

  1. Creating loggers, handlers, and formatters explicitly using Python
     code that calls the configuration methods listed above.

  2. Creating a logging config file and reading it using the *note
     fileConfig(): 12f4.  function.

  3. Creating a dictionary of configuration information and passing it
     to the *note dictConfig(): 12f2. function.

  For the reference documentation on the last two options, see *note
Configuration functions: 1d6.  The following example configures a very
simple logger, a console handler, and a simple formatter using Python
code:

    import logging

    # create logger
    logger = logging.getLogger('simple_example')
    logger.setLevel(logging.DEBUG)

    # create console handler and set level to debug
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)

    # create formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # add formatter to ch
    ch.setFormatter(formatter)

    # add ch to logger
    logger.addHandler(ch)

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Running this module from the command line produces the following output:

    $ python simple_logging_module.py
    2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message
    2005-03-19 15:10:26,620 - simple_example - INFO - info message
    2005-03-19 15:10:26,695 - simple_example - WARNING - warn message
    2005-03-19 15:10:26,697 - simple_example - ERROR - error message
    2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message

The following Python module creates a logger, handler, and formatter
nearly identical to those in the example listed above, with the only
difference being the names of the objects:

    import logging
    import logging.config

    logging.config.fileConfig('logging.conf')

    # create logger
    logger = logging.getLogger('simpleExample')

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Here is the logging.conf file:

    [loggers]
    keys=root,simpleExample

    [handlers]
    keys=consoleHandler

    [formatters]
    keys=simpleFormatter

    [logger_root]
    level=DEBUG
    handlers=consoleHandler

    [logger_simpleExample]
    level=DEBUG
    handlers=consoleHandler
    qualname=simpleExample
    propagate=0

    [handler_consoleHandler]
    class=StreamHandler
    level=DEBUG
    formatter=simpleFormatter
    args=(sys.stdout,)

    [formatter_simpleFormatter]
    format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
    datefmt=

The output is nearly identical to that of the non-config-file-based
example:

    $ python simple_logging_config.py
    2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message
    2005-03-19 15:38:55,979 - simpleExample - INFO - info message
    2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message
    2005-03-19 15:38:56,055 - simpleExample - ERROR - error message
    2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message

You can see that the config file approach has a few advantages over the
Python code approach, mainly separation of configuration and code and
the ability of noncoders to easily modify the logging properties.

     Warning: The *note fileConfig(): 12f4. function takes a default
     parameter, `disable_existing_loggers', which defaults to `True'
     for reasons of backward compatibility. This may or may not be what
     you want, since it will cause any loggers existing before the
     *note fileConfig(): 12f4. call to be disabled unless they (or an
     ancestor) are explicitly named in the configuration.  Please refer
     to the reference documentation for more information, and specify
     `False' for this parameter if you wish.

     The dictionary passed to *note dictConfig(): 12f2. can also
     specify a Boolean value with key `disable_existing_loggers', which
     if not specified explicitly in the dictionary also defaults to
     being interpreted as `True'.  This leads to the logger-disabling
     behaviour described above, which may not be what you want - in
     which case, provide the key explicitly with a value of `False'.

  Note that the class names referenced in config files need to be
either relative to the logging module, or absolute values which can be
resolved using normal import mechanisms. Thus, you could use either
*note WatchedFileHandler: 131d. (relative to the logging module) or
`mypackage.mymodule.MyHandler' (for a class defined in package
`mypackage' and module `mymodule', where `mypackage' is available on
the Python import path).

  In Python 2.7, a new means of configuring logging has been
introduced, using dictionaries to hold configuration information. This
provides a superset of the functionality of the config-file-based
approach outlined above, and is the recommended configuration method
for new applications and deployments. Because a Python dictionary is
used to hold configuration information, and since you can populate that
dictionary using different means, you have more options for
configuration. For example, you can use a configuration file in JSON
format, or, if you have access to YAML processing functionality, a file
in YAML format, to populate the configuration dictionary. Or, of
course, you can construct the dictionary in Python code, receive it in
pickled form over a socket, or use whatever approach makes sense for
your application.

  Here's an example of the same configuration as above, in YAML format
for the new dictionary-based approach:

    version: 1
    formatters:
      simple:
        format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        level: DEBUG
        formatter: simple
        stream: ext://sys.stdout
    loggers:
      simpleExample:
        level: DEBUG
        handlers: [console]
        propagate: no
    root:
      level: DEBUG
      handlers: [console]

For more information about logging using a dictionary, see *note
Configuration functions: 1d6.



Local Variables:
coding: utf-8
End:
