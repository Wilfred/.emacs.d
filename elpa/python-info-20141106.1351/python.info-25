This is python.info, produced by makeinfo version 5.2 from python.texi.

     Python 2.7.8, November 06, 2014

     Georg Brandl

     Copyright © 1990-2014, Python Software Foundation

INFO-DIR-SECTION Programming
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY


   Generated by Sphinx 1.1.3.


File: python.info,  Node: from __future__ import unicode_literals,  Next: Bytes/string literals,  Prev: from __future__ import print_function,  Up: Try to Support Python 2 6 and Newer Only

10.1.3.6 ‘from __future__ import unicode_literals’
..................................................

If you choose to use this future statement then all string literals in
Python 2 will be assumed to be Unicode (as is already the case in Python
3).  If you choose not to use this future statement then you should mark
all of your text strings with a ‘u’ prefix and only support Python 3.3
or newer.  But you are *strongly* advised to do one or the other (six(1)
provides a function in case you don’t want to use the future statement
*and* you want to support Python 3.2 or older).

   ---------- Footnotes ----------

   (1) https://pypi.python.org/pypi/six


File: python.info,  Node: Bytes/string literals,  Next: from __future__ import absolute_import,  Prev: from __future__ import unicode_literals,  Up: Try to Support Python 2 6 and Newer Only

10.1.3.7 Bytes/string literals
..............................

This is a *very* important one.  Prefix Python 2 strings that are meant
to contain bytes with a ‘b’ prefix to very clearly delineate what is and
is not a Python 3 text string (six(1) provides a function to use for
Python 2.5 compatibility).

  This point cannot be stressed enough: make sure you know what all of
your string literals in Python 2 are meant to be in Python 3.  Any
string literal that should be treated as bytes should have the ‘b’
prefix.  Any string literal that should be Unicode/text in Python 2
should either have the ‘u’ literal (supported, but ignored, in Python
3.3 and later) or you should have ‘from __future__ import
unicode_literals’ at the top of the file.  But the key point is you
should know how Python 3 will treat every one one of your string
literals and you should mark them as appropriate.

  There are some differences between byte literals in Python 2 and those
in Python 3 thanks to the bytes type just being an alias to ‘str’ in
Python 2.  See the *note Handle Common "Gotchas": 2faf. section for what
to watch out for.

   ---------- Footnotes ----------

   (1) https://pypi.python.org/pypi/six


File: python.info,  Node: from __future__ import absolute_import,  Prev: Bytes/string literals,  Up: Try to Support Python 2 6 and Newer Only

10.1.3.8 ‘from __future__ import absolute_import’
.................................................

Discussed in more detail below, but you should use this future statement
to prevent yourself from accidentally using implicit relative imports.


File: python.info,  Node: Supporting Python 2 5 and Newer Only,  Next: Handle Common "Gotchas",  Prev: Try to Support Python 2 6 and Newer Only,  Up: Tips & Tricks

10.1.3.9 Supporting Python 2.5 and Newer Only
.............................................

If you are supporting Python 2.5(1) and newer there are still some
features of Python that you can utilize.

* Menu:

* from __future__ import absolute_import: from __future__ import absolute_import<2>. 
* Mark all Unicode strings with a u prefix:: 
* Capturing the Currently Raised Exception:: 

   ---------- Footnotes ----------

   (1) http://www.python.org/2.5.x


File: python.info,  Node: from __future__ import absolute_import<2>,  Next: Mark all Unicode strings with a u prefix,  Up: Supporting Python 2 5 and Newer Only

10.1.3.10 ‘from __future__ import absolute_import’
..................................................

Implicit relative imports (e.g., importing ‘spam.bacon’ from within
‘spam.eggs’ with the statement ‘import bacon’) do not work in Python 3.
This future statement moves away from that and allows the use of
explicit relative imports (e.g., ‘from . import bacon’).

  In Python 2.5(1) you must use the __future__ statement to get to use
explicit relative imports and prevent implicit ones.  In Python 2.6(2)
explicit relative imports are available without the statement, but you
still want the __future__ statement to prevent implicit relative
imports.  In Python 2.7(3) the __future__ statement is not needed.  In
other words, unless you are only supporting Python 2.7 or a version
earlier than Python 2.5, use this __future__ statement.

   ---------- Footnotes ----------

   (1) http://www.python.org/2.5.x

   (2) http://www.python.org/2.6.x

   (3) http://www.python.org/2.7.x


File: python.info,  Node: Mark all Unicode strings with a u prefix,  Next: Capturing the Currently Raised Exception,  Prev: from __future__ import absolute_import<2>,  Up: Supporting Python 2 5 and Newer Only

10.1.3.11 Mark all Unicode strings with a ‘u’ prefix
....................................................

While Python 2.6 has a ‘__future__’ statement to automatically cause
Python 2 to treat all string literals as Unicode, Python 2.5 does not
have that shortcut.  This means you should go through and mark all
string literals with a ‘u’ prefix to turn them explicitly into text
strings where appropriate and only support Python 3.3 or newer.
Otherwise use a project like six(1) which provides a function to pass
all text string literals through.

   ---------- Footnotes ----------

   (1) https://pypi.python.org/pypi/six


File: python.info,  Node: Capturing the Currently Raised Exception,  Prev: Mark all Unicode strings with a u prefix,  Up: Supporting Python 2 5 and Newer Only

10.1.3.12 Capturing the Currently Raised Exception
..................................................

In Python 2.5 and earlier the syntax to access the current exception is:

     try:
       raise Exception()
     except Exception, exc:
       # Current exception is 'exc'.
       pass

  This syntax changed in Python 3 (and backported to Python 2.6(1) and
later) to:

     try:
       raise Exception()
     except Exception as exc:
       # Current exception is 'exc'.
       # In Python 3, 'exc' is restricted to the block; in Python 2.6/2.7 it will "leak".
       pass

  Because of this syntax change you must change how you capture the
current exception in Python 2.5 and earlier to:

     try:
       raise Exception()
     except Exception:
       import sys
       exc = sys.exc_info()[1]
       # Current exception is 'exc'.
       pass

  You can get more information about the raised exception from *note
sys.exc_info(): 2f3. than simply the current exception instance, but you
most likely don’t need it.

     Note: In Python 3, the traceback is attached to the exception
     instance through the ‘__traceback__’ attribute.  If the instance is
     saved in a local variable that persists outside of the ‘except’
     block, the traceback will create a reference cycle with the current
     frame and its dictionary of local variables.  This will delay
     reclaiming dead resources until the next cyclic *note garbage
     collection: 60e. pass.

     In Python 2, this problem only occurs if you save the traceback
     itself (e.g.  the third element of the tuple returned by *note
     sys.exc_info(): 2f3.) in a variable.

   ---------- Footnotes ----------

   (1) http://www.python.org/2.6.x


File: python.info,  Node: Handle Common "Gotchas",  Prev: Supporting Python 2 5 and Newer Only,  Up: Tips & Tricks

10.1.3.13 Handle Common "Gotchas"
.................................

These are things to watch out for no matter what version of Python 2 you
are supporting which are not syntactic considerations.

* Menu:

* from __future__ import division:: 
* Specify when opening a file as binary:: 
* Text files:: 
* Subclass object:: 
* Deal With the Bytes/String Dichotomy:: 
* Indexing bytes objects:: 
* __str__()/__unicode__(): __str__ /__unicode__. 
* Don't Index on Exceptions:: 
* Don't use __getslice__ & Friends:: 
* Updating doctests:: 
* Update map for imbalanced input sequences:: 


File: python.info,  Node: from __future__ import division,  Next: Specify when opening a file as binary,  Up: Handle Common "Gotchas"

10.1.3.14 ‘from __future__ import division’
...........................................

While the exact same outcome can be had by using the ‘-Qnew’ argument to
Python, using this future statement lifts the requirement that your
users use the flag to get the expected behavior of division in Python 3
(e.g., ‘1/2 == 0.5; 1//2 == 0’).


File: python.info,  Node: Specify when opening a file as binary,  Next: Text files,  Prev: from __future__ import division,  Up: Handle Common "Gotchas"

10.1.3.15 Specify when opening a file as binary
...............................................

Unless you have been working on Windows, there is a chance you have not
always bothered to add the ‘b’ mode when opening a binary file (e.g.,
‘rb’ for binary reading).  Under Python 3, binary files and text files
are clearly distinct and mutually incompatible; see the *note io: f9.
module for details.  Therefore, you *must* make a decision of whether a
file will be used for binary access (allowing to read and/or write bytes
data) or text access (allowing to read and/or write unicode data).


File: python.info,  Node: Text files,  Next: Subclass object,  Prev: Specify when opening a file as binary,  Up: Handle Common "Gotchas"

10.1.3.16 Text files
....................

Text files created using ‘open()’ under Python 2 return byte strings,
while under Python 3 they return unicode strings.  Depending on your
porting strategy, this can be an issue.

  If you want text files to return unicode strings in Python 2, you have
two possibilities:

   * Under Python 2.6 and higher, use *note io.open(): 11b1.  Since
     *note io.open(): 11b1. is essentially the same function in both
     Python 2 and Python 3, it will help iron out any issues that might
     arise.

   * If pre-2.6 compatibility is needed, then you should use *note
     codecs.open(): a5d. instead.  This will make sure that you get back
     unicode strings in Python 2.


File: python.info,  Node: Subclass object,  Next: Deal With the Bytes/String Dichotomy,  Prev: Text files,  Up: Handle Common "Gotchas"

10.1.3.17 Subclass ‘object’
...........................

New-style classes have been around since Python 2.2(1).  You need to
make sure you are subclassing from ‘object’ to avoid odd edge cases
involving method resolution order, etc.  This continues to be totally
valid in Python 3 (although unneeded as all classes implicitly inherit
from ‘object’).

   ---------- Footnotes ----------

   (1) http://www.python.org/2.2.x


File: python.info,  Node: Deal With the Bytes/String Dichotomy,  Next: Indexing bytes objects,  Prev: Subclass object,  Up: Handle Common "Gotchas"

10.1.3.18 Deal With the Bytes/String Dichotomy
..............................................

One of the biggest issues people have when porting code to Python 3 is
handling the bytes/string dichotomy.  Because Python 2 allowed the ‘str’
type to hold textual data, people have over the years been rather loose
in their delineation of what ‘str’ instances held text compared to
bytes.  In Python 3 you cannot be so care-free anymore and need to
properly handle the difference.  The key to handling this issue is to
make sure that *every* string literal in your Python 2 code is either
syntactically or functionally marked as either bytes or text data.
After this is done you then need to make sure your APIs are designed to
either handle a specific type or made to be properly polymorphic.

* Menu:

* Mark Up Python 2 String Literals:: 
* Decide what APIs Will Accept:: 
* Bytes / Unicode Comparison:: 


File: python.info,  Node: Mark Up Python 2 String Literals,  Next: Decide what APIs Will Accept,  Up: Deal With the Bytes/String Dichotomy

10.1.3.19 Mark Up Python 2 String Literals
..........................................

First thing you must do is designate every single string literal in
Python 2 as either textual or bytes data.  If you are only supporting
Python 2.6 or newer, this can be accomplished by marking bytes literals
with a ‘b’ prefix and then designating textual data with a ‘u’ prefix or
using the ‘unicode_literals’ future statement.

  If your project supports versions of Python predating 2.6, then you
should use the six(1) project and its ‘b()’ function to denote bytes
literals.  For text literals you can either use six’s ‘u()’ function or
use a ‘u’ prefix.

   ---------- Footnotes ----------

   (1) https://pypi.python.org/pypi/six


File: python.info,  Node: Decide what APIs Will Accept,  Next: Bytes / Unicode Comparison,  Prev: Mark Up Python 2 String Literals,  Up: Deal With the Bytes/String Dichotomy

10.1.3.20 Decide what APIs Will Accept
......................................

In Python 2 it was very easy to accidentally create an API that accepted
both bytes and textual data.  But in Python 3, thanks to the more strict
handling of disparate types, this loose usage of bytes and text together
tends to fail.

  Take the dict ‘{b'a': 'bytes', u'a': 'text'}’ in Python 2.6.  It
creates the dict ‘{u'a': 'text'}’ since ‘b'a' == u'a'’.  But in Python 3
the equivalent dict creates ‘{b'a': 'bytes', 'a': 'text'}’, i.e., no
lost data.  Similar issues can crop up when transitioning Python 2 code
to Python 3.

  This means you need to choose what an API is going to accept and
create and consistently stick to that API in both Python 2 and 3.


File: python.info,  Node: Bytes / Unicode Comparison,  Prev: Decide what APIs Will Accept,  Up: Deal With the Bytes/String Dichotomy

10.1.3.21 Bytes / Unicode Comparison
....................................

In Python 3, mixing bytes and unicode is forbidden in most situations;
it will raise a ‘TypeError’ where Python 2 would have attempted an
implicit coercion between types.  However, there is one case where it
doesn’t and it can be very misleading:

     >>> b"" == ""
     False

  This is because an equality comparison is required by the language to
always succeed (and return ‘False’ for incompatible types).  However,
this also means that code incorrectly ported to Python 3 can display
buggy behaviour if such comparisons are silently executed.  To detect
such situations, Python 3 has a ‘-b’ flag that will display a warning:

     $ python3 -b
     >>> b"" == ""
     __main__:1: BytesWarning: Comparison between bytes and string
     False

  To turn the warning into an exception, use the ‘-bb’ flag instead:

     $ python3 -bb
     >>> b"" == ""
     Traceback (most recent call last):
       File "<stdin>", line 1, in <module>
     BytesWarning: Comparison between bytes and string


File: python.info,  Node: Indexing bytes objects,  Next: __str__ /__unicode__,  Prev: Deal With the Bytes/String Dichotomy,  Up: Handle Common "Gotchas"

10.1.3.22 Indexing bytes objects
................................

Another potentially surprising change is the indexing behaviour of bytes
objects in Python 3:

     >>> b"xyz"[0]
     120

  Indeed, Python 3 bytes objects (as well as *note bytearray: 1f7.
objects) are sequences of integers.  But code converted from Python 2
will often assume that indexing a bytestring produces another
bytestring, not an integer.  To reconcile both behaviours, use slicing:

     >>> b"xyz"[0:1]
     b'x'
     >>> n = 1
     >>> b"xyz"[n:n+1]
     b'y'

  The only remaining gotcha is that an out-of-bounds slice returns an
empty bytes object instead of raising ‘IndexError’:

     >>> b"xyz"[3]
     Traceback (most recent call last):
       File "<stdin>", line 1, in <module>
     IndexError: index out of range
     >>> b"xyz"[3:4]
     b''


File: python.info,  Node: __str__ /__unicode__,  Next: Don't Index on Exceptions,  Prev: Indexing bytes objects,  Up: Handle Common "Gotchas"

10.1.3.23 ‘__str__()’/‘__unicode__()’
.....................................

In Python 2, objects can specify both a string and unicode
representation of themselves.  In Python 3, though, there is only a
string representation.  This becomes an issue as people can
inadvertently do things in their ‘__str__()’ methods which have
unpredictable results (e.g., infinite recursion if you happen to use the
‘unicode(self).encode('utf8')’ idiom as the body of your ‘__str__()’
method).

  You can use a mixin class to work around this.  This allows you to
only define a ‘__unicode__()’ method for your class and let the mixin
derive ‘__str__()’ for you (code from
‘http://lucumr.pocoo.org/2011/1/22/forwards-compatible-python/’):

     import sys

     class UnicodeMixin(object):

       """Mixin class to handle defining the proper __str__/__unicode__
       methods in Python 2 or 3."""

       if sys.version_info[0] >= 3: # Python 3
           def __str__(self):
               return self.__unicode__()
       else:  # Python 2
           def __str__(self):
               return self.__unicode__().encode('utf8')


     class Spam(UnicodeMixin):

       def __unicode__(self):
           return u'spam-spam-bacon-spam'  # 2to3 will remove the 'u' prefix


File: python.info,  Node: Don't Index on Exceptions,  Next: Don't use __getslice__ & Friends,  Prev: __str__ /__unicode__,  Up: Handle Common "Gotchas"

10.1.3.24 Don’t Index on Exceptions
...................................

In Python 2, the following worked:

     >>> exc = Exception(1, 2, 3)
     >>> exc.args[1]
     2
     >>> exc[1]  # Python 2 only!
     2

  But in Python 3, indexing directly on an exception is an error.  You
need to make sure to only index on the ‘BaseException.args’ attribute
which is a sequence containing all arguments passed to the *note
__init__(): 37c. method.

  Even better is to use the documented attributes the exception
provides.


File: python.info,  Node: Don't use __getslice__ & Friends,  Next: Updating doctests,  Prev: Don't Index on Exceptions,  Up: Handle Common "Gotchas"

10.1.3.25 Don’t use ‘__getslice__’ & Friends
............................................

Been deprecated for a while, but Python 3 finally drops support for
‘__getslice__()’, etc.  Move completely over to *note __getitem__():
44f. and friends.


File: python.info,  Node: Updating doctests,  Next: Update map for imbalanced input sequences,  Prev: Don't use __getslice__ & Friends,  Up: Handle Common "Gotchas"

10.1.3.26 Updating doctests
...........................

Don’t forget to make them Python 2/3 compatible as well.  If you wrote a
monolithic set of doctests (e.g., a single docstring containing all of
your doctests), you should at least consider breaking the doctests up
into smaller pieces to make it more manageable to fix.  Otherwise it
might very well be worth your time and effort to port your tests to
*note unittest: 187.


File: python.info,  Node: Update map for imbalanced input sequences,  Prev: Updating doctests,  Up: Handle Common "Gotchas"

10.1.3.27 Update ‘map’ for imbalanced input sequences
.....................................................

With Python 2, when ‘map’ was given more than one input sequence it
would pad the shorter sequences with ‘None’ values, returning a sequence
as long as the longest input sequence.

  With Python 3, if the input sequences to ‘map’ are of unequal length,
‘map’ will stop at the termination of the shortest of the sequences.
For full compatibility with ‘map’ from Python 2.x, wrap the sequence
arguments in ‘itertools.zip_longest()’, e.g.  ‘map(func, *sequences)’
becomes ‘list(map(func, itertools.zip_longest(*sequences)))’.


File: python.info,  Node: Eliminate -3 Warnings,  Prev: Tips & Tricks,  Up: Writing Source-Compatible Python 2/3 Code

10.1.3.28 Eliminate ‘-3’ Warnings
.................................

When you run your application’s test suite, run it using the ‘-3’ flag
passed to Python.  This will cause various warnings to be raised during
execution about things that are semantic changes between Python 2 and 3.
Try to eliminate those warnings to make your code even more portable to
Python 3.


File: python.info,  Node: Alternative Approaches,  Next: Other Resources<2>,  Prev: Writing Source-Compatible Python 2/3 Code,  Up: Porting Python 2 Code to Python 3

10.1.4 Alternative Approaches
-----------------------------

While supporting Python 2 & 3 simultaneously is typically the preferred
choice by people so that they can continue to improve code and have it
work for the most number of users, your life may be easier if you only
have to support one major version of Python going forward.

* Menu:

* Supporting Only Python 3 Going Forward From Python 2 Code:: 
* Backporting Python 3 code to Python 2:: 


File: python.info,  Node: Supporting Only Python 3 Going Forward From Python 2 Code,  Next: Backporting Python 3 code to Python 2,  Up: Alternative Approaches

10.1.4.1 Supporting Only Python 3 Going Forward From Python 2 Code
..................................................................

If you have Python 2 code but going forward only want to improve it as
Python 3 code, then you can use 2to3(1) to translate your Python 2 code
to Python 3 code.  This is only recommended, though, if your current
version of your project is going into maintenance mode and you want all
new features to be exclusive to Python 3.

   ---------- Footnotes ----------

   (1) http://docs.python.org/2/library/2to3.html


File: python.info,  Node: Backporting Python 3 code to Python 2,  Prev: Supporting Only Python 3 Going Forward From Python 2 Code,  Up: Alternative Approaches

10.1.4.2 Backporting Python 3 code to Python 2
..............................................

If you have Python 3 code and have little interest in supporting Python
2 you can use 3to2(1) to translate from Python 3 code to Python 2 code.
This is only recommended if you don’t plan to heavily support Python 2
users.  Otherwise write your code for Python 3 and then backport as far
back as you want.  This is typically easier than going from Python 2 to
3 as you will have worked out any difficulties with e.g.  bytes/strings,
etc.

   ---------- Footnotes ----------

   (1) https://pypi.python.org/pypi/3to2


File: python.info,  Node: Other Resources<2>,  Prev: Alternative Approaches,  Up: Porting Python 2 Code to Python 3

10.1.5 Other Resources
----------------------

The authors of the following blog posts, wiki pages, and books deserve
special thanks for making public their tips for porting Python 2 code to
Python 3 (and thus helping provide information for this document and its
various revisions over the years):

   * ‘http://wiki.python.org/moin/PortingPythonToPy3k’

   * ‘http://python3porting.com/’

   * ‘http://docs.pythonsprints.com/python3_porting/py-porting.html’

   * 
     ‘http://techspot.zzzeek.org/2011/01/24/zzzeek-s-guide-to-python-3-porting/’

   * 
     ‘http://dabeaz.blogspot.com/2011/01/porting-py65-and-my-superboard-to.html’

   * ‘http://lucumr.pocoo.org/2011/1/22/forwards-compatible-python/’

   * ‘http://lucumr.pocoo.org/2010/2/11/porting-to-python-3-a-guide/’

   * ‘https://wiki.ubuntu.com/Python/3’

  If you feel there is something missing from this document that should
be added, please email the python-porting(1) mailing list.

   ---------- Footnotes ----------

   (1) http://mail.python.org/mailman/listinfo/python-porting


File: python.info,  Node: Porting Extension Modules to Python 3,  Next: Curses Programming with Python,  Prev: Porting Python 2 Code to Python 3,  Up: Python HOWTOs

10.2 Porting Extension Modules to Python 3
==========================================

     author: Benjamin Peterson

Abstract
........

Although changing the C-API was not one of Python 3’s objectives, the
many Python-level changes made leaving Python 2’s API intact impossible.
In fact, some changes such as *note int(): 1f2. and *note long(): 1f3.
unification are more obvious on the C level.  This document endeavors to
document incompatibilities and how they can be worked around.

* Menu:

* Conditional compilation:: 
* Changes to Object APIs:: 
* Module initialization and state:: 
* CObject replaced with Capsule:: 
* Other options: Other options<2>. 


File: python.info,  Node: Conditional compilation,  Next: Changes to Object APIs,  Up: Porting Extension Modules to Python 3

10.2.1 Conditional compilation
------------------------------

The easiest way to compile only some code for Python 3 is to check if
‘PY_MAJOR_VERSION’ is greater than or equal to 3.

     #if PY_MAJOR_VERSION >= 3
     #define IS_PY3K
     #endif

  API functions that are not present can be aliased to their equivalents
within conditional blocks.


File: python.info,  Node: Changes to Object APIs,  Next: Module initialization and state,  Prev: Conditional compilation,  Up: Porting Extension Modules to Python 3

10.2.2 Changes to Object APIs
-----------------------------

Python 3 merged together some types with similar functions while cleanly
separating others.

* Menu:

* str/unicode Unification:: 
* long/int Unification:: 


File: python.info,  Node: str/unicode Unification,  Next: long/int Unification,  Up: Changes to Object APIs

10.2.2.1 str/unicode Unification
................................

Python 3’s *note str(): 1ea. (‘PyString_*’ functions in C) type is
equivalent to Python 2’s *note unicode(): 1f5. (‘PyUnicode_*’).  The old
8-bit string type has become ‘bytes()’.  Python 2.6 and later provide a
compatibility header, ‘bytesobject.h’, mapping ‘PyBytes’ names to
‘PyString’ ones.  For best compatibility with Python 3, ‘PyUnicode’
should be used for textual data and ‘PyBytes’ for binary data.  It’s
also important to remember that ‘PyBytes’ and ‘PyUnicode’ in Python 3
are not interchangeable like ‘PyString’ and ‘PyUnicode’ are in Python 2.
The following example shows best practices with regards to ‘PyUnicode’,
‘PyString’, and ‘PyBytes’.

     #include "stdlib.h"
     #include "Python.h"
     #include "bytesobject.h"

     /* text example */
     static PyObject *
     say_hello(PyObject *self, PyObject *args) {
         PyObject *name, *result;

         if (!PyArg_ParseTuple(args, "U:say_hello", &name))
             return NULL;

         result = PyUnicode_FromFormat("Hello, %S!", name);
         return result;
     }

     /* just a forward */
     static char * do_encode(PyObject *);

     /* bytes example */
     static PyObject *
     encode_object(PyObject *self, PyObject *args) {
         char *encoded;
         PyObject *result, *myobj;

         if (!PyArg_ParseTuple(args, "O:encode_object", &myobj))
             return NULL;

         encoded = do_encode(myobj);
         if (encoded == NULL)
             return NULL;
         result = PyBytes_FromString(encoded);
         free(encoded);
         return result;
     }


File: python.info,  Node: long/int Unification,  Prev: str/unicode Unification,  Up: Changes to Object APIs

10.2.2.2 long/int Unification
.............................

Python 3 has only one integer type, *note int(): 1f2.  But it actually
corresponds to Python 2’s *note long(): 1f3. type–the *note int(): 1f2.
type used in Python 2 was removed.  In the C-API, ‘PyInt_*’ functions
are replaced by their ‘PyLong_*’ equivalents.


File: python.info,  Node: Module initialization and state,  Next: CObject replaced with Capsule,  Prev: Changes to Object APIs,  Up: Porting Extension Modules to Python 3

10.2.3 Module initialization and state
--------------------------------------

Python 3 has a revamped extension module initialization system.  (See
PEP 3121(1).)  Instead of storing module state in globals, they should
be stored in an interpreter specific structure.  Creating modules that
act correctly in both Python 2 and Python 3 is tricky.  The following
simple example demonstrates how.

     #include "Python.h"

     struct module_state {
         PyObject *error;
     };

     #if PY_MAJOR_VERSION >= 3
     #define GETSTATE(m) ((struct module_state*)PyModule_GetState(m))
     #else
     #define GETSTATE(m) (&_state)
     static struct module_state _state;
     #endif

     static PyObject *
     error_out(PyObject *m) {
         struct module_state *st = GETSTATE(m);
         PyErr_SetString(st->error, "something bad happened");
         return NULL;
     }

     static PyMethodDef myextension_methods[] = {
         {"error_out", (PyCFunction)error_out, METH_NOARGS, NULL},
         {NULL, NULL}
     };

     #if PY_MAJOR_VERSION >= 3

     static int myextension_traverse(PyObject *m, visitproc visit, void *arg) {
         Py_VISIT(GETSTATE(m)->error);
         return 0;
     }

     static int myextension_clear(PyObject *m) {
         Py_CLEAR(GETSTATE(m)->error);
         return 0;
     }


     static struct PyModuleDef moduledef = {
             PyModuleDef_HEAD_INIT,
             "myextension",
             NULL,
             sizeof(struct module_state),
             myextension_methods,
             NULL,
             myextension_traverse,
             myextension_clear,
             NULL
     };

     #define INITERROR return NULL

     PyObject *
     PyInit_myextension(void)

     #else
     #define INITERROR return

     void
     initmyextension(void)
     #endif
     {
     #if PY_MAJOR_VERSION >= 3
         PyObject *module = PyModule_Create(&moduledef);
     #else
         PyObject *module = Py_InitModule("myextension", myextension_methods);
     #endif

         if (module == NULL)
             INITERROR;
         struct module_state *st = GETSTATE(module);

         st->error = PyErr_NewException("myextension.Error", NULL, NULL);
         if (st->error == NULL) {
             Py_DECREF(module);
             INITERROR;
         }

     #if PY_MAJOR_VERSION >= 3
         return module;
     #endif
     }

   ---------- Footnotes ----------

   (1) http://www.python.org/dev/peps/pep-3121


File: python.info,  Node: CObject replaced with Capsule,  Next: Other options<2>,  Prev: Module initialization and state,  Up: Porting Extension Modules to Python 3

10.2.4 CObject replaced with Capsule
------------------------------------

The ‘Capsule’ object was introduced in Python 3.1 and 2.7 to replace
‘CObject’.  CObjects were useful, but the ‘CObject’ API was problematic:
it didn’t permit distinguishing between valid CObjects, which allowed
mismatched CObjects to crash the interpreter, and some of its APIs
relied on undefined behavior in C. (For further reading on the rationale
behind Capsules, please see issue 5630(1).)

  If you’re currently using CObjects, and you want to migrate to 3.1 or
newer, you’ll need to switch to Capsules.  ‘CObject’ was deprecated in
3.1 and 2.7 and completely removed in Python 3.2.  If you only support
2.7, or 3.1 and above, you can simply switch to ‘Capsule’.  If you need
to support Python 3.0, or versions of Python earlier than 2.7, you’ll
have to support both CObjects and Capsules.  (Note that Python 3.0 is no
longer supported, and it is not recommended for production use.)

  The following example header file ‘capsulethunk.h’ may solve the
problem for you.  Simply write your code against the ‘Capsule’ API and
include this header file after ‘Python.h’.  Your code will automatically
use Capsules in versions of Python with Capsules, and switch to CObjects
when Capsules are unavailable.

  ‘capsulethunk.h’ simulates Capsules using CObjects.  However,
‘CObject’ provides no place to store the capsule’s "name".  As a result
the simulated ‘Capsule’ objects created by ‘capsulethunk.h’ behave
slightly differently from real Capsules.  Specifically:

        * The name parameter passed in to *note PyCapsule_New(): 29d1.
          is ignored.

        * The name parameter passed in to *note PyCapsule_IsValid():
          2c8. and *note PyCapsule_GetPointer(): 2d71. is ignored, and
          no error checking of the name is performed.

        * *note PyCapsule_GetName(): 2d74. always returns NULL.

        * *note PyCapsule_SetName(): 2d77. always raises an exception
          and returns failure.  (Since there’s no way to store a name in
          a CObject, noisy failure of *note PyCapsule_SetName(): 2d77.
          was deemed preferable to silent failure here.  If this is
          inconvenient, feel free to modify your local copy as you see
          fit.)

  You can find ‘capsulethunk.h’ in the Python source distribution as
Doc/includes/capsulethunk.h(2).  We also include it here for your
convenience:

     #ifndef __CAPSULETHUNK_H
     #define __CAPSULETHUNK_H

     #if (    (PY_VERSION_HEX <  0x02070000) \
          || ((PY_VERSION_HEX >= 0x03000000) \
           && (PY_VERSION_HEX <  0x03010000)) )

     #define __PyCapsule_GetField(capsule, field, default_value) \
         ( PyCapsule_CheckExact(capsule) \
             ? (((PyCObject *)capsule)->field) \
             : (default_value) \
         ) \

     #define __PyCapsule_SetField(capsule, field, value) \
         ( PyCapsule_CheckExact(capsule) \
             ? (((PyCObject *)capsule)->field = value), 1 \
             : 0 \
         ) \


     #define PyCapsule_Type PyCObject_Type

     #define PyCapsule_CheckExact(capsule) (PyCObject_Check(capsule))
     #define PyCapsule_IsValid(capsule, name) (PyCObject_Check(capsule))


     #define PyCapsule_New(pointer, name, destructor) \
         (PyCObject_FromVoidPtr(pointer, destructor))


     #define PyCapsule_GetPointer(capsule, name) \
         (PyCObject_AsVoidPtr(capsule))

     /* Don't call PyCObject_SetPointer here, it fails if there's a destructor */
     #define PyCapsule_SetPointer(capsule, pointer) \
         __PyCapsule_SetField(capsule, cobject, pointer)


     #define PyCapsule_GetDestructor(capsule) \
         __PyCapsule_GetField(capsule, destructor)

     #define PyCapsule_SetDestructor(capsule, dtor) \
         __PyCapsule_SetField(capsule, destructor, dtor)


     /*
      * Sorry, there's simply no place
      * to store a Capsule "name" in a CObject.
      */
     #define PyCapsule_GetName(capsule) NULL

     static int
     PyCapsule_SetName(PyObject *capsule, const char *unused)
     {
         unused = unused;
         PyErr_SetString(PyExc_NotImplementedError,
             "can't use PyCapsule_SetName with CObjects");
         return 1;
     }



     #define PyCapsule_GetContext(capsule) \
         __PyCapsule_GetField(capsule, descr)

     #define PyCapsule_SetContext(capsule, context) \
         __PyCapsule_SetField(capsule, descr, context)


     static void *
     PyCapsule_Import(const char *name, int no_block)
     {
         PyObject *object = NULL;
         void *return_value = NULL;
         char *trace;
         size_t name_length = (strlen(name) + 1) * sizeof(char);
         char *name_dup = (char *)PyMem_MALLOC(name_length);

         if (!name_dup) {
             return NULL;
         }

         memcpy(name_dup, name, name_length);

         trace = name_dup;
         while (trace) {
             char *dot = strchr(trace, '.');
             if (dot) {
                 *dot++ = '\0';
             }

             if (object == NULL) {
                 if (no_block) {
                     object = PyImport_ImportModuleNoBlock(trace);
                 } else {
                     object = PyImport_ImportModule(trace);
                     if (!object) {
                         PyErr_Format(PyExc_ImportError,
                             "PyCapsule_Import could not "
                             "import module \"%s\"", trace);
                     }
                 }
             } else {
                 PyObject *object2 = PyObject_GetAttrString(object, trace);
                 Py_DECREF(object);
                 object = object2;
             }
             if (!object) {
                 goto EXIT;
             }

             trace = dot;
         }

         if (PyCObject_Check(object)) {
             PyCObject *cobject = (PyCObject *)object;
             return_value = cobject->cobject;
         } else {
             PyErr_Format(PyExc_AttributeError,
                 "PyCapsule_Import \"%s\" is not valid",
                 name);
         }

     EXIT:
         Py_XDECREF(object);
         if (name_dup) {
             PyMem_FREE(name_dup);
         }
         return return_value;
     }

     #endif /* #if PY_VERSION_HEX < 0x02070000 */

     #endif /* __CAPSULETHUNK_H */


   ---------- Footnotes ----------

   (1) http://bugs.python.org/issue5630

   (2) http://hg.python.org/cpython/file/2.7/Doc/includes/capsulethunk.h


File: python.info,  Node: Other options<2>,  Prev: CObject replaced with Capsule,  Up: Porting Extension Modules to Python 3

10.2.5 Other options
--------------------

If you are writing a new extension module, you might consider Cython(1).
It translates a Python-like language to C. The extension modules it
creates are compatible with Python 3 and Python 2.

   ---------- Footnotes ----------

   (1) http://www.cython.org


File: python.info,  Node: Curses Programming with Python,  Next: Descriptor HowTo Guide,  Prev: Porting Extension Modules to Python 3,  Up: Python HOWTOs

10.3 Curses Programming with Python
===================================

     Author: A.M. Kuchling, Eric S. Raymond

     Release: 2.03

Abstract
........

This document describes how to write text-mode programs with Python 2.x,
using the *note curses: 79. extension module to control the display.

* Menu:

* What is curses?:: 
* Starting and ending a curses application:: 
* Windows and Pads:: 
* Displaying Text:: 
* User Input:: 
* For More Information:: 

What is curses?

* The Python curses module:: 

Displaying Text

* Attributes and Color:: 


File: python.info,  Node: What is curses?,  Next: Starting and ending a curses application,  Up: Curses Programming with Python

10.3.1 What is curses?
----------------------

The curses library supplies a terminal-independent screen-painting and
keyboard-handling facility for text-based terminals; such terminals
include VT100s, the Linux console, and the simulated terminal provided
by X11 programs such as xterm and rxvt.  Display terminals support
various control codes to perform common operations such as moving the
cursor, scrolling the screen, and erasing areas.  Different terminals
use widely differing codes, and often have their own minor quirks.

  In a world of X displays, one might ask "why bother"?  It’s true that
character-cell display terminals are an obsolete technology, but there
are niches in which being able to do fancy things with them are still
valuable.  One is on small-footprint or embedded Unixes that don’t carry
an X server.  Another is for tools like OS installers and kernel
configurators that may have to run before X is available.

  The curses library hides all the details of different terminals, and
provides the programmer with an abstraction of a display, containing
multiple non-overlapping windows.  The contents of a window can be
changed in various ways– adding text, erasing it, changing its
appearance–and the curses library will automagically figure out what
control codes need to be sent to the terminal to produce the right
output.

  The curses library was originally written for BSD Unix; the later
System V versions of Unix from AT&T added many enhancements and new
functions.  BSD curses is no longer maintained, having been replaced by
ncurses, which is an open-source implementation of the AT&T interface.
If you’re using an open-source Unix such as Linux or FreeBSD, your
system almost certainly uses ncurses.  Since most current commercial
Unix versions are based on System V code, all the functions described
here will probably be available.  The older versions of curses carried
by some proprietary Unixes may not support everything, though.

  No one has made a Windows port of the curses module.  On a Windows
platform, try the Console module written by Fredrik Lundh.  The Console
module provides cursor-addressable text output, plus full support for
mouse and keyboard input, and is available from
‘http://effbot.org/zone/console-index.htm’.

* Menu:

* The Python curses module:: 


File: python.info,  Node: The Python curses module,  Up: What is curses?

10.3.1.1 The Python curses module
.................................

Thy Python module is a fairly simple wrapper over the C functions
provided by curses; if you’re already familiar with curses programming
in C, it’s really easy to transfer that knowledge to Python.  The
biggest difference is that the Python interface makes things simpler, by
merging different C functions such as ‘addstr()’, ‘mvaddstr()’,
‘mvwaddstr()’, into a single ‘addstr()’ method.  You’ll see this covered
in more detail later.

  This HOWTO is simply an introduction to writing text-mode programs
with curses and Python.  It doesn’t attempt to be a complete guide to
the curses API; for that, see the Python library guide’s section on
ncurses, and the C manual pages for ncurses.  It will, however, give you
the basic ideas.


File: python.info,  Node: Starting and ending a curses application,  Next: Windows and Pads,  Prev: What is curses?,  Up: Curses Programming with Python

10.3.2 Starting and ending a curses application
-----------------------------------------------

Before doing anything, curses must be initialized.  This is done by
calling the ‘initscr()’ function, which will determine the terminal
type, send any required setup codes to the terminal, and create various
internal data structures.  If successful, ‘initscr()’ returns a window
object representing the entire screen; this is usually called ‘stdscr’,
after the name of the corresponding C variable.

     import curses
     stdscr = curses.initscr()

  Usually curses applications turn off automatic echoing of keys to the
screen, in order to be able to read keys and only display them under
certain circumstances.  This requires calling the ‘noecho()’ function.

     curses.noecho()

  Applications will also commonly need to react to keys instantly,
without requiring the Enter key to be pressed; this is called cbreak
mode, as opposed to the usual buffered input mode.

     curses.cbreak()

  Terminals usually return special keys, such as the cursor keys or
navigation keys such as Page Up and Home, as a multibyte escape
sequence.  While you could write your application to expect such
sequences and process them accordingly, curses can do it for you,
returning a special value such as ‘curses.KEY_LEFT’.  To get curses to
do the job, you’ll have to enable keypad mode.

     stdscr.keypad(1)

  Terminating a curses application is much easier than starting one.
You’ll need to call

     curses.nocbreak(); stdscr.keypad(0); curses.echo()

  to reverse the curses-friendly terminal settings.  Then call the
‘endwin()’ function to restore the terminal to its original operating
mode.

     curses.endwin()

  A common problem when debugging a curses application is to get your
terminal messed up when the application dies without restoring the
terminal to its previous state.  In Python this commonly happens when
your code is buggy and raises an uncaught exception.  Keys are no longer
echoed to the screen when you type them, for example, which makes using
the shell difficult.

  In Python you can avoid these complications and make debugging much
easier by importing the module *note curses.wrapper: 13d6.  It supplies
a ‘wrapper()’ function that takes a callable.  It does the
initializations described above, and also initializes colors if color
support is present.  It then runs your provided callable and finally
deinitializes appropriately.  The callable is called inside a try-catch
clause which catches exceptions, performs curses deinitialization, and
then passes the exception upwards.  Thus, your terminal won’t be left in
a funny state on exception.


File: python.info,  Node: Windows and Pads,  Next: Displaying Text,  Prev: Starting and ending a curses application,  Up: Curses Programming with Python

10.3.3 Windows and Pads
-----------------------

Windows are the basic abstraction in curses.  A window object represents
a rectangular area of the screen, and supports various methods to
display text, erase it, allow the user to input strings, and so forth.

  The ‘stdscr’ object returned by the ‘initscr()’ function is a window
object that covers the entire screen.  Many programs may need only this
single window, but you might wish to divide the screen into smaller
windows, in order to redraw or clear them separately.  The ‘newwin()’
function creates a new window of a given size, returning the new window
object.

     begin_x = 20; begin_y = 7
     height = 5; width = 40
     win = curses.newwin(height, width, begin_y, begin_x)

  A word about the coordinate system used in curses: coordinates are
always passed in the order _y,x_, and the top-left corner of a window is
coordinate (0,0).  This breaks a common convention for handling
coordinates, where the _x_ coordinate usually comes first.  This is an
unfortunate difference from most other computer applications, but it’s
been part of curses since it was first written, and it’s too late to
change things now.

  When you call a method to display or erase text, the effect doesn’t
immediately show up on the display.  This is because curses was
originally written with slow 300-baud terminal connections in mind; with
these terminals, minimizing the time required to redraw the screen is
very important.  This lets curses accumulate changes to the screen, and
display them in the most efficient manner.  For example, if your program
displays some characters in a window, and then clears the window,
there’s no need to send the original characters because they’d never be
visible.

  Accordingly, curses requires that you explicitly tell it to redraw
windows, using the ‘refresh()’ method of window objects.  In practice,
this doesn’t really complicate programming with curses much.  Most
programs go into a flurry of activity, and then pause waiting for a
keypress or some other action on the part of the user.  All you have to
do is to be sure that the screen has been redrawn before pausing to wait
for user input, by simply calling ‘stdscr.refresh()’ or the ‘refresh()’
method of some other relevant window.

  A pad is a special case of a window; it can be larger than the actual
display screen, and only a portion of it displayed at a time.  Creating
a pad simply requires the pad’s height and width, while refreshing a pad
requires giving the coordinates of the on-screen area where a subsection
of the pad will be displayed.

     pad = curses.newpad(100, 100)
     #  These loops fill the pad with letters; this is
     # explained in the next section
     for y in range(0, 100):
         for x in range(0, 100):
             try:
                 pad.addch(y,x, ord('a') + (x*x+y*y) % 26)
             except curses.error:
                 pass

     #  Displays a section of the pad in the middle of the screen
     pad.refresh(0,0, 5,5, 20,75)

  The ‘refresh()’ call displays a section of the pad in the rectangle
extending from coordinate (5,5) to coordinate (20,75) on the screen; the
upper left corner of the displayed section is coordinate (0,0) on the
pad.  Beyond that difference, pads are exactly like ordinary windows and
support the same methods.

  If you have multiple windows and pads on screen there is a more
efficient way to go, which will prevent annoying screen flicker at
refresh time.  Use the ‘noutrefresh()’ method of each window to update
the data structure representing the desired state of the screen; then
change the physical screen to match the desired state in one go with the
function ‘doupdate()’.  The normal ‘refresh()’ method calls ‘doupdate()’
as its last act.


File: python.info,  Node: Displaying Text,  Next: User Input,  Prev: Windows and Pads,  Up: Curses Programming with Python

10.3.4 Displaying Text
----------------------

From a C programmer’s point of view, curses may sometimes look like a
twisty maze of functions, all subtly different.  For example, ‘addstr()’
displays a string at the current cursor location in the ‘stdscr’ window,
while ‘mvaddstr()’ moves to a given y,x coordinate first before
displaying the string.  ‘waddstr()’ is just like ‘addstr()’, but allows
specifying a window to use, instead of using ‘stdscr’ by default.
‘mvwaddstr()’ follows similarly.

  Fortunately the Python interface hides all these details; ‘stdscr’ is
a window object like any other, and methods like ‘addstr()’ accept
multiple argument forms.  Usually there are four different forms.

Form                                  Description
                                      
------------------------------------------------------------------------------------------
                                      
_str_ or _ch_                         Display the string _str_ or character _ch_ at the
                                      current position
                                      
                                      
_str_ or _ch_, _attr_                 Display the string _str_ or character _ch_, using
                                      attribute _attr_ at the current position
                                      
                                      
_y_, _x_, _str_ or _ch_               Move to position _y,x_ within the window, and
                                      display _str_ or _ch_
                                      
                                      
_y_, _x_, _str_ or _ch_, _attr_       Move to position _y,x_ within the window, and
                                      display _str_ or _ch_, using attribute _attr_
                                      

  Attributes allow displaying text in highlighted forms, such as in
boldface, underline, reverse code, or in color.  They’ll be explained in
more detail in the next subsection.

  The ‘addstr()’ function takes a Python string as the value to be
displayed, while the ‘addch()’ functions take a character, which can be
either a Python string of length 1 or an integer.  If it’s a string,
you’re limited to displaying characters between 0 and 255.  SVr4 curses
provides constants for extension characters; these constants are
integers greater than 255.  For example, ‘ACS_PLMINUS’ is a +/- symbol,
and ‘ACS_ULCORNER’ is the upper left corner of a box (handy for drawing
borders).

  Windows remember where the cursor was left after the last operation,
so if you leave out the _y,x_ coordinates, the string or character will
be displayed wherever the last operation left off.  You can also move
the cursor with the ‘move(y,x)’ method.  Because some terminals always
display a flashing cursor, you may want to ensure that the cursor is
positioned in some location where it won’t be distracting; it can be
confusing to have the cursor blinking at some apparently random
location.

  If your application doesn’t need a blinking cursor at all, you can
call ‘curs_set(0)’ to make it invisible.  Equivalently, and for
compatibility with older curses versions, there’s a ‘leaveok(bool)’
function.  When _bool_ is true, the curses library will attempt to
suppress the flashing cursor, and you won’t need to worry about leaving
it in odd locations.

* Menu:

* Attributes and Color:: 


File: python.info,  Node: Attributes and Color,  Up: Displaying Text

10.3.4.1 Attributes and Color
.............................

Characters can be displayed in different ways.  Status lines in a
text-based application are commonly shown in reverse video; a text
viewer may need to highlight certain words.  curses supports this by
allowing you to specify an attribute for each cell on the screen.

  An attribute is an integer, each bit representing a different
attribute.  You can try to display text with multiple attribute bits
set, but curses doesn’t guarantee that all the possible combinations are
available, or that they’re all visually distinct.  That depends on the
ability of the terminal being used, so it’s safest to stick to the most
commonly available attributes, listed here.

Attribute                  Description
                           
----------------------------------------------------------------------
                           
‘A_BLINK’                  Blinking text
                           
                           
‘A_BOLD’                   Extra bright or bold text
                           
                           
‘A_DIM’                    Half bright text
                           
                           
‘A_REVERSE’                Reverse-video text
                           
                           
‘A_STANDOUT’               The best highlighting mode available
                           
                           
‘A_UNDERLINE’              Underlined text
                           

  So, to display a reverse-video status line on the top line of the
screen, you could code:

     stdscr.addstr(0, 0, "Current mode: Typing mode",
                   curses.A_REVERSE)
     stdscr.refresh()

  The curses library also supports color on those terminals that provide
it.  The most common such terminal is probably the Linux console,
followed by color xterms.

  To use color, you must call the ‘start_color()’ function soon after
calling ‘initscr()’, to initialize the default color set (the
‘curses.wrapper.wrapper()’ function does this automatically).  Once
that’s done, the ‘has_colors()’ function returns TRUE if the terminal in
use can actually display color.  (Note: curses uses the American
spelling ’color’, instead of the Canadian/British spelling ’colour’.  If
you’re used to the British spelling, you’ll have to resign yourself to
misspelling it for the sake of these functions.)

  The curses library maintains a finite number of color pairs,
containing a foreground (or text) color and a background color.  You can
get the attribute value corresponding to a color pair with the
‘color_pair()’ function; this can be bitwise-OR’ed with other attributes
such as ‘A_REVERSE’, but again, such combinations are not guaranteed to
work on all terminals.

  An example, which displays a line of text using color pair 1:

     stdscr.addstr("Pretty text", curses.color_pair(1))
     stdscr.refresh()

  As I said before, a color pair consists of a foreground and background
color.  ‘start_color()’ initializes 8 basic colors when it activates
color mode.  They are: 0:black, 1:red, 2:green, 3:yellow, 4:blue,
5:magenta, 6:cyan, and 7:white.  The curses module defines named
constants for each of these colors: ‘curses.COLOR_BLACK’,
‘curses.COLOR_RED’, and so forth.

  The ‘init_pair(n, f, b)’ function changes the definition of color pair
_n_, to foreground color f and background color b.  Color pair 0 is
hard-wired to white on black, and cannot be changed.

  Let’s put all this together.  To change color 1 to red text on a white
background, you would call:

     curses.init_pair(1, curses.COLOR_RED, curses.COLOR_WHITE)

  When you change a color pair, any text already displayed using that
color pair will change to the new colors.  You can also display new text
in this color with:

     stdscr.addstr(0,0, "RED ALERT!", curses.color_pair(1))

  Very fancy terminals can change the definitions of the actual colors
to a given RGB value.  This lets you change color 1, which is usually
red, to purple or blue or any other color you like.  Unfortunately, the
Linux console doesn’t support this, so I’m unable to try it out, and
can’t provide any examples.  You can check if your terminal can do this
by calling ‘can_change_color()’, which returns TRUE if the capability is
there.  If you’re lucky enough to have such a talented terminal, consult
your system’s man pages for more information.


File: python.info,  Node: User Input,  Next: For More Information,  Prev: Displaying Text,  Up: Curses Programming with Python

10.3.5 User Input
-----------------

The curses library itself offers only very simple input mechanisms.
Python’s support adds a text-input widget that makes up some of the
lack.

  The most common way to get input to a window is to use its ‘getch()’
method.  ‘getch()’ pauses and waits for the user to hit a key,
displaying it if ‘echo()’ has been called earlier.  You can optionally
specify a coordinate to which the cursor should be moved before pausing.

  It’s possible to change this behavior with the method ‘nodelay()’.
After ‘nodelay(1)’, ‘getch()’ for the window becomes non-blocking and
returns ‘curses.ERR’ (a value of -1) when no input is ready.  There’s
also a ‘halfdelay()’ function, which can be used to (in effect) set a
timer on each ‘getch()’; if no input becomes available within a
specified delay (measured in tenths of a second), curses raises an
exception.

  The ‘getch()’ method returns an integer; if it’s between 0 and 255, it
represents the ASCII code of the key pressed.  Values greater than 255
are special keys such as Page Up, Home, or the cursor keys.  You can
compare the value returned to constants such as ‘curses.KEY_PPAGE’,
‘curses.KEY_HOME’, or ‘curses.KEY_LEFT’.  Usually the main loop of your
program will look something like this:

     while 1:
         c = stdscr.getch()
         if c == ord('p'):
             PrintDocument()
         elif c == ord('q'):
             break  # Exit the while()
         elif c == curses.KEY_HOME:
             x = y = 0

  The *note curses.ascii: 7a. module supplies ASCII class membership
functions that take either integer or 1-character-string arguments;
these may be useful in writing more readable tests for your command
interpreters.  It also supplies conversion functions that take either
integer or 1-character-string arguments and return the same type.  For
example, *note curses.ascii.ctrl(): 1441. returns the control character
corresponding to its argument.

  There’s also a method to retrieve an entire string, ‘getstr()’.  It
isn’t used very often, because its functionality is quite limited; the
only editing keys available are the backspace key and the Enter key,
which terminates the string.  It can optionally be limited to a fixed
number of characters.

     curses.echo()            # Enable echoing of characters

     # Get a 15-character string, with the cursor on the top line
     s = stdscr.getstr(0,0, 15)

  The Python *note curses.textpad: 7c. module supplies something better.
With it, you can turn a window into a text box that supports an
Emacs-like set of keybindings.  Various methods of ‘Textbox’ class
support editing with input validation and gathering the edit results
either with or without trailing spaces.  See the library documentation
on *note curses.textpad: 7c. for the details.


File: python.info,  Node: For More Information,  Prev: User Input,  Up: Curses Programming with Python

10.3.6 For More Information
---------------------------

This HOWTO didn’t cover some advanced topics, such as screen-scraping or
capturing mouse events from an xterm instance.  But the Python library
page for the curses modules is now pretty complete.  You should browse
it next.

  If you’re in doubt about the detailed behavior of any of the ncurses
entry points, consult the manual pages for your curses implementation,
whether it’s ncurses or a proprietary Unix vendor’s.  The manual pages
will document any quirks, and provide complete lists of all the
functions, attributes, and ‘ACS_*’ characters available to you.

  Because the curses API is so large, some functions aren’t supported in
the Python interface, not because they’re difficult to implement, but
because no one has needed them yet.  Feel free to add them and then
submit a patch.  Also, we don’t yet have support for the menu library
associated with ncurses; feel free to add that.

  If you write an interesting little program, feel free to contribute it
as another demo.  We can always use more of them!

  The ncurses FAQ:
‘http://invisible-island.net/ncurses/ncurses.faq.html’


File: python.info,  Node: Descriptor HowTo Guide,  Next: Idioms and Anti-Idioms in Python,  Prev: Curses Programming with Python,  Up: Python HOWTOs

10.4 Descriptor HowTo Guide
===========================

     Author: Raymond Hettinger

     Contact: <python at rcn dot com>

* Menu:

* Abstract:: 
* Definition and Introduction:: 
* Descriptor Protocol:: 
* Invoking Descriptors: Invoking Descriptors<2>. 
* Descriptor Example:: 
* Properties:: 
* Functions and Methods:: 
* Static Methods and Class Methods:: 


File: python.info,  Node: Abstract,  Next: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.1 Abstract
---------------

Defines descriptors, summarizes the protocol, and shows how descriptors
are called.  Examines a custom descriptor and several built-in python
descriptors including functions, properties, static methods, and class
methods.  Shows how each works by giving a pure Python equivalent and a
sample application.

  Learning about descriptors not only provides access to a larger
toolset, it creates a deeper understanding of how Python works and an
appreciation for the elegance of its design.


File: python.info,  Node: Definition and Introduction,  Next: Descriptor Protocol,  Prev: Abstract,  Up: Descriptor HowTo Guide

10.4.2 Definition and Introduction
----------------------------------

In general, a descriptor is an object attribute with "binding behavior",
one whose attribute access has been overridden by methods in the
descriptor protocol.  Those methods are *note __get__(): 713, *note
__set__(): 714, and *note __delete__(): 715.  If any of those methods
are defined for an object, it is said to be a descriptor.

  The default behavior for attribute access is to get, set, or delete
the attribute from an object’s dictionary.  For instance, ‘a.x’ has a
lookup chain starting with ‘a.__dict__['x']’, then
‘type(a).__dict__['x']’, and continuing through the base classes of
‘type(a)’ excluding metaclasses.  If the looked-up value is an object
defining one of the descriptor methods, then Python may override the
default behavior and invoke the descriptor method instead.  Where this
occurs in the precedence chain depends on which descriptor methods were
defined.  Note that descriptors are only invoked for new style objects
or classes (a class is new style if it inherits from *note object: 1f1.
or *note type: 490.).

  Descriptors are a powerful, general purpose protocol.  They are the
mechanism behind properties, methods, static methods, class methods, and
*note super(): 37d.  They are used throughout Python itself to implement
the new style classes introduced in version 2.2.  Descriptors simplify
the underlying C-code and offer a flexible set of new tools for everyday
Python programs.


File: python.info,  Node: Descriptor Protocol,  Next: Invoking Descriptors<2>,  Prev: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.3 Descriptor Protocol
--------------------------

‘descr.__get__(self, obj, type=None) --> value’

  ‘descr.__set__(self, obj, value) --> None’

  ‘descr.__delete__(self, obj) --> None’

  That is all there is to it.  Define any of these methods and an object
is considered a descriptor and can override default behavior upon being
looked up as an attribute.

  If an object defines both *note __get__(): 713. and *note __set__():
714, it is considered a data descriptor.  Descriptors that only define
*note __get__(): 713. are called non-data descriptors (they are
typically used for methods but other uses are possible).

  Data and non-data descriptors differ in how overrides are calculated
with respect to entries in an instance’s dictionary.  If an instance’s
dictionary has an entry with the same name as a data descriptor, the
data descriptor takes precedence.  If an instance’s dictionary has an
entry with the same name as a non-data descriptor, the dictionary entry
takes precedence.

  To make a read-only data descriptor, define both *note __get__(): 713.
and *note __set__(): 714. with the *note __set__(): 714. raising an
*note AttributeError: 1f8. when called.  Defining the *note __set__():
714. method with an exception raising placeholder is enough to make it a
data descriptor.


File: python.info,  Node: Invoking Descriptors<2>,  Next: Descriptor Example,  Prev: Descriptor Protocol,  Up: Descriptor HowTo Guide

10.4.4 Invoking Descriptors
---------------------------

A descriptor can be called directly by its method name.  For example,
‘d.__get__(obj)’.

  Alternatively, it is more common for a descriptor to be invoked
automatically upon attribute access.  For example, ‘obj.d’ looks up ‘d’
in the dictionary of ‘obj’.  If ‘d’ defines the method *note __get__():
713, then ‘d.__get__(obj)’ is invoked according to the precedence rules
listed below.

  The details of invocation depend on whether ‘obj’ is an object or a
class.  Either way, descriptors only work for new style objects and
classes.  A class is new style if it is a subclass of *note object: 1f1.

  For objects, the machinery is in *note object.__getattribute__(): 33b.
which transforms ‘b.x’ into ‘type(b).__dict__['x'].__get__(b, type(b))’.
The implementation works through a precedence chain that gives data
descriptors priority over instance variables, instance variables
priority over non-data descriptors, and assigns lowest priority to *note
__getattr__(): 331. if provided.  The full C implementation can be found
in *note PyObject_GenericGetAttr(): 2b25. in Objects/object.c(1).

  For classes, the machinery is in ‘type.__getattribute__()’ which
transforms ‘B.x’ into ‘B.__dict__['x'].__get__(None, B)’.  In pure
Python, it looks like:

     def __getattribute__(self, key):
         "Emulate type_getattro() in Objects/typeobject.c"
         v = object.__getattribute__(self, key)
         if hasattr(v, '__get__'):
            return v.__get__(None, self)
         return v

  The important points to remember are:

   * descriptors are invoked by the *note __getattribute__(): 33b.
     method

   * overriding *note __getattribute__(): 33b. prevents automatic
     descriptor calls

   * *note __getattribute__(): 33b. is only available with new style
     classes and objects

   * *note object.__getattribute__(): 33b. and ‘type.__getattribute__()’
     make different calls to *note __get__(): 713.

   * data descriptors always override instance dictionaries.

   * non-data descriptors may be overridden by instance dictionaries.

  The object returned by ‘super()’ also has a custom *note
__getattribute__(): 33b. method for invoking descriptors.  The call
‘super(B, obj).m()’ searches ‘obj.__class__.__mro__’ for the base class
‘A’ immediately following ‘B’ and then returns
‘A.__dict__['m'].__get__(obj, B)’.  If not a descriptor, ‘m’ is returned
unchanged.  If not in the dictionary, ‘m’ reverts to a search using
*note object.__getattribute__(): 33b.

  Note, in Python 2.2, ‘super(B, obj).m()’ would only invoke *note
__get__(): 713. if ‘m’ was a data descriptor.  In Python 2.3, non-data
descriptors also get invoked unless an old-style class is involved.  The
implementation details are in ‘super_getattro()’ in
Objects/typeobject.c(2) and a pure Python equivalent can be found in
Guido’s Tutorial(3).

  The details above show that the mechanism for descriptors is embedded
in the *note __getattribute__(): 33b. methods for *note object: 1f1,
*note type: 490, and *note super(): 37d.  Classes inherit this machinery
when they derive from *note object: 1f1. or if they have a meta-class
providing similar functionality.  Likewise, classes can turn-off
descriptor invocation by overriding *note __getattribute__(): 33b.

   ---------- Footnotes ----------

   (1) 
http://svn.python.org/view/python/trunk/Objects/object.c?view=markup

   (2) 
http://svn.python.org/view/python/trunk/Objects/typeobject.c?view=markup

   (3) http://www.python.org/2.2.3/descrintro.html#cooperation


File: python.info,  Node: Descriptor Example,  Next: Properties,  Prev: Invoking Descriptors<2>,  Up: Descriptor HowTo Guide

10.4.5 Descriptor Example
-------------------------

The following code creates a class whose objects are data descriptors
which print a message for each get or set.  Overriding *note
__getattribute__(): 33b. is alternate approach that could do this for
every attribute.  However, this descriptor is useful for monitoring just
a few chosen attributes:

     class RevealAccess(object):
         """A data descriptor that sets and returns values
            normally and prints a message logging their access.
         """

         def __init__(self, initval=None, name='var'):
             self.val = initval
             self.name = name

         def __get__(self, obj, objtype):
             print 'Retrieving', self.name
             return self.val

         def __set__(self, obj, val):
             print 'Updating', self.name
             self.val = val

     >>> class MyClass(object):
         x = RevealAccess(10, 'var "x"')
         y = 5

     >>> m = MyClass()
     >>> m.x
     Retrieving var "x"
     10
     >>> m.x = 20
     Updating var "x"
     >>> m.x
     Retrieving var "x"
     20
     >>> m.y
     5

  The protocol is simple and offers exciting possibilities.  Several use
cases are so common that they have been packaged into individual
function calls.  Properties, bound and unbound methods, static methods,
and class methods are all based on the descriptor protocol.


File: python.info,  Node: Properties,  Next: Functions and Methods,  Prev: Descriptor Example,  Up: Descriptor HowTo Guide

10.4.6 Properties
-----------------

Calling *note property(): 487. is a succinct way of building a data
descriptor that triggers function calls upon access to an attribute.
Its signature is:

     property(fget=None, fset=None, fdel=None, doc=None) -> property attribute

  The documentation shows a typical use to define a managed attribute
‘x’:

     class C(object):
         def getx(self): return self.__x
         def setx(self, value): self.__x = value
         def delx(self): del self.__x
         x = property(getx, setx, delx, "I'm the 'x' property.")

  To see how *note property(): 487. is implemented in terms of the
descriptor protocol, here is a pure Python equivalent:

     class Property(object):
         "Emulate PyProperty_Type() in Objects/descrobject.c"

         def __init__(self, fget=None, fset=None, fdel=None, doc=None):
             self.fget = fget
             self.fset = fset
             self.fdel = fdel
             if doc is None and fget is not None:
                 doc = fget.__doc__
             self.__doc__ = doc

         def __get__(self, obj, objtype=None):
             if obj is None:
                 return self
             if self.fget is None:
                 raise AttributeError("unreadable attribute")
             return self.fget(obj)

         def __set__(self, obj, value):
             if self.fset is None:
                 raise AttributeError("can't set attribute")
             self.fset(obj, value)

         def __delete__(self, obj):
             if self.fdel is None:
                 raise AttributeError("can't delete attribute")
             self.fdel(obj)

         def getter(self, fget):
             return type(self)(fget, self.fset, self.fdel, self.__doc__)

         def setter(self, fset):
             return type(self)(self.fget, fset, self.fdel, self.__doc__)

         def deleter(self, fdel):
             return type(self)(self.fget, self.fset, fdel, self.__doc__)

  The *note property(): 487. builtin helps whenever a user interface has
granted attribute access and then subsequent changes require the
intervention of a method.

  For instance, a spreadsheet class may grant access to a cell value
through ‘Cell('b10').value’.  Subsequent improvements to the program
require the cell to be recalculated on every access; however, the
programmer does not want to affect existing client code accessing the
attribute directly.  The solution is to wrap access to the value
attribute in a property data descriptor:

     class Cell(object):
         . . .
         def getvalue(self, obj):
             "Recalculate cell before returning value"
             self.recalc()
             return obj._value
         value = property(getvalue)


File: python.info,  Node: Functions and Methods,  Next: Static Methods and Class Methods,  Prev: Properties,  Up: Descriptor HowTo Guide

10.4.7 Functions and Methods
----------------------------

Python’s object oriented features are built upon a function based
environment.  Using non-data descriptors, the two are merged seamlessly.

  Class dictionaries store methods as functions.  In a class definition,
methods are written using *note def: 3f4. and *note lambda: 403, the
usual tools for creating functions.  The only difference from regular
functions is that the first argument is reserved for the object
instance.  By Python convention, the instance reference is called _self_
but may be called _this_ or any other variable name.

  To support method calls, functions include the *note __get__(): 713.
method for binding methods during attribute access.  This means that all
functions are non-data descriptors which return bound or unbound methods
depending whether they are invoked from an object or a class.  In pure
python, it works like this:

     class Function(object):
         . . .
         def __get__(self, obj, objtype=None):
             "Simulate func_descr_get() in Objects/funcobject.c"
             return types.MethodType(self, obj, objtype)

  Running the interpreter shows how the function descriptor works in
practice:

     >>> class D(object):
          def f(self, x):
               return x

     >>> d = D()
     >>> D.__dict__['f'] # Stored internally as a function
     <function f at 0x00C45070>
     >>> D.f             # Get from a class becomes an unbound method
     <unbound method D.f>
     >>> d.f             # Get from an instance becomes a bound method
     <bound method D.f of <__main__.D object at 0x00B18C90>>

  The output suggests that bound and unbound methods are two different
types.  While they could have been implemented that way, the actual C
implementation of *note PyMethod_Type: 2d1c. in Objects/classobject.c(1)
is a single object with two different representations depending on
whether the ‘im_self’ field is set or is _NULL_ (the C equivalent of
_None_).

  Likewise, the effects of calling a method object depend on the
‘im_self’ field.  If set (meaning bound), the original function (stored
in the ‘im_func’ field) is called as expected with the first argument
set to the instance.  If unbound, all of the arguments are passed
unchanged to the original function.  The actual C implementation of
‘instancemethod_call()’ is only slightly more complex in that it
includes some type checking.

   ---------- Footnotes ----------

   (1) 
http://svn.python.org/view/python/trunk/Objects/classobject.c?view=markup


File: python.info,  Node: Static Methods and Class Methods,  Prev: Functions and Methods,  Up: Descriptor HowTo Guide

10.4.8 Static Methods and Class Methods
---------------------------------------

Non-data descriptors provide a simple mechanism for variations on the
usual patterns of binding functions into methods.

  To recap, functions have a *note __get__(): 713. method so that they
can be converted to a method when accessed as attributes.  The non-data
descriptor transforms a ‘obj.f(*args)’ call into ‘f(obj, *args)’.
Calling ‘klass.f(*args)’ becomes ‘f(*args)’.

  This chart summarizes the binding and its two most useful variants:

     Transformation        Called from an Object      Called from a Class
                                                      
     ------------------------------------------------------------------------
                                                      
     function              f(obj, *args)              f(*args)
                                                      
                                                      
     staticmethod          f(*args)                   f(*args)
                                                      
                                                      
     classmethod           f(type(obj), *args)        f(klass, *args)
                                                      

  Static methods return the underlying function without changes.
Calling either ‘c.f’ or ‘C.f’ is the equivalent of a direct lookup into
‘object.__getattribute__(c, "f")’ or ‘object.__getattribute__(C, "f")’.
As a result, the function becomes identically accessible from either an
object or a class.

  Good candidates for static methods are methods that do not reference
the ‘self’ variable.

  For instance, a statistics package may include a container class for
experimental data.  The class provides normal methods for computing the
average, mean, median, and other descriptive statistics that depend on
the data.  However, there may be useful functions which are conceptually
related but do not depend on the data.  For instance, ‘erf(x)’ is handy
conversion routine that comes up in statistical work but does not
directly depend on a particular dataset.  It can be called either from
an object or the class: ‘s.erf(1.5) --> .9332’ or ‘Sample.erf(1.5) -->
.9332’.

  Since staticmethods return the underlying function with no changes,
the example calls are unexciting:

     >>> class E(object):
          def f(x):
               print x
          f = staticmethod(f)

     >>> print E.f(3)
     3
     >>> print E().f(3)
     3

  Using the non-data descriptor protocol, a pure Python version of *note
staticmethod(): 3f5. would look like this:

     class StaticMethod(object):
      "Emulate PyStaticMethod_Type() in Objects/funcobject.c"

      def __init__(self, f):
           self.f = f

      def __get__(self, obj, objtype=None):
           return self.f

  Unlike static methods, class methods prepend the class reference to
the argument list before calling the function.  This format is the same
for whether the caller is an object or a class:

     >>> class E(object):
          def f(klass, x):
               return klass.__name__, x
          f = classmethod(f)

     >>> print E.f(3)
     ('E', 3)
     >>> print E().f(3)
     ('E', 3)

  This behavior is useful whenever the function only needs to have a
class reference and does not care about any underlying data.  One use
for classmethods is to create alternate class constructors.  In Python
2.3, the classmethod *note dict.fromkeys(): 8fc. creates a new
dictionary from a list of keys.  The pure Python equivalent is:

     class Dict(object):
         . . .
         def fromkeys(klass, iterable, value=None):
             "Emulate dict_fromkeys() in Objects/dictobject.c"
             d = klass()
             for key in iterable:
                 d[key] = value
             return d
         fromkeys = classmethod(fromkeys)

  Now a new dictionary of unique keys can be constructed like this:

     >>> Dict.fromkeys('abracadabra')
     {'a': None, 'r': None, 'b': None, 'c': None, 'd': None}

  Using the non-data descriptor protocol, a pure Python version of *note
classmethod(): 3f6. would look like this:

     class ClassMethod(object):
          "Emulate PyClassMethod_Type() in Objects/funcobject.c"

          def __init__(self, f):
               self.f = f

          def __get__(self, obj, klass=None):
               if klass is None:
                    klass = type(obj)
               def newfunc(*args):
                    return self.f(klass, *args)
               return newfunc


File: python.info,  Node: Idioms and Anti-Idioms in Python,  Next: Functional Programming HOWTO,  Prev: Descriptor HowTo Guide,  Up: Python HOWTOs

10.5 Idioms and Anti-Idioms in Python
=====================================

     Author: Moshe Zadka

  This document is placed in the public domain.

Abstract
........

This document can be considered a companion to the tutorial.  It shows
how to use Python, and even more importantly, how _not_ to use Python.

* Menu:

* Language Constructs You Should Not Use:: 
* Exceptions: Exceptions<8>. 
* Using the Batteries:: 
* Using Backslash to Continue Statements:: 

Language Constructs You Should Not Use

* from module import *:: 
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends. 
* from module import name1, name2: from module import name1 name2. 
* except;: except. 

from module import *

* Inside Function Definitions:: 
* At Module Level:: 
* When It Is Just Fine:: 


File: python.info,  Node: Language Constructs You Should Not Use,  Next: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

10.5.1 Language Constructs You Should Not Use
---------------------------------------------

While Python has relatively few gotchas compared to other languages, it
still has some constructs which are only useful in corner cases, or are
plain dangerous.

* Menu:

* from module import *:: 
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends. 
* from module import name1, name2: from module import name1 name2. 
* except;: except. 

from module import *

* Inside Function Definitions:: 
* At Module Level:: 
* When It Is Just Fine:: 


File: python.info,  Node: from module import *,  Next: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

10.5.1.1 from module import *
.............................

* Menu:

* Inside Function Definitions:: 
* At Module Level:: 
* When It Is Just Fine:: 


File: python.info,  Node: Inside Function Definitions,  Next: At Module Level,  Up: from module import *

10.5.1.2 Inside Function Definitions
....................................

‘from module import *’ is _invalid_ inside function definitions.  While
many versions of Python do not check for the invalidity, it does not
make it more valid, no more than having a smart lawyer makes a man
innocent.  Do not use it like that ever.  Even in versions where it was
accepted, it made the function execution slower, because the compiler
could not be certain which names were local and which were global.  In
Python 2.1 this construct causes warnings, and sometimes even errors.


File: python.info,  Node: At Module Level,  Next: When It Is Just Fine,  Prev: Inside Function Definitions,  Up: from module import *

10.5.1.3 At Module Level
........................

While it is valid to use ‘from module import *’ at module level it is
usually a bad idea.  For one, this loses an important property Python
otherwise has — you can know where each toplevel name is defined by a
simple "search" function in your favourite editor.  You also open
yourself to trouble in the future, if some module grows additional
functions or classes.

  One of the most awful questions asked on the newsgroup is why this
code:

     f = open("www")
     f.read()

  does not work.  Of course, it works just fine (assuming you have a
file called "www".)  But it does not work if somewhere in the module,
the statement ‘from os import *’ is present.  The *note os: 128. module
has a function called *note open(): 2d6. which returns an integer.
While it is very useful, shadowing a builtin is one of its least useful
properties.

  Remember, you can never know for sure what names a module exports, so
either take what you need — ‘from module import name1, name2’, or keep
them in the module and access on a per-need basis — ‘import module;print
module.name’.


File: python.info,  Node: When It Is Just Fine,  Prev: At Module Level,  Up: from module import *

10.5.1.4 When It Is Just Fine
.............................

There are situations in which ‘from module import *’ is just fine:

   * The interactive prompt.  For example, ‘from math import *’ makes
     Python an amazing scientific calculator.

   * When extending a module in C with a module in Python.

   * When the module advertises itself as ‘from import *’ safe.


File: python.info,  Node: Unadorned exec execfile and friends,  Next: from module import name1 name2,  Prev: from module import *,  Up: Language Constructs You Should Not Use

10.5.1.5 Unadorned ‘exec’, ‘execfile()’ and friends
...................................................

The word "unadorned" refers to the use without an explicit dictionary,
in which case those constructs evaluate code in the _current_
environment.  This is dangerous for the same reasons ‘from import *’ is
dangerous — it might step over variables you are counting on and mess up
things for the rest of your code.  Simply do not do that.

  Bad examples:

     >>> for name in sys.argv[1:]:
     >>>     exec "%s=1" % name
     >>> def func(s, **kw):
     >>>     for var, val in kw.items():
     >>>         exec "s.%s=val" % var  # invalid!
     >>> execfile("handler.py")
     >>> handle()

  Good examples:

     >>> d = {}
     >>> for name in sys.argv[1:]:
     >>>     d[name] = 1
     >>> def func(s, **kw):
     >>>     for var, val in kw.items():
     >>>         setattr(s, var, val)
     >>> d={}
     >>> execfile("handle.py", d, d)
     >>> handle = d['handle']
     >>> handle()


File: python.info,  Node: from module import name1 name2,  Next: except,  Prev: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

10.5.1.6 from module import name1, name2
........................................

This is a "don’t" which is much weaker than the previous "don’t"s but is
still something you should not do if you don’t have good reasons to do
that.  The reason it is usually a bad idea is because you suddenly have
an object which lives in two separate namespaces.  When the binding in
one namespace changes, the binding in the other will not, so there will
be a discrepancy between them.  This happens when, for example, one
module is reloaded, or changes the definition of a function at runtime.

  Bad example:

     # foo.py
     a = 1

     # bar.py
     from foo import a
     if something():
         a = 2 # danger: foo.a != a

  Good example:

     # foo.py
     a = 1

     # bar.py
     import foo
     if something():
         foo.a = 2


File: python.info,  Node: except,  Prev: from module import name1 name2,  Up: Language Constructs You Should Not Use

10.5.1.7 except:
................

Python has the ‘except:’ clause, which catches all exceptions.  Since
_every_ error in Python raises an exception, using ‘except:’ can make
many programming errors look like runtime problems, which hinders the
debugging process.

  The following code shows a great example of why this is bad:

     try:
         foo = opne("file") # misspelled "open"
     except:
         sys.exit("could not open file!")

  The second line triggers a *note NameError: 3a3, which is caught by
the except clause.  The program will exit, and the error message the
program prints will make you think the problem is the readability of
‘"file"’ when in fact the real error has nothing to do with ‘"file"’.

  A better way to write the above is

     try:
         foo = opne("file")
     except IOError:
         sys.exit("could not open file")

  When this is run, Python will produce a traceback showing the *note
NameError: 3a3, and it will be immediately apparent what needs to be
fixed.

  Because ‘except:’ catches _all_ exceptions, including *note
SystemExit: 332, *note KeyboardInterrupt: 251, and *note GeneratorExit:
337. (which is not an error and should not normally be caught by user
code), using a bare ‘except:’ is almost never a good idea.  In
situations where you need to catch all "normal" errors, such as in a
framework that runs callbacks, you can catch the base class for all
normal exceptions, *note Exception: 339.  Unfortunately in Python 2.x it
is possible for third-party code to raise exceptions that do not inherit
from *note Exception: 339, so in Python 2.x there are some cases where
you may have to use a bare ‘except:’ and manually re-raise the
exceptions you don’t want to catch.


File: python.info,  Node: Exceptions<8>,  Next: Using the Batteries,  Prev: Language Constructs You Should Not Use,  Up: Idioms and Anti-Idioms in Python

10.5.2 Exceptions
-----------------

Exceptions are a useful feature of Python.  You should learn to raise
them whenever something unexpected occurs, and catch them only where you
can do something about them.

  The following is a very popular anti-idiom

     def get_status(file):
         if not os.path.exists(file):
             print "file not found"
             sys.exit(1)
         return open(file).readline()

  Consider the case where the file gets deleted between the time the
call to *note os.path.exists(): dfe. is made and the time *note open():
2d6. is called.  In that case the last line will raise an *note IOError:
1fa.  The same thing would happen if _file_ exists but has no read
permission.  Since testing this on a normal machine on existent and
non-existent files makes it seem bugless, the test results will seem
fine, and the code will get shipped.  Later an unhandled *note IOError:
1fa. (or perhaps some other *note EnvironmentError: 94c.) escapes to the
user, who gets to watch the ugly traceback.

  Here is a somewhat better way to do it.

     def get_status(file):
         try:
             return open(file).readline()
         except EnvironmentError as err:
             print "Unable to open file: {}".format(err)
             sys.exit(1)

  In this version, _either_ the file gets opened and the line is read
(so it works even on flaky NFS or SMB connections), or an error message
is printed that provides all the available information on why the open
failed, and the application is aborted.

  However, even this version of ‘get_status()’ makes too many
assumptions — that it will only be used in a short running script, and
not, say, in a long running server.  Sure, the caller could do something
like

     try:
         status = get_status(log)
     except SystemExit:
         status = None

  But there is a better way.  You should try to use as few ‘except’
clauses in your code as you can — the ones you do use will usually be
inside calls which should always succeed, or a catch-all in a main
function.

  So, an even better version of ‘get_status()’ is probably

     def get_status(file):
         return open(file).readline()

  The caller can deal with the exception if it wants (for example, if it
tries several files in a loop), or just let the exception filter upwards
to _its_ caller.

  But the last version still has a serious problem — due to
implementation details in CPython, the file would not be closed when an
exception is raised until the exception handler finishes; and, worse, in
other implementations (e.g., Jython) it might not be closed at all
regardless of whether or not an exception is raised.

  The best version of this function uses the ‘open()’ call as a context
manager, which will ensure that the file gets closed as soon as the
function returns:

     def get_status(file):
         with open(file) as fp:
             return fp.readline()


File: python.info,  Node: Using the Batteries,  Next: Using Backslash to Continue Statements,  Prev: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

10.5.3 Using the Batteries
--------------------------

Every so often, people seem to be writing stuff in the Python library
again, usually poorly.  While the occasional module has a poor
interface, it is usually much better to use the rich standard library
and data types that come with Python than inventing your own.

  A useful module very few people know about is *note os.path: 129.  It
always has the correct path arithmetic for your operating system, and
will usually be much better than whatever you come up with yourself.

  Compare:

     # ugh!
     return dir+"/"+file
     # better
     return os.path.join(dir, file)

  More useful functions in *note os.path: 129.: ‘basename()’,
‘dirname()’ and ‘splitext()’.

  There are also many useful built-in functions people seem not to be
aware of for some reason: *note min(): 224. and *note max(): 225. can
find the minimum/maximum of any sequence with comparable semantics, for
example, yet many people write their own *note max(): 225./*note min():
224.  Another highly useful function is *note reduce(): 2e9. which can
be used to repeatly apply a binary operation to a sequence, reducing it
to a single value.  For example, compute a factorial with a series of
multiply operations:

     >>> n = 4
     >>> import operator
     >>> reduce(operator.mul, range(1, n+1))
     24

  When it comes to parsing numbers, note that *note float(): 1eb, *note
int(): 1f2. and *note long(): 1f3. all accept string arguments and will
reject ill-formed strings by raising an *note ValueError: 236.


File: python.info,  Node: Using Backslash to Continue Statements,  Prev: Using the Batteries,  Up: Idioms and Anti-Idioms in Python

10.5.4 Using Backslash to Continue Statements
---------------------------------------------

Since Python treats a newline as a statement terminator, and since
statements are often more than is comfortable to put in one line, many
people do:

     if foo.bar()['first'][0] == baz.quux(1, 2)[5:9] and \
        calculate_number(10, 20) != forbulate(500, 360):
           pass

  You should realize that this is dangerous: a stray space after the ‘\’
would make this line wrong, and stray spaces are notoriously hard to see
in editors.  In this case, at least it would be a syntax error, but if
the code was:

     value = foo.bar()['first'][0]*baz.quux(1, 2)[5:9] \
             + calculate_number(10, 20)*forbulate(500, 360)

  then it would just be subtly wrong.

  It is usually much better to use the implicit continuation inside
parenthesis:

  This version is bulletproof:

     value = (foo.bar()['first'][0]*baz.quux(1, 2)[5:9]
             + calculate_number(10, 20)*forbulate(500, 360))


File: python.info,  Node: Functional Programming HOWTO,  Next: Logging HOWTO,  Prev: Idioms and Anti-Idioms in Python,  Up: Python HOWTOs

10.6 Functional Programming HOWTO
=================================

     Author: A. M. Kuchling

     Release: 0.31

  In this document, we’ll take a tour of Python’s features suitable for
implementing programs in a functional style.  After an introduction to
the concepts of functional programming, we’ll look at language features
such as *note iterator: 87f.s and *note generator: 5dc.s and relevant
library modules such as *note itertools: fa. and *note functools: d9.

* Menu:

* Introduction: Introduction<12>. 
* Iterators: Iterators<2>. 
* Generator expressions and list comprehensions:: 
* Generators: Generators<2>. 
* Built-in functions:: 
* Small functions and the lambda expression:: 
* The itertools module:: 
* The functools module:: 
* Revision History and Acknowledgements:: 
* References:: 

Introduction

* Formal provability:: 
* Modularity:: 
* Ease of debugging and testing:: 
* Composability:: 

Iterators

* Data Types That Support Iterators:: 

Generators

* Passing values into a generator:: 

The itertools module

* Creating new iterators:: 
* Calling functions on elements:: 
* Selecting elements:: 
* Grouping elements:: 

The functools module

* The operator module:: 

References

* General:: 
* Python-specific:: 
* Python documentation:: 


File: python.info,  Node: Introduction<12>,  Next: Iterators<2>,  Up: Functional Programming HOWTO

10.6.1 Introduction
-------------------

This section explains the basic concept of functional programming; if
you’re just interested in learning about Python language features, skip
to the next section.

  Programming languages support decomposing problems in several
different ways:

   * Most programming languages are *procedural*: programs are lists of
     instructions that tell the computer what to do with the program’s
     input.  C, Pascal, and even Unix shells are procedural languages.

   * In *declarative* languages, you write a specification that
     describes the problem to be solved, and the language implementation
     figures out how to perform the computation efficiently.  SQL is the
     declarative language you’re most likely to be familiar with; a SQL
     query describes the data set you want to retrieve, and the SQL
     engine decides whether to scan tables or use indexes, which
     subclauses should be performed first, etc.

   * *Object-oriented* programs manipulate collections of objects.
     Objects have internal state and support methods that query or
     modify this internal state in some way.  Smalltalk and Java are
     object-oriented languages.  C++ and Python are languages that
     support object-oriented programming, but don’t force the use of
     object-oriented features.

   * *Functional* programming decomposes a problem into a set of
     functions.  Ideally, functions only take inputs and produce
     outputs, and don’t have any internal state that affects the output
     produced for a given input.  Well-known functional languages
     include the ML family (Standard ML, OCaml, and other variants) and
     Haskell.

  The designers of some computer languages choose to emphasize one
particular approach to programming.  This often makes it difficult to
write programs that use a different approach.  Other languages are
multi-paradigm languages that support several different approaches.
Lisp, C++, and Python are multi-paradigm; you can write programs or
libraries that are largely procedural, object-oriented, or functional in
all of these languages.  In a large program, different sections might be
written using different approaches; the GUI might be object-oriented
while the processing logic is procedural or functional, for example.

  In a functional program, input flows through a set of functions.  Each
function operates on its input and produces some output.  Functional
style discourages functions with side effects that modify internal state
or make other changes that aren’t visible in the function’s return
value.  Functions that have no side effects at all are called *purely
functional*.  Avoiding side effects means not using data structures that
get updated as a program runs; every function’s output must only depend
on its input.

  Some languages are very strict about purity and don’t even have
assignment statements such as ‘a=3’ or ‘c = a + b’, but it’s difficult
to avoid all side effects.  Printing to the screen or writing to a disk
file are side effects, for example.  For example, in Python a ‘print’
statement or a ‘time.sleep(1)’ both return no useful value; they’re only
called for their side effects of sending some text to the screen or
pausing execution for a second.

  Python programs written in functional style usually won’t go to the
extreme of avoiding all I/O or all assignments; instead, they’ll provide
a functional-appearing interface but will use non-functional features
internally.  For example, the implementation of a function will still
use assignments to local variables, but won’t modify global variables or
have other side effects.

  Functional programming can be considered the opposite of
object-oriented programming.  Objects are little capsules containing
some internal state along with a collection of method calls that let you
modify this state, and programs consist of making the right set of state
changes.  Functional programming wants to avoid state changes as much as
possible and works with data flowing between functions.  In Python you
might combine the two approaches by writing functions that take and
return instances representing objects in your application (e-mail
messages, transactions, etc.).

  Functional design may seem like an odd constraint to work under.  Why
should you avoid objects and side effects?  There are theoretical and
practical advantages to the functional style:

   * Formal provability.

   * Modularity.

   * Composability.

   * Ease of debugging and testing.

* Menu:

* Formal provability:: 
* Modularity:: 
* Ease of debugging and testing:: 
* Composability:: 


File: python.info,  Node: Formal provability,  Next: Modularity,  Up: Introduction<12>

10.6.1.1 Formal provability
...........................

A theoretical benefit is that it’s easier to construct a mathematical
proof that a functional program is correct.

  For a long time researchers have been interested in finding ways to
mathematically prove programs correct.  This is different from testing a
program on numerous inputs and concluding that its output is usually
correct, or reading a program’s source code and concluding that the code
looks right; the goal is instead a rigorous proof that a program
produces the right result for all possible inputs.

  The technique used to prove programs correct is to write down
*invariants*, properties of the input data and of the program’s
variables that are always true.  For each line of code, you then show
that if invariants X and Y are true *before* the line is executed, the
slightly different invariants X’ and Y’ are true *after* the line is
executed.  This continues until you reach the end of the program, at
which point the invariants should match the desired conditions on the
program’s output.

  Functional programming’s avoidance of assignments arose because
assignments are difficult to handle with this technique; assignments can
break invariants that were true before the assignment without producing
any new invariants that can be propagated onward.

  Unfortunately, proving programs correct is largely impractical and not
relevant to Python software.  Even trivial programs require proofs that
are several pages long; the proof of correctness for a moderately
complicated program would be enormous, and few or none of the programs
you use daily (the Python interpreter, your XML parser, your web
browser) could be proven correct.  Even if you wrote down or generated a
proof, there would then be the question of verifying the proof; maybe
there’s an error in it, and you wrongly believe you’ve proved the
program correct.


File: python.info,  Node: Modularity,  Next: Ease of debugging and testing,  Prev: Formal provability,  Up: Introduction<12>

10.6.1.2 Modularity
...................

A more practical benefit of functional programming is that it forces you
to break apart your problem into small pieces.  Programs are more
modular as a result.  It’s easier to specify and write a small function
that does one thing than a large function that performs a complicated
transformation.  Small functions are also easier to read and to check
for errors.


File: python.info,  Node: Ease of debugging and testing,  Next: Composability,  Prev: Modularity,  Up: Introduction<12>

10.6.1.3 Ease of debugging and testing
......................................

Testing and debugging a functional-style program is easier.

  Debugging is simplified because functions are generally small and
clearly specified.  When a program doesn’t work, each function is an
interface point where you can check that the data are correct.  You can
look at the intermediate inputs and outputs to quickly isolate the
function that’s responsible for a bug.

  Testing is easier because each function is a potential subject for a
unit test.  Functions don’t depend on system state that needs to be
replicated before running a test; instead you only have to synthesize
the right input and then check that the output matches expectations.


File: python.info,  Node: Composability,  Prev: Ease of debugging and testing,  Up: Introduction<12>

10.6.1.4 Composability
......................

As you work on a functional-style program, you’ll write a number of
functions with varying inputs and outputs.  Some of these functions will
be unavoidably specialized to a particular application, but others will
be useful in a wide variety of programs.  For example, a function that
takes a directory path and returns all the XML files in the directory,
or a function that takes a filename and returns its contents, can be
applied to many different situations.

  Over time you’ll form a personal library of utilities.  Often you’ll
assemble new programs by arranging existing functions in a new
configuration and writing a few functions specialized for the current
task.


File: python.info,  Node: Iterators<2>,  Next: Generator expressions and list comprehensions,  Prev: Introduction<12>,  Up: Functional Programming HOWTO

10.6.2 Iterators
----------------

I’ll start by looking at a Python language feature that’s an important
foundation for writing functional-style programs: iterators.

  An iterator is an object representing a stream of data; this object
returns the data one element at a time.  A Python iterator must support
a method called ‘next()’ that takes no arguments and always returns the
next element of the stream.  If there are no more elements in the
stream, ‘next()’ must raise the ‘StopIteration’ exception.  Iterators
don’t have to be finite, though; it’s perfectly reasonable to write an
iterator that produces an infinite stream of data.

  The built-in *note iter(): 320. function takes an arbitrary object and
tries to return an iterator that will return the object’s contents or
elements, raising *note TypeError: 218. if the object doesn’t support
iteration.  Several of Python’s built-in data types support iteration,
the most common being lists and dictionaries.  An object is called an
*iterable* object if you can get an iterator for it.

  You can experiment with the iteration interface manually:

     >>> L = [1,2,3]
     >>> it = iter(L)
     >>> print it
     <...iterator object at ...>
     >>> it.next()
     1
     >>> it.next()
     2
     >>> it.next()
     3
     >>> it.next()
     Traceback (most recent call last):
       File "<stdin>", line 1, in ?
     StopIteration
     >>>

  Python expects iterable objects in several different contexts, the
most important being the ‘for’ statement.  In the statement ‘for X in
Y’, Y must be an iterator or some object for which ‘iter()’ can create
an iterator.  These two statements are equivalent:

     for i in iter(obj):
         print i

     for i in obj:
         print i

  Iterators can be materialized as lists or tuples by using the *note
list(): 3bc. or *note tuple(): 408. constructor functions:

     >>> L = [1,2,3]
     >>> iterator = iter(L)
     >>> t = tuple(iterator)
     >>> t
     (1, 2, 3)

  Sequence unpacking also supports iterators: if you know an iterator
will return N elements, you can unpack them into an N-tuple:

     >>> L = [1,2,3]
     >>> iterator = iter(L)
     >>> a,b,c = iterator
     >>> a,b,c
     (1, 2, 3)

  Built-in functions such as *note max(): 225. and *note min(): 224. can
take a single iterator argument and will return the largest or smallest
element.  The ‘"in"’ and ‘"not in"’ operators also support iterators: ‘X
in iterator’ is true if X is found in the stream returned by the
iterator.  You’ll run into obvious problems if the iterator is infinite;
‘max()’, ‘min()’ will never return, and if the element X never appears
in the stream, the ‘"in"’ and ‘"not in"’ operators won’t return either.

  Note that you can only go forward in an iterator; there’s no way to
get the previous element, reset the iterator, or make a copy of it.
Iterator objects can optionally provide these additional capabilities,
but the iterator protocol only specifies the ‘next()’ method.  Functions
may therefore consume all of the iterator’s output, and if you need to
do something different with the same stream, you’ll have to create a new
iterator.

* Menu:

* Data Types That Support Iterators:: 


File: python.info,  Node: Data Types That Support Iterators,  Up: Iterators<2>

10.6.2.1 Data Types That Support Iterators
..........................................

We’ve already seen how lists and tuples support iterators.  In fact, any
Python sequence type, such as strings, will automatically support
creation of an iterator.

  Calling *note iter(): 320. on a dictionary returns an iterator that
will loop over the dictionary’s keys:

     >>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
     ...      'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
     >>> for key in m:
     ...     print key, m[key]
     Mar 3
     Feb 2
     Aug 8
     Sep 9
     Apr 4
     Jun 6
     Jul 7
     Jan 1
     May 5
     Nov 11
     Dec 12
     Oct 10

  Note that the order is essentially random, because it’s based on the
hash ordering of the objects in the dictionary.

  Applying ‘iter()’ to a dictionary always loops over the keys, but
dictionaries have methods that return other iterators.  If you want to
iterate over keys, values, or key/value pairs, you can explicitly call
the ‘iterkeys()’, ‘itervalues()’, or ‘iteritems()’ methods to get an
appropriate iterator.

  The *note dict(): 305. constructor can accept an iterator that returns
a finite stream of ‘(key, value)’ tuples:

     >>> L = [('Italy', 'Rome'), ('France', 'Paris'), ('US', 'Washington DC')]
     >>> dict(iter(L))
     {'Italy': 'Rome', 'US': 'Washington DC', 'France': 'Paris'}

  Files also support iteration by calling the ‘readline()’ method until
there are no more lines in the file.  This means you can read each line
of a file like this:

     for line in file:
         # do something for each line
         ...

  Sets can take their contents from an iterable and let you iterate over
the set’s elements:

     S = set((2, 3, 5, 7, 11, 13))
     for i in S:
         print i


File: python.info,  Node: Generator expressions and list comprehensions,  Next: Generators<2>,  Prev: Iterators<2>,  Up: Functional Programming HOWTO

10.6.3 Generator expressions and list comprehensions
----------------------------------------------------

Two common operations on an iterator’s output are 1) performing some
operation for every element, 2) selecting a subset of elements that meet
some condition.  For example, given a list of strings, you might want to
strip off trailing whitespace from each line or extract all the strings
containing a given substring.

  List comprehensions and generator expressions (short form: "listcomps"
and "genexps") are a concise notation for such operations, borrowed from
the functional programming language Haskell (‘http://www.haskell.org/’).
You can strip all the whitespace from a stream of strings with the
following code:

     line_list = ['  line 1\n', 'line 2  \n', ...]

     # Generator expression -- returns iterator
     stripped_iter = (line.strip() for line in line_list)

     # List comprehension -- returns list
     stripped_list = [line.strip() for line in line_list]

  You can select only certain elements by adding an ‘"if"’ condition:

     stripped_list = [line.strip() for line in line_list
                      if line != ""]

  With a list comprehension, you get back a Python list; ‘stripped_list’
is a list containing the resulting lines, not an iterator.  Generator
expressions return an iterator that computes the values as necessary,
not needing to materialize all the values at once.  This means that list
comprehensions aren’t useful if you’re working with iterators that
return an infinite stream or a very large amount of data.  Generator
expressions are preferable in these situations.

  Generator expressions are surrounded by parentheses ("()") and list
comprehensions are surrounded by square brackets ("[]").  Generator
expressions have the form:

     ( expression for expr in sequence1
                  if condition1
                  for expr2 in sequence2
                  if condition2
                  for expr3 in sequence3 ...
                  if condition3
                  for exprN in sequenceN
                  if conditionN )

  Again, for a list comprehension only the outside brackets are
different (square brackets instead of parentheses).

  The elements of the generated output will be the successive values of
‘expression’.  The ‘if’ clauses are all optional; if present,
‘expression’ is only evaluated and added to the result when ‘condition’
is true.

  Generator expressions always have to be written inside parentheses,
but the parentheses signalling a function call also count.  If you want
to create an iterator that will be immediately passed to a function you
can write:

     obj_total = sum(obj.count for obj in list_all_objects())

  The ‘for...in’ clauses contain the sequences to be iterated over.  The
sequences do not have to be the same length, because they are iterated
over from left to right, *not* in parallel.  For each element in
‘sequence1’, ‘sequence2’ is looped over from the beginning.  ‘sequence3’
is then looped over for each resulting pair of elements from ‘sequence1’
and ‘sequence2’.

  To put it another way, a list comprehension or generator expression is
equivalent to the following Python code:

     for expr1 in sequence1:
         if not (condition1):
             continue   # Skip this element
         for expr2 in sequence2:
             if not (condition2):
                 continue    # Skip this element
             ...
             for exprN in sequenceN:
                  if not (conditionN):
                      continue   # Skip this element

                  # Output the value of
                  # the expression.

  This means that when there are multiple ‘for...in’ clauses but no ‘if’
clauses, the length of the resulting output will be equal to the product
of the lengths of all the sequences.  If you have two lists of length 3,
the output list is 9 elements long:

     >>> seq1 = 'abc'
     >>> seq2 = (1,2,3)
     >>> [(x,y) for x in seq1 for y in seq2]
     [('a', 1), ('a', 2), ('a', 3),
      ('b', 1), ('b', 2), ('b', 3),
      ('c', 1), ('c', 2), ('c', 3)]

  To avoid introducing an ambiguity into Python’s grammar, if
‘expression’ is creating a tuple, it must be surrounded with
parentheses.  The first list comprehension below is a syntax error,
while the second one is correct:

     # Syntax error
     [ x,y for x in seq1 for y in seq2]
     # Correct
     [ (x,y) for x in seq1 for y in seq2]


File: python.info,  Node: Generators<2>,  Next: Built-in functions,  Prev: Generator expressions and list comprehensions,  Up: Functional Programming HOWTO

10.6.4 Generators
-----------------

Generators are a special class of functions that simplify the task of
writing iterators.  Regular functions compute a value and return it, but
generators return an iterator that returns a stream of values.

  You’re doubtless familiar with how regular function calls work in
Python or C. When you call a function, it gets a private namespace where
its local variables are created.  When the function reaches a ‘return’
statement, the local variables are destroyed and the value is returned
to the caller.  A later call to the same function creates a new private
namespace and a fresh set of local variables.  But, what if the local
variables weren’t thrown away on exiting a function?  What if you could
later resume the function where it left off?  This is what generators
provide; they can be thought of as resumable functions.

  Here’s the simplest example of a generator function:

     def generate_ints(N):
         for i in range(N):
             yield i

  Any function containing a ‘yield’ keyword is a generator function;
this is detected by Python’s *note bytecode: 583. compiler which
compiles the function specially as a result.

  When you call a generator function, it doesn’t return a single value;
instead it returns a generator object that supports the iterator
protocol.  On executing the ‘yield’ expression, the generator outputs
the value of ‘i’, similar to a ‘return’ statement.  The big difference
between ‘yield’ and a ‘return’ statement is that on reaching a ‘yield’
the generator’s state of execution is suspended and local variables are
preserved.  On the next call to the generator’s ‘.next()’ method, the
function will resume executing.

  Here’s a sample usage of the ‘generate_ints()’ generator:

     >>> gen = generate_ints(3)
     >>> gen
     <generator object generate_ints at ...>
     >>> gen.next()
     0
     >>> gen.next()
     1
     >>> gen.next()
     2
     >>> gen.next()
     Traceback (most recent call last):
       File "stdin", line 1, in ?
       File "stdin", line 2, in generate_ints
     StopIteration

  You could equally write ‘for i in generate_ints(5)’, or ‘a,b,c =
generate_ints(3)’.

  Inside a generator function, the ‘return’ statement can only be used
without a value, and signals the end of the procession of values; after
executing a ‘return’ the generator cannot return any further values.
‘return’ with a value, such as ‘return 5’, is a syntax error inside a
generator function.  The end of the generator’s results can also be
indicated by raising ‘StopIteration’ manually, or by just letting the
flow of execution fall off the bottom of the function.

  You could achieve the effect of generators manually by writing your
own class and storing all the local variables of the generator as
instance variables.  For example, returning a list of integers could be
done by setting ‘self.count’ to 0, and having the ‘next()’ method
increment ‘self.count’ and return it.  However, for a moderately
complicated generator, writing a corresponding class can be much
messier.

  The test suite included with Python’s library, ‘test_generators.py’,
contains a number of more interesting examples.  Here’s one generator
that implements an in-order traversal of a tree using generators
recursively.

     # A recursive generator that generates Tree leaves in in-order.
     def inorder(t):
         if t:
             for x in inorder(t.left):
                 yield x

             yield t.label

             for x in inorder(t.right):
                 yield x

  Two other examples in ‘test_generators.py’ produce solutions for the
N-Queens problem (placing N queens on an NxN chess board so that no
queen threatens another) and the Knight’s Tour (finding a route that
takes a knight to every square of an NxN chessboard without visiting any
square twice).

* Menu:

* Passing values into a generator:: 


File: python.info,  Node: Passing values into a generator,  Up: Generators<2>

10.6.4.1 Passing values into a generator
........................................

In Python 2.4 and earlier, generators only produced output.  Once a
generator’s code was invoked to create an iterator, there was no way to
pass any new information into the function when its execution is
resumed.  You could hack together this ability by making the generator
look at a global variable or by passing in some mutable object that
callers then modify, but these approaches are messy.

  In Python 2.5 there’s a simple way to pass values into a generator.
*note yield: 2f7. became an expression, returning a value that can be
assigned to a variable or otherwise operated on:

     val = (yield i)

  I recommend that you *always* put parentheses around a ‘yield’
expression when you’re doing something with the returned value, as in
the above example.  The parentheses aren’t always necessary, but it’s
easier to always add them instead of having to remember when they’re
needed.

  (PEP 342 explains the exact rules, which are that a ‘yield’-expression
must always be parenthesized except when it occurs at the top-level
expression on the right-hand side of an assignment.  This means you can
write ‘val = yield i’ but have to use parentheses when there’s an
operation, as in ‘val = (yield i) + 12’.)

  Values are sent into a generator by calling its ‘send(value)’ method.
This method resumes the generator’s code and the ‘yield’ expression
returns the specified value.  If the regular ‘next()’ method is called,
the ‘yield’ returns ‘None’.

  Here’s a simple counter that increments by 1 and allows changing the
value of the internal counter.

     def counter (maximum):
         i = 0
         while i < maximum:
             val = (yield i)
             # If value provided, change counter
             if val is not None:
                 i = val
             else:
                 i += 1

  And here’s an example of changing the counter:

     >>> it = counter(10)
     >>> print it.next()
     0
     >>> print it.next()
     1
     >>> print it.send(8)
     8
     >>> print it.next()
     9
     >>> print it.next()
     Traceback (most recent call last):
       File "t.py", line 15, in ?
         print it.next()
     StopIteration

  Because ‘yield’ will often be returning ‘None’, you should always
check for this case.  Don’t just use its value in expressions unless
you’re sure that the ‘send()’ method will be the only method used to
resume your generator function.

  In addition to ‘send()’, there are two other new methods on
generators:

   * ‘throw(type, value=None, traceback=None)’ is used to raise an
     exception inside the generator; the exception is raised by the
     ‘yield’ expression where the generator’s execution is paused.

   * ‘close()’ raises a *note GeneratorExit: 337. exception inside the
     generator to terminate the iteration.  On receiving this exception,
     the generator’s code must either raise *note GeneratorExit: 337. or
     *note StopIteration: 333.; catching the exception and doing
     anything else is illegal and will trigger a *note RuntimeError:
     39b.  ‘close()’ will also be called by Python’s garbage collector
     when the generator is garbage-collected.

     If you need to run cleanup code when a *note GeneratorExit: 337.
     occurs, I suggest using a ‘try: ... finally:’ suite instead of
     catching *note GeneratorExit: 337.

  The cumulative effect of these changes is to turn generators from
one-way producers of information into both producers and consumers.

  Generators also become *coroutines*, a more generalized form of
subroutines.  Subroutines are entered at one point and exited at another
point (the top of the function, and a ‘return’ statement), but
coroutines can be entered, exited, and resumed at many different points
(the ‘yield’ statements).


File: python.info,  Node: Built-in functions,  Next: Small functions and the lambda expression,  Prev: Generators<2>,  Up: Functional Programming HOWTO

10.6.5 Built-in functions
-------------------------

Let’s look in more detail at built-in functions often used with
iterators.

  Two of Python’s built-in functions, *note map(): 304. and *note
filter(): 409, are somewhat obsolete; they duplicate the features of
list comprehensions but return actual lists instead of iterators.

  ‘map(f, iterA, iterB, ...)’ returns a list containing ‘f(iterA[0],
iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]), ...’.

     >>> def upper(s):
     ...     return s.upper()

     >>> map(upper, ['sentence', 'fragment'])
     ['SENTENCE', 'FRAGMENT']

     >>> [upper(s) for s in ['sentence', 'fragment']]
     ['SENTENCE', 'FRAGMENT']

  As shown above, you can achieve the same effect with a list
comprehension.  The *note itertools.imap(): d66. function does the same
thing but can handle infinite iterators; it’ll be discussed later, in
the section on the *note itertools: fa. module.

  ‘filter(predicate, iter)’ returns a list that contains all the
sequence elements that meet a certain condition, and is similarly
duplicated by list comprehensions.  A *predicate* is a function that
returns the truth value of some condition; for use with *note filter():
409, the predicate must take a single value.

     >>> def is_even(x):
     ...     return (x % 2) == 0

     >>> filter(is_even, range(10))
     [0, 2, 4, 6, 8]

  This can also be written as a list comprehension:

     >>> [x for x in range(10) if is_even(x)]
     [0, 2, 4, 6, 8]

  *note filter(): 409. also has a counterpart in the *note itertools:
fa. module, *note itertools.ifilter(): 881, that returns an iterator and
can therefore handle infinite sequences just as *note itertools.imap():
d66. can.

  ‘reduce(func, iter, [initial_value])’ doesn’t have a counterpart in
the *note itertools: fa. module because it cumulatively performs an
operation on all the iterable’s elements and therefore can’t be applied
to infinite iterables.  ‘func’ must be a function that takes two
elements and returns a single value.  *note reduce(): 2e9. takes the
first two elements A and B returned by the iterator and calculates
‘func(A, B)’.  It then requests the third element, C, calculates
‘func(func(A, B), C)’, combines this result with the fourth element
returned, and continues until the iterable is exhausted.  If the
iterable returns no values at all, a *note TypeError: 218. exception is
raised.  If the initial value is supplied, it’s used as a starting point
and ‘func(initial_value, A)’ is the first calculation.

     >>> import operator
     >>> reduce(operator.concat, ['A', 'BB', 'C'])
     'ABBC'
     >>> reduce(operator.concat, [])
     Traceback (most recent call last):
       ...
     TypeError: reduce() of empty sequence with no initial value
     >>> reduce(operator.mul, [1,2,3], 1)
     6
     >>> reduce(operator.mul, [], 1)
     1

  If you use *note operator.add(): d94. with *note reduce(): 2e9, you’ll
add up all the elements of the iterable.  This case is so common that
there’s a special built-in called *note sum(): 426. to compute it:

     >>> reduce(operator.add, [1,2,3,4], 0)
     10
     >>> sum([1,2,3,4])
     10
     >>> sum([])
     0

  For many uses of *note reduce(): 2e9, though, it can be clearer to
just write the obvious *note for: 2f0. loop:

     # Instead of:
     product = reduce(operator.mul, [1,2,3], 1)

     # You can write:
     product = 1
     for i in [1,2,3]:
         product *= i

  ‘enumerate(iter)’ counts off the elements in the iterable, returning
2-tuples containing the count and each element.

     >>> for item in enumerate(['subject', 'verb', 'object']):
     ...     print item
     (0, 'subject')
     (1, 'verb')
     (2, 'object')

  *note enumerate(): 427. is often used when looping through a list and
recording the indexes at which certain conditions are met:

     f = open('data.txt', 'r')
     for i, line in enumerate(f):
         if line.strip() == '':
             print 'Blank line at line #%i' % i

  ‘sorted(iterable, [cmp=None], [key=None], [reverse=False])’ collects
all the elements of the iterable into a list, sorts the list, and
returns the sorted result.  The ‘cmp’, ‘key’, and ‘reverse’ arguments
are passed through to the constructed list’s ‘.sort()’ method.

     >>> import random
     >>> # Generate 8 random numbers between [0, 10000)
     >>> rand_list = random.sample(range(10000), 8)
     >>> rand_list
     [769, 7953, 9828, 6431, 8442, 9878, 6213, 2207]
     >>> sorted(rand_list)
     [769, 2207, 6213, 6431, 7953, 8442, 9828, 9878]
     >>> sorted(rand_list, reverse=True)
     [9878, 9828, 8442, 7953, 6431, 6213, 2207, 769]

  (For a more detailed discussion of sorting, see the Sorting mini-HOWTO
in the Python wiki at ‘http://wiki.python.org/moin/HowTo/Sorting’.)

  The ‘any(iter)’ and ‘all(iter)’ built-ins look at the truth values of
an iterable’s contents.  *note any(): 3ae. returns ‘True’ if any element
in the iterable is a true value, and *note all(): 3af. returns ‘True’ if
all of the elements are true values:

     >>> any([0,1,0])
     True
     >>> any([0,0,0])
     False
     >>> any([1,1,1])
     True
     >>> all([0,1,0])
     False
     >>> all([0,0,0])
     False
     >>> all([1,1,1])
     True


File: python.info,  Node: Small functions and the lambda expression,  Next: The itertools module,  Prev: Built-in functions,  Up: Functional Programming HOWTO

10.6.6 Small functions and the lambda expression
------------------------------------------------

When writing functional-style programs, you’ll often need little
functions that act as predicates or that combine elements in some way.

  If there’s a Python built-in or a module function that’s suitable, you
don’t need to define a new function at all:

     stripped_lines = [line.strip() for line in lines]
     existing_files = filter(os.path.exists, file_list)

  If the function you need doesn’t exist, you need to write it.  One way
to write small functions is to use the ‘lambda’ statement.  ‘lambda’
takes a number of parameters and an expression combining these
parameters, and creates a small function that returns the value of the
expression:

     lowercase = lambda x: x.lower()

     print_assign = lambda name, value: name + '=' + str(value)

     adder = lambda x, y: x+y

  An alternative is to just use the ‘def’ statement and define a
function in the usual way:

     def lowercase(x):
         return x.lower()

     def print_assign(name, value):
         return name + '=' + str(value)

     def adder(x,y):
         return x + y

  Which alternative is preferable?  That’s a style question; my usual
course is to avoid using ‘lambda’.

  One reason for my preference is that ‘lambda’ is quite limited in the
functions it can define.  The result has to be computable as a single
expression, which means you can’t have multiway ‘if... elif... else’
comparisons or ‘try... except’ statements.  If you try to do too much in
a ‘lambda’ statement, you’ll end up with an overly complicated
expression that’s hard to read.  Quick, what’s the following code doing?

     total = reduce(lambda a, b: (0, a[1] + b[1]), items)[1]

  You can figure it out, but it takes time to disentangle the expression
to figure out what’s going on.  Using a short nested ‘def’ statements
makes things a little bit better:

     def combine (a, b):
         return 0, a[1] + b[1]

     total = reduce(combine, items)[1]

  But it would be best of all if I had simply used a ‘for’ loop:

     total = 0
     for a, b in items:
         total += b

  Or the *note sum(): 426. built-in and a generator expression:

     total = sum(b for a,b in items)

  Many uses of *note reduce(): 2e9. are clearer when written as ‘for’
loops.

  Fredrik Lundh once suggested the following set of rules for
refactoring uses of ‘lambda’:

  1. Write a lambda function.

  2. Write a comment explaining what the heck that lambda does.

  3. Study the comment for a while, and think of a name that captures
     the essence of the comment.

  4. Convert the lambda to a def statement, using that name.

  5. Remove the comment.

  I really like these rules, but you’re free to disagree about whether
this lambda-free style is better.


File: python.info,  Node: The itertools module,  Next: The functools module,  Prev: Small functions and the lambda expression,  Up: Functional Programming HOWTO

10.6.7 The itertools module
---------------------------

The *note itertools: fa. module contains a number of commonly-used
iterators as well as functions for combining several iterators.  This
section will introduce the module’s contents by showing small examples.

  The module’s functions fall into a few broad classes:

   * Functions that create a new iterator based on an existing iterator.

   * Functions for treating an iterator’s elements as function
     arguments.

   * Functions for selecting portions of an iterator’s output.

   * A function for grouping an iterator’s output.

* Menu:

* Creating new iterators:: 
* Calling functions on elements:: 
* Selecting elements:: 
* Grouping elements:: 


File: python.info,  Node: Creating new iterators,  Next: Calling functions on elements,  Up: The itertools module

10.6.7.1 Creating new iterators
...............................

‘itertools.count(n)’ returns an infinite stream of integers, increasing
by 1 each time.  You can optionally supply the starting number, which
defaults to 0:

     itertools.count() =>
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
     itertools.count(10) =>
       10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

  ‘itertools.cycle(iter)’ saves a copy of the contents of a provided
iterable and returns a new iterator that returns its elements from first
to last.  The new iterator will repeat these elements infinitely.

     itertools.cycle([1,2,3,4,5]) =>
       1, 2, 3, 4, 5, 1, 2, 3, 4, 5, ...

  ‘itertools.repeat(elem, [n])’ returns the provided element ‘n’ times,
or returns the element endlessly if ‘n’ is not provided.

     itertools.repeat('abc') =>
       abc, abc, abc, abc, abc, abc, abc, abc, abc, abc, ...
     itertools.repeat('abc', 5) =>
       abc, abc, abc, abc, abc

  ‘itertools.chain(iterA, iterB, ...)’ takes an arbitrary number of
iterables as input, and returns all the elements of the first iterator,
then all the elements of the second, and so on, until all of the
iterables have been exhausted.

     itertools.chain(['a', 'b', 'c'], (1, 2, 3)) =>
       a, b, c, 1, 2, 3

  ‘itertools.izip(iterA, iterB, ...)’ takes one element from each
iterable and returns them in a tuple:

     itertools.izip(['a', 'b', 'c'], (1, 2, 3)) =>
       ('a', 1), ('b', 2), ('c', 3)

  It’s similar to the built-in *note zip(): 405. function, but doesn’t
construct an in-memory list and exhaust all the input iterators before
returning; instead tuples are constructed and returned only if they’re
requested.  (The technical term for this behaviour is lazy
evaluation(1).)

  This iterator is intended to be used with iterables that are all of
the same length.  If the iterables are of different lengths, the
resulting stream will be the same length as the shortest iterable.

     itertools.izip(['a', 'b'], (1, 2, 3)) =>
       ('a', 1), ('b', 2)

  You should avoid doing this, though, because an element may be taken
from the longer iterators and discarded.  This means you can’t go on to
use the iterators further because you risk skipping a discarded element.

  ‘itertools.islice(iter, [start], stop, [step])’ returns a stream
that’s a slice of the iterator.  With a single ‘stop’ argument, it will
return the first ‘stop’ elements.  If you supply a starting index,
you’ll get ‘stop-start’ elements, and if you supply a value for ‘step’,
elements will be skipped accordingly.  Unlike Python’s string and list
slicing, you can’t use negative values for ‘start’, ‘stop’, or ‘step’.

     itertools.islice(range(10), 8) =>
       0, 1, 2, 3, 4, 5, 6, 7
     itertools.islice(range(10), 2, 8) =>
       2, 3, 4, 5, 6, 7
     itertools.islice(range(10), 2, 8, 2) =>
       2, 4, 6

  ‘itertools.tee(iter, [n])’ replicates an iterator; it returns ‘n’
independent iterators that will all return the contents of the source
iterator.  If you don’t supply a value for ‘n’, the default is 2.
Replicating iterators requires saving some of the contents of the source
iterator, so this can consume significant memory if the iterator is
large and one of the new iterators is consumed more than the others.

     itertools.tee( itertools.count() ) =>
        iterA, iterB

     where iterA ->
        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

     and   iterB ->
        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

   ---------- Footnotes ----------

   (1) http://en.wikipedia.org/wiki/Lazy_evaluation


File: python.info,  Node: Calling functions on elements,  Next: Selecting elements,  Prev: Creating new iterators,  Up: The itertools module

10.6.7.2 Calling functions on elements
......................................

Two functions are used for calling other functions on the contents of an
iterable.

  ‘itertools.imap(f, iterA, iterB, ...)’ returns a stream containing
‘f(iterA[0], iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]),
...’:

     itertools.imap(operator.add, [5, 6, 5], [1, 2, 3]) =>
       6, 8, 8

  The ‘operator’ module contains a set of functions corresponding to
Python’s operators.  Some examples are ‘operator.add(a, b)’ (adds two
values), ‘operator.ne(a, b)’ (same as ‘a!=b’), and
‘operator.attrgetter('id')’ (returns a callable that fetches the ‘"id"’
attribute).

  ‘itertools.starmap(func, iter)’ assumes that the iterable will return
a stream of tuples, and calls ‘f()’ using these tuples as the arguments:

     itertools.starmap(os.path.join,
                       [('/usr', 'bin', 'java'), ('/bin', 'python'),
                        ('/usr', 'bin', 'perl'),('/usr', 'bin', 'ruby')])
     =>
       /usr/bin/java, /bin/python, /usr/bin/perl, /usr/bin/ruby


File: python.info,  Node: Selecting elements,  Next: Grouping elements,  Prev: Calling functions on elements,  Up: The itertools module

10.6.7.3 Selecting elements
...........................

Another group of functions chooses a subset of an iterator’s elements
based on a predicate.

  ‘itertools.ifilter(predicate, iter)’ returns all the elements for
which the predicate returns true:

     def is_even(x):
         return (x % 2) == 0

     itertools.ifilter(is_even, itertools.count()) =>
       0, 2, 4, 6, 8, 10, 12, 14, ...

  ‘itertools.ifilterfalse(predicate, iter)’ is the opposite, returning
all elements for which the predicate returns false:

     itertools.ifilterfalse(is_even, itertools.count()) =>
       1, 3, 5, 7, 9, 11, 13, 15, ...

  ‘itertools.takewhile(predicate, iter)’ returns elements for as long as
the predicate returns true.  Once the predicate returns false, the
iterator will signal the end of its results.

     def less_than_10(x):
         return (x < 10)

     itertools.takewhile(less_than_10, itertools.count()) =>
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9

     itertools.takewhile(is_even, itertools.count()) =>
       0

  ‘itertools.dropwhile(predicate, iter)’ discards elements while the
predicate returns true, and then returns the rest of the iterable’s
results.

     itertools.dropwhile(less_than_10, itertools.count()) =>
       10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

     itertools.dropwhile(is_even, itertools.count()) =>
       1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...


File: python.info,  Node: Grouping elements,  Prev: Selecting elements,  Up: The itertools module

10.6.7.4 Grouping elements
..........................

The last function I’ll discuss, ‘itertools.groupby(iter,
key_func=None)’, is the most complicated.  ‘key_func(elem)’ is a
function that can compute a key value for each element returned by the
iterable.  If you don’t supply a key function, the key is simply each
element itself.

  ‘groupby()’ collects all the consecutive elements from the underlying
iterable that have the same key value, and returns a stream of 2-tuples
containing a key value and an iterator for the elements with that key.

     city_list = [('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL'),
                  ('Anchorage', 'AK'), ('Nome', 'AK'),
                  ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ'),
                  ...
                 ]

     def get_state ((city, state)):
         return state

     itertools.groupby(city_list, get_state) =>
       ('AL', iterator-1),
       ('AK', iterator-2),
       ('AZ', iterator-3), ...

     where
     iterator-1 =>
       ('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL')
     iterator-2 =>
       ('Anchorage', 'AK'), ('Nome', 'AK')
     iterator-3 =>
       ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ')

  ‘groupby()’ assumes that the underlying iterable’s contents will
already be sorted based on the key.  Note that the returned iterators
also use the underlying iterable, so you have to consume the results of
iterator-1 before requesting iterator-2 and its corresponding key.


File: python.info,  Node: The functools module,  Next: Revision History and Acknowledgements,  Prev: The itertools module,  Up: Functional Programming HOWTO

10.6.8 The functools module
---------------------------

The *note functools: d9. module in Python 2.5 contains some higher-order
functions.  A *higher-order function* takes one or more functions as
input and returns a new function.  The most useful tool in this module
is the *note functools.partial(): d77. function.

  For programs written in a functional style, you’ll sometimes want to
construct variants of existing functions that have some of the
parameters filled in.  Consider a Python function ‘f(a, b, c)’; you may
wish to create a new function ‘g(b, c)’ that’s equivalent to ‘f(1, b,
c)’; you’re filling in a value for one of ‘f()’’s parameters.  This is
called "partial function application".

  The constructor for ‘partial’ takes the arguments ‘(function, arg1,
arg2, ... kwarg1=value1, kwarg2=value2)’.  The resulting object is
callable, so you can just call it to invoke ‘function’ with the
filled-in arguments.

  Here’s a small but realistic example:

     import functools

     def log (message, subsystem):
         "Write the contents of 'message' to the specified subsystem."
         print '%s: %s' % (subsystem, message)
         ...

     server_log = functools.partial(log, subsystem='server')
     server_log('Unable to open socket')

* Menu:

* The operator module:: 


File: python.info,  Node: The operator module,  Up: The functools module

10.6.8.1 The operator module
............................

The *note operator: 126. module was mentioned earlier.  It contains a
set of functions corresponding to Python’s operators.  These functions
are often useful in functional-style code because they save you from
writing trivial functions that perform a single operation.

  Some of the functions in this module are:

   * Math operations: ‘add()’, ‘sub()’, ‘mul()’, ‘div()’, ‘floordiv()’,
     ‘abs()’, ...

   * Logical operations: ‘not_()’, ‘truth()’.

   * Bitwise operations: ‘and_()’, ‘or_()’, ‘invert()’.

   * Comparisons: ‘eq()’, ‘ne()’, ‘lt()’, ‘le()’, ‘gt()’, and ‘ge()’.

   * Object identity: ‘is_()’, ‘is_not()’.

  Consult the operator module’s documentation for a complete list.


File: python.info,  Node: Revision History and Acknowledgements,  Next: References,  Prev: The functools module,  Up: Functional Programming HOWTO

10.6.9 Revision History and Acknowledgements
--------------------------------------------

The author would like to thank the following people for offering
suggestions, corrections and assistance with various drafts of this
article: Ian Bicking, Nick Coghlan, Nick Efford, Raymond Hettinger, Jim
Jewett, Mike Krell, Leandro Lameiro, Jussi Salmela, Collin Winter, Blake
Winton.

  Version 0.1: posted June 30 2006.

  Version 0.11: posted July 1 2006.  Typo fixes.

  Version 0.2: posted July 10 2006.  Merged genexp and listcomp sections
into one.  Typo fixes.

  Version 0.21: Added more references suggested on the tutor mailing
list.

  Version 0.30: Adds a section on the ‘functional’ module written by
Collin Winter; adds short section on the operator module; a few other
edits.


File: python.info,  Node: References,  Prev: Revision History and Acknowledgements,  Up: Functional Programming HOWTO

10.6.10 References
------------------

* Menu:

* General:: 
* Python-specific:: 
* Python documentation:: 


File: python.info,  Node: General,  Next: Python-specific,  Up: References

10.6.10.1 General
.................

*Structure and Interpretation of Computer Programs*, by Harold Abelson
and Gerald Jay Sussman with Julie Sussman.  Full text at
‘http://mitpress.mit.edu/sicp/’.  In this classic textbook of computer
science, chapters 2 and 3 discuss the use of sequences and streams to
organize the data flow inside a program.  The book uses Scheme for its
examples, but many of the design approaches described in these chapters
are applicable to functional-style Python code.

  ‘http://www.defmacro.org/ramblings/fp.html’: A general introduction to
functional programming that uses Java examples and has a lengthy
historical introduction.

  ‘http://en.wikipedia.org/wiki/Functional_programming’: General
Wikipedia entry describing functional programming.

  ‘http://en.wikipedia.org/wiki/Coroutine’: Entry for coroutines.

  ‘http://en.wikipedia.org/wiki/Currying’: Entry for the concept of
currying.


File: python.info,  Node: Python-specific,  Next: Python documentation,  Prev: General,  Up: References

10.6.10.2 Python-specific
.........................

‘http://gnosis.cx/TPiP/’: The first chapter of David Mertz’s book ‘Text
Processing in Python’ discusses functional programming for text
processing, in the section titled "Utilizing Higher-Order Functions in
Text Processing".

  Mertz also wrote a 3-part series of articles on functional programming
for IBM’s DeveloperWorks site; see

  part 1(1), part 2(2), and part 3(3),

   ---------- Footnotes ----------

   (1) http://www.ibm.com/developerworks/linux/library/l-prog/index.html

   (2) 
http://www.ibm.com/developerworks/linux/library/l-prog2/index.html

   (3) 
http://www.ibm.com/developerworks/linux/library/l-prog3/index.html


File: python.info,  Node: Python documentation,  Prev: Python-specific,  Up: References

10.6.10.3 Python documentation
..............................

Documentation for the *note itertools: fa. module.

  Documentation for the *note operator: 126. module.

  PEP 289(1): "Generator Expressions"

  PEP 342(2): "Coroutines via Enhanced Generators" describes the new
generator features in Python 2.5.

   ---------- Footnotes ----------

   (1) http://www.python.org/dev/peps/pep-0289

   (2) http://www.python.org/dev/peps/pep-0342


File: python.info,  Node: Logging HOWTO,  Next: Logging Cookbook,  Prev: Functional Programming HOWTO,  Up: Python HOWTOs

10.7 Logging HOWTO
==================

     Author: Vinay Sajip <vinay_sajip at red-dove dot com>

* Menu:

* Basic Logging Tutorial:: 
* Advanced Logging Tutorial:: 
* Logging Levels: Logging Levels<2>. 
* Useful Handlers:: 
* Exceptions raised during logging:: 
* Using arbitrary objects as messages:: 
* Optimization:: 

Basic Logging Tutorial

* When to use logging:: 
* A simple example:: 
* Logging to a file:: 
* Logging from multiple modules:: 
* Logging variable data:: 
* Changing the format of displayed messages:: 
* Displaying the date/time in messages:: 
* Next Steps:: 

Advanced Logging Tutorial

* Logging Flow:: 
* Loggers:: 
* Handlers:: 
* Formatters:: 
* Configuring Logging:: 
* What happens if no configuration is provided:: 
* Configuring Logging for a Library:: 

Logging Levels

* Custom Levels:: 


File: python.info,  Node: Basic Logging Tutorial,  Next: Advanced Logging Tutorial,  Up: Logging HOWTO

10.7.1 Basic Logging Tutorial
-----------------------------

Logging is a means of tracking events that happen when some software
runs.  The software’s developer adds logging calls to their code to
indicate that certain events have occurred.  An event is described by a
descriptive message which can optionally contain variable data (i.e.
data that is potentially different for each occurrence of the event).
Events also have an importance which the developer ascribes to the
event; the importance can also be called the _level_ or _severity_.

* Menu:

* When to use logging:: 
* A simple example:: 
* Logging to a file:: 
* Logging from multiple modules:: 
* Logging variable data:: 
* Changing the format of displayed messages:: 
* Displaying the date/time in messages:: 
* Next Steps:: 


File: python.info,  Node: When to use logging,  Next: A simple example,  Up: Basic Logging Tutorial

10.7.1.1 When to use logging
............................

Logging provides a set of convenience functions for simple logging
usage.  These are *note debug(): 12c9, *note info(): 12f6, *note
warning(): 1306, *note error(): 1307. and *note critical(): 1308.  To
determine when to use logging, see the table below, which states, for
each of a set of common tasks, the best tool to use for it.

Task you want to perform                  The best tool for the task
                                          
-------------------------------------------------------------------------------------
                                          
Display console output for ordinary       *note print(): 30b.
usage of a command line script or         
program

Report events that occur during normal    *note logging.info(): 12f6. (or *note
operation of a program (e.g.  for         logging.debug(): 12c9. for very detailed
status monitoring or fault                output for diagnostic purposes)
investigation)                            

Issue a warning regarding a particular    *note warnings.warn(): 4c0. in library
runtime event                             code if the issue is avoidable and the
                                          client application should be modified to
                                          eliminate the warning
                                          
                                          *note logging.warning(): 1306. if there
                                          is nothing the client application can do
                                          about the situation, but the event
                                          should still be noted
                                          
                                          
Report an error regarding a particular    Raise an exception
runtime event                             

Report suppression of an error without    *note logging.error(): 1307, *note
raising an exception (e.g.  error         logging.exception(): 1309. or *note
handler in a long-running server          logging.critical(): 1308. as appropriate
process)                                  for the specific error and application
                                          domain
                                          

  The logging functions are named after the level or severity of the
events they are used to track.  The standard levels and their
applicability are described below (in increasing order of severity):

Level              When it’s used
                   
---------------------------------------------------------------------
                   
‘DEBUG’            Detailed information, typically of interest
                   only when diagnosing problems.
                   
                   
‘INFO’             Confirmation that things are working as
                   expected.
                   
                   
‘WARNING’          An indication that something unexpected
                   happened, or indicative of some problem in the
                   near future (e.g.  ’disk space low’).  The
                   software is still working as expected.
                   
                   
‘ERROR’            Due to a more serious problem, the software has
                   not been able to perform some function.
                   
                   
‘CRITICAL’         A serious error, indicating that the program
                   itself may be unable to continue running.
                   

  The default level is ‘WARNING’, which means that only events of this
level and above will be tracked, unless the logging package is
configured to do otherwise.

  Events that are tracked can be handled in different ways.  The
simplest way of handling tracked events is to print them to the console.
Another common way is to write them to a disk file.


File: python.info,  Node: A simple example,  Next: Logging to a file,  Prev: When to use logging,  Up: Basic Logging Tutorial

10.7.1.2 A simple example
.........................

A very simple example is:

     import logging
     logging.warning('Watch out!') # will print a message to the console
     logging.info('I told you so') # will not print anything

  If you type these lines into a script and run it, you’ll see:

     WARNING:root:Watch out!

  printed out on the console.  The ‘INFO’ message doesn’t appear because
the default level is ‘WARNING’.  The printed message includes the
indication of the level and the description of the event provided in the
logging call, i.e.  ’Watch out!’.  Don’t worry about the ’root’ part for
now: it will be explained later.  The actual output can be formatted
quite flexibly if you need that; formatting options will also be
explained later.


File: python.info,  Node: Logging to a file,  Next: Logging from multiple modules,  Prev: A simple example,  Up: Basic Logging Tutorial

10.7.1.3 Logging to a file
..........................

A very common situation is that of recording logging events in a file,
so let’s look at that next.  Be sure to try the following in a
newly-started Python interpreter, and don’t just continue from the
session described above:

     import logging
     logging.basicConfig(filename='example.log',level=logging.DEBUG)
     logging.debug('This message should go to the log file')
     logging.info('So should this')
     logging.warning('And this, too')

  And now if we open the file and look at what we have, we should find
the log messages:

     DEBUG:root:This message should go to the log file
     INFO:root:So should this
     WARNING:root:And this, too

  This example also shows how you can set the logging level which acts
as the threshold for tracking.  In this case, because we set the
threshold to ‘DEBUG’, all of the messages were printed.

  If you want to set the logging level from a command-line option such
as:

     --log=INFO

  and you have the value of the parameter passed for ‘--log’ in some
variable _loglevel_, you can use:

     getattr(logging, loglevel.upper())

  to get the value which you’ll pass to *note basicConfig(): 130b. via
the _level_ argument.  You may want to error check any user input value,
perhaps as in the following example:

     # assuming loglevel is bound to the string value obtained from the
     # command line argument. Convert to upper case to allow the user to
     # specify --log=DEBUG or --log=debug
     numeric_level = getattr(logging, loglevel.upper(), None)
     if not isinstance(numeric_level, int):
         raise ValueError('Invalid log level: %s' % loglevel)
     logging.basicConfig(level=numeric_level, ...)

  The call to *note basicConfig(): 130b. should come _before_ any calls
to *note debug(): 12c9, *note info(): 12f6. etc.  As it’s intended as a
one-off simple configuration facility, only the first call will actually
do anything: subsequent calls are effectively no-ops.

  If you run the above script several times, the messages from
successive runs are appended to the file _example.log_.  If you want
each run to start afresh, not remembering the messages from earlier
runs, you can specify the _filemode_ argument, by changing the call in
the above example to:

     logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)

  The output will be the same as before, but the log file is no longer
appended to, so the messages from earlier runs are lost.


File: python.info,  Node: Logging from multiple modules,  Next: Logging variable data,  Prev: Logging to a file,  Up: Basic Logging Tutorial

10.7.1.4 Logging from multiple modules
......................................

If your program consists of multiple modules, here’s an example of how
you could organize logging in it:

     # myapp.py
     import logging
     import mylib

     def main():
         logging.basicConfig(filename='myapp.log', level=logging.INFO)
         logging.info('Started')
         mylib.do_something()
         logging.info('Finished')

     if __name__ == '__main__':
         main()

     # mylib.py
     import logging

     def do_something():
         logging.info('Doing something')

  If you run _myapp.py_, you should see this in _myapp.log_:

     INFO:root:Started
     INFO:root:Doing something
     INFO:root:Finished

  which is hopefully what you were expecting to see.  You can generalize
this to multiple modules, using the pattern in _mylib.py_.  Note that
for this simple usage pattern, you won’t know, by looking in the log
file, _where_ in your application your messages came from, apart from
looking at the event description.  If you want to track the location of
your messages, you’ll need to refer to the documentation beyond the
tutorial level – see *note Advanced Logging Tutorial: 12bc.


File: python.info,  Node: Logging variable data,  Next: Changing the format of displayed messages,  Prev: Logging from multiple modules,  Up: Basic Logging Tutorial

10.7.1.5 Logging variable data
..............................

To log variable data, use a format string for the event description
message and append the variable data as arguments.  For example:

     import logging
     logging.warning('%s before you %s', 'Look', 'leap!')

  will display:

     WARNING:root:Look before you leap!

  As you can see, merging of variable data into the event description
message uses the old, %-style of string formatting.  This is for
backwards compatibility: the logging package pre-dates newer formatting
options such as *note str.format(): 1d2. and *note string.Template: 593.
These newer formatting options _are_ supported, but exploring them is
outside the scope of this tutorial.


File: python.info,  Node: Changing the format of displayed messages,  Next: Displaying the date/time in messages,  Prev: Logging variable data,  Up: Basic Logging Tutorial

10.7.1.6 Changing the format of displayed messages
..................................................

To change the format which is used to display messages, you need to
specify the format you want to use:

     import logging
     logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
     logging.debug('This message should appear on the console')
     logging.info('So should this')
     logging.warning('And this, too')

  which would print:

     DEBUG:This message should appear on the console
     INFO:So should this
     WARNING:And this, too

  Notice that the ’root’ which appeared in earlier examples has
disappeared.  For a full set of things that can appear in format
strings, you can refer to the documentation for *note LogRecord
attributes: 12ed, but for simple usage, you just need the _levelname_
(severity), _message_ (event description, including variable data) and
perhaps to display when the event occurred.  This is described in the
next section.


File: python.info,  Node: Displaying the date/time in messages,  Next: Next Steps,  Prev: Changing the format of displayed messages,  Up: Basic Logging Tutorial

10.7.1.7 Displaying the date/time in messages
.............................................

To display the date and time of an event, you would place ’%(asctime)s’
in your format string:

     import logging
     logging.basicConfig(format='%(asctime)s %(message)s')
     logging.warning('is when this event was logged.')

  which should print something like this:

     2010-12-12 11:41:42,612 is when this event was logged.

  The default format for date/time display (shown above) is ISO8601.  If
you need more control over the formatting of the date/time, provide a
_datefmt_ argument to ‘basicConfig’, as in this example:

     import logging
     logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
     logging.warning('is when this event was logged.')

  which would display something like this:

     12/12/2010 11:46:36 AM is when this event was logged.

  The format of the _datefmt_ argument is the same as supported by *note
time.strftime(): 3be.


File: python.info,  Node: Next Steps,  Prev: Displaying the date/time in messages,  Up: Basic Logging Tutorial

10.7.1.8 Next Steps
...................

That concludes the basic tutorial.  It should be enough to get you up
and running with logging.  There’s a lot more that the logging package
offers, but to get the best out of it, you’ll need to invest a little
more of your time in reading the following sections.  If you’re ready
for that, grab some of your favourite beverage and carry on.

  If your logging needs are simple, then use the above examples to
incorporate logging into your own scripts, and if you run into problems
or don’t understand something, please post a question on the
comp.lang.python Usenet group (available at
‘http://groups.google.com/group/comp.lang.python’) and you should
receive help before too long.

  Still here?  You can carry on reading the next few sections, which
provide a slightly more advanced/in-depth tutorial than the basic one
above.  After that, you can take a look at the *note Logging Cookbook:
12bd.


File: python.info,  Node: Advanced Logging Tutorial,  Next: Logging Levels<2>,  Prev: Basic Logging Tutorial,  Up: Logging HOWTO

10.7.2 Advanced Logging Tutorial
--------------------------------

The logging library takes a modular approach and offers several
categories of components: loggers, handlers, filters, and formatters.

   * Loggers expose the interface that application code directly uses.

   * Handlers send the log records (created by loggers) to the
     appropriate destination.

   * Filters provide a finer grained facility for determining which log
     records to output.

   * Formatters specify the layout of log records in the final output.

  Log event information is passed between loggers, handlers, filters and
formatters in a *note LogRecord: 12d7. instance.

  Logging is performed by calling methods on instances of the *note
Logger: 1dd. class (hereafter called _loggers_).  Each instance has a
name, and they are conceptually arranged in a namespace hierarchy using
dots (periods) as separators.  For example, a logger named ’scan’ is the
parent of loggers ’scan.text’, ’scan.html’ and ’scan.pdf’.  Logger names
can be anything you want, and indicate the area of an application in
which a logged message originates.

  A good convention to use when naming loggers is to use a module-level
logger, in each module which uses logging, named as follows:

     logger = logging.getLogger(__name__)

  This means that logger names track the package/module hierarchy, and
it’s intuitively obvious where events are logged just from the logger
name.

  The root of the hierarchy of loggers is called the root logger.
That’s the logger used by the functions *note debug(): 12c9, *note
info(): 12f6, *note warning(): 1306, *note error(): 1307. and *note
critical(): 1308, which just call the same-named method of the root
logger.  The functions and the methods have the same signatures.  The
root logger’s name is printed as ’root’ in the logged output.

  It is, of course, possible to log messages to different destinations.
Support is included in the package for writing log messages to files,
HTTP GET/POST locations, email via SMTP, generic sockets, or OS-specific
logging mechanisms such as syslog or the Windows NT event log.
Destinations are served by _handler_ classes.  You can create your own
log destination class if you have special requirements not met by any of
the built-in handler classes.

  By default, no destination is set for any logging messages.  You can
specify a destination (such as console or file) by using *note
basicConfig(): 130b. as in the tutorial examples.  If you call the
functions *note debug(): 12c9, *note info(): 12f6, *note warning():
1306, *note error(): 1307. and *note critical(): 1308, they will check
to see if no destination is set; and if one is not set, they will set a
destination of the console (‘sys.stderr’) and a default format for the
displayed message before delegating to the root logger to do the actual
message output.

  The default format set by *note basicConfig(): 130b. for messages is:

     severity:logger name:message

  You can change this by passing a format string to *note basicConfig():
130b. with the _format_ keyword argument.  For all options regarding how
a format string is constructed, see *note Formatter Objects: 12eb.

* Menu:

* Logging Flow:: 
* Loggers:: 
* Handlers:: 
* Formatters:: 
* Configuring Logging:: 
* What happens if no configuration is provided:: 
* Configuring Logging for a Library:: 


File: python.info,  Node: Logging Flow,  Next: Loggers,  Up: Advanced Logging Tutorial

10.7.2.1 Logging Flow
.....................

The flow of log event information in loggers and handlers is illustrated
in the following diagram.

[logging_flow]

File: python.info,  Node: Loggers,  Next: Handlers,  Prev: Logging Flow,  Up: Advanced Logging Tutorial

10.7.2.2 Loggers
................

*note Logger: 1dd. objects have a threefold job.  First, they expose
several methods to application code so that applications can log
messages at runtime.  Second, logger objects determine which log
messages to act upon based upon severity (the default filtering
facility) or filter objects.  Third, logger objects pass along relevant
log messages to all interested log handlers.

  The most widely used methods on logger objects fall into two
categories: configuration and message sending.

  These are the most common configuration methods:

   * *note Logger.setLevel(): 12c2. specifies the lowest-severity log
     message a logger will handle, where debug is the lowest built-in
     severity level and critical is the highest built-in severity.  For
     example, if the severity level is INFO, the logger will handle only
     INFO, WARNING, ERROR, and CRITICAL messages and will ignore DEBUG
     messages.

   * *note Logger.addHandler(): 12d2. and *note Logger.removeHandler():
     12d3. add and remove handler objects from the logger object.
     Handlers are covered in more detail in *note Handlers: 301c.

   * *note Logger.addFilter(): 12cf. and *note Logger.removeFilter():
     12d0. add and remove filter objects from the logger object.
     Filters are covered in more detail in *note Filter Objects: 12f2.

  You don’t need to always call these methods on every logger you
create.  See the last two paragraphs in this section.

  With the logger object configured, the following methods create log
messages:

   * *note Logger.debug(): 12c6, *note Logger.info(): 12c8, *note
     Logger.warning(): 12ca, *note Logger.error(): 12cb, and *note
     Logger.critical(): 12cc. all create log records with a message and
     a level that corresponds to their respective method names.  The
     message is actually a format string, which may contain the standard
     string substitution syntax of ‘%s’, ‘%d’, ‘%f’, and so on.  The
     rest of their arguments is a list of objects that correspond with
     the substitution fields in the message.  With regard to ‘**kwargs’,
     the logging methods care only about a keyword of ‘exc_info’ and use
     it to determine whether to log exception information.

   * *note Logger.exception(): 12ce. creates a log message similar to
     *note Logger.error(): 12cb.  The difference is that *note
     Logger.exception(): 12ce. dumps a stack trace along with it.  Call
     this method only from an exception handler.

   * *note Logger.log(): 12cd. takes a log level as an explicit
     argument.  This is a little more verbose for logging messages than
     using the log level convenience methods listed above, but this is
     how to log at custom log levels.

  *note getLogger(): 12c0. returns a reference to a logger instance with
the specified name if it is provided, or ‘root’ if not.  The names are
period-separated hierarchical structures.  Multiple calls to *note
getLogger(): 12c0. with the same name will return a reference to the
same logger object.  Loggers that are further down in the hierarchical
list are children of loggers higher up in the list.  For example, given
a logger with a name of ‘foo’, loggers with names of ‘foo.bar’,
‘foo.bar.baz’, and ‘foo.bam’ are all descendants of ‘foo’.

  Loggers have a concept of _effective level_.  If a level is not
explicitly set on a logger, the level of its parent is used instead as
its effective level.  If the parent has no explicit level set, _its_
parent is examined, and so on - all ancestors are searched until an
explicitly set level is found.  The root logger always has an explicit
level set (‘WARNING’ by default).  When deciding whether to process an
event, the effective level of the logger is used to determine whether
the event is passed to the logger’s handlers.

  Child loggers propagate messages up to the handlers associated with
their ancestor loggers.  Because of this, it is unnecessary to define
and configure handlers for all the loggers an application uses.  It is
sufficient to configure handlers for a top-level logger and create child
loggers as needed.  (You can, however, turn off propagation by setting
the _propagate_ attribute of a logger to _False_.)


File: python.info,  Node: Handlers,  Next: Formatters,  Prev: Loggers,  Up: Advanced Logging Tutorial

10.7.2.3 Handlers
.................

‘Handler’ objects are responsible for dispatching the appropriate log
messages (based on the log messages’ severity) to the handler’s
specified destination.  *note Logger: 1dd. objects can add zero or more
handler objects to themselves with an *note addHandler(): 12d2. method.
As an example scenario, an application may want to send all log messages
to a log file, all log messages of error or higher to stdout, and all
messages of critical to an email address.  This scenario requires three
individual handlers where each handler is responsible for sending
messages of a specific severity to a specific location.

  The standard library includes quite a few handler types (see *note
Useful Handlers: 301e.); the tutorials use mainly *note StreamHandler:
1310. and *note FileHandler: 1333. in its examples.

  There are very few methods in a handler for application developers to
concern themselves with.  The only handler methods that seem relevant
for application developers who are using the built-in handler objects
(that is, not creating custom handlers) are the following configuration
methods:

   * The *note setLevel(): 12df. method, just as in logger objects,
     specifies the lowest severity that will be dispatched to the
     appropriate destination.  Why are there two ‘setLevel()’ methods?
     The level set in the logger determines which severity of messages
     it will pass to its handlers.  The level set in each handler
     determines which messages that handler will send on.

   * *note setFormatter(): 12e0. selects a Formatter object for this
     handler to use.

   * *note addFilter(): 12e1. and *note removeFilter(): 12e2.
     respectively configure and deconfigure filter objects on handlers.

  Application code should not directly instantiate and use instances of
‘Handler’.  Instead, the ‘Handler’ class is a base class that defines
the interface that all handlers should have and establishes some default
behavior that child classes can use (or override).


File: python.info,  Node: Formatters,  Next: Configuring Logging,  Prev: Handlers,  Up: Advanced Logging Tutorial

10.7.2.4 Formatters
...................

Formatter objects configure the final order, structure, and contents of
the log message.  Unlike the base ‘logging.Handler’ class, application
code may instantiate formatter classes, although you could likely
subclass the formatter if your application needs special behavior.  The
constructor takes two optional arguments – a message format string and a
date format string.

 -- Method: logging.Formatter.__init__ (fmt=None, datefmt=None)

  If there is no message format string, the default is to use the raw
message.  If there is no date format string, the default date format is:

     %Y-%m-%d %H:%M:%S

  with the milliseconds tacked on at the end.

  The message format string uses ‘%(<dictionary key>)s’ styled string
substitution; the possible keys are documented in *note LogRecord
attributes: 12ed.

  The following message format string will log the time in a
human-readable format, the severity of the message, and the contents of
the message, in that order:

     '%(asctime)s - %(levelname)s - %(message)s'

  Formatters use a user-configurable function to convert the creation
time of a record to a tuple.  By default, *note time.localtime(): ae1.
is used; to change this for a particular formatter instance, set the
‘converter’ attribute of the instance to a function with the same
signature as *note time.localtime(): ae1. or *note time.gmtime(): b54.
To change it for all formatters, for example if you want all logging
times to be shown in GMT, set the ‘converter’ attribute in the Formatter
class (to ‘time.gmtime’ for GMT display).


File: python.info,  Node: Configuring Logging,  Next: What happens if no configuration is provided,  Prev: Formatters,  Up: Advanced Logging Tutorial

10.7.2.5 Configuring Logging
............................

Programmers can configure logging in three ways:

  1. Creating loggers, handlers, and formatters explicitly using Python
     code that calls the configuration methods listed above.

  2. Creating a logging config file and reading it using the *note
     fileConfig(): 1319. function.

  3. Creating a dictionary of configuration information and passing it
     to the *note dictConfig(): 1317. function.

  For the reference documentation on the last two options, see *note
Configuration functions: 1d9.  The following example configures a very
simple logger, a console handler, and a simple formatter using Python
code:

     import logging

     # create logger
     logger = logging.getLogger('simple_example')
     logger.setLevel(logging.DEBUG)

     # create console handler and set level to debug
     ch = logging.StreamHandler()
     ch.setLevel(logging.DEBUG)

     # create formatter
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

     # add formatter to ch
     ch.setFormatter(formatter)

     # add ch to logger
     logger.addHandler(ch)

     # 'application' code
     logger.debug('debug message')
     logger.info('info message')
     logger.warn('warn message')
     logger.error('error message')
     logger.critical('critical message')

  Running this module from the command line produces the following
output:

     $ python simple_logging_module.py
     2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message
     2005-03-19 15:10:26,620 - simple_example - INFO - info message
     2005-03-19 15:10:26,695 - simple_example - WARNING - warn message
     2005-03-19 15:10:26,697 - simple_example - ERROR - error message
     2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message

  The following Python module creates a logger, handler, and formatter
nearly identical to those in the example listed above, with the only
difference being the names of the objects:

     import logging
     import logging.config

     logging.config.fileConfig('logging.conf')

     # create logger
     logger = logging.getLogger('simpleExample')

     # 'application' code
     logger.debug('debug message')
     logger.info('info message')
     logger.warn('warn message')
     logger.error('error message')
     logger.critical('critical message')

  Here is the logging.conf file:

     [loggers]
     keys=root,simpleExample

     [handlers]
     keys=consoleHandler

     [formatters]
     keys=simpleFormatter

     [logger_root]
     level=DEBUG
     handlers=consoleHandler

     [logger_simpleExample]
     level=DEBUG
     handlers=consoleHandler
     qualname=simpleExample
     propagate=0

     [handler_consoleHandler]
     class=StreamHandler
     level=DEBUG
     formatter=simpleFormatter
     args=(sys.stdout,)

     [formatter_simpleFormatter]
     format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
     datefmt=

  The output is nearly identical to that of the non-config-file-based
example:

     $ python simple_logging_config.py
     2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message
     2005-03-19 15:38:55,979 - simpleExample - INFO - info message
     2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message
     2005-03-19 15:38:56,055 - simpleExample - ERROR - error message
     2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message

  You can see that the config file approach has a few advantages over
the Python code approach, mainly separation of configuration and code
and the ability of noncoders to easily modify the logging properties.

     Warning: The *note fileConfig(): 1319. function takes a default
     parameter, ‘disable_existing_loggers’, which defaults to ‘True’ for
     reasons of backward compatibility.  This may or may not be what you
     want, since it will cause any loggers existing before the *note
     fileConfig(): 1319. call to be disabled unless they (or an
     ancestor) are explicitly named in the configuration.  Please refer
     to the reference documentation for more information, and specify
     ‘False’ for this parameter if you wish.

     The dictionary passed to *note dictConfig(): 1317. can also specify
     a Boolean value with key ‘disable_existing_loggers’, which if not
     specified explicitly in the dictionary also defaults to being
     interpreted as ‘True’.  This leads to the logger-disabling
     behaviour described above, which may not be what you want - in
     which case, provide the key explicitly with a value of ‘False’.

  Note that the class names referenced in config files need to be either
relative to the logging module, or absolute values which can be resolved
using normal import mechanisms.  Thus, you could use either *note
WatchedFileHandler: 1345. (relative to the logging module) or
‘mypackage.mymodule.MyHandler’ (for a class defined in package
‘mypackage’ and module ‘mymodule’, where ‘mypackage’ is available on the
Python import path).

  In Python 2.7, a new means of configuring logging has been introduced,
using dictionaries to hold configuration information.  This provides a
superset of the functionality of the config-file-based approach outlined
above, and is the recommended configuration method for new applications
and deployments.  Because a Python dictionary is used to hold
configuration information, and since you can populate that dictionary
using different means, you have more options for configuration.  For
example, you can use a configuration file in JSON format, or, if you
have access to YAML processing functionality, a file in YAML format, to
populate the configuration dictionary.  Or, of course, you can construct
the dictionary in Python code, receive it in pickled form over a socket,
or use whatever approach makes sense for your application.

  Here’s an example of the same configuration as above, in YAML format
for the new dictionary-based approach:

     version: 1
     formatters:
       simple:
         format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
     handlers:
       console:
         class: logging.StreamHandler
         level: DEBUG
         formatter: simple
         stream: ext://sys.stdout
     loggers:
       simpleExample:
         level: DEBUG
         handlers: [console]
         propagate: no
     root:
       level: DEBUG
       handlers: [console]

  For more information about logging using a dictionary, see *note
Configuration functions: 1d9.


File: python.info,  Node: What happens if no configuration is provided,  Next: Configuring Logging for a Library,  Prev: Configuring Logging,  Up: Advanced Logging Tutorial

10.7.2.6 What happens if no configuration is provided
.....................................................

If no logging configuration is provided, it is possible to have a
situation where a logging event needs to be output, but no handlers can
be found to output the event.  The behaviour of the logging package in
these circumstances is dependent on the Python version.

  For Python 2.x, the behaviour is as follows:

   * If _logging.raiseExceptions_ is _False_ (production mode), the
     event is silently dropped.

   * If _logging.raiseExceptions_ is _True_ (development mode), a
     message ’No handlers could be found for logger X.Y.Z’ is printed
     once.


File: python.info,  Node: Configuring Logging for a Library,  Prev: What happens if no configuration is provided,  Up: Advanced Logging Tutorial

10.7.2.7 Configuring Logging for a Library
..........................................

When developing a library which uses logging, you should take care to
document how the library uses logging - for example, the names of
loggers used.  Some consideration also needs to be given to its logging
configuration.  If the using application does not configure logging, and
library code makes logging calls, then (as described in the previous
section) an error message will be printed to ‘sys.stderr’.

  If for some reason you _don’t_ want this message printed in the
absence of any logging configuration, you can attach a do-nothing
handler to the top-level logger for your library.  This avoids the
message being printed, since a handler will be always be found for the
library’s events: it just doesn’t produce any output.  If the library
user configures logging for application use, presumably that
configuration will add some handlers, and if levels are suitably
configured then logging calls made in library code will send output to
those handlers, as normal.

  A do-nothing handler is included in the logging package: *note
NullHandler: 1334. (since Python 2.7).  An instance of this handler
could be added to the top-level logger of the logging namespace used by
the library (_if_ you want to prevent an error message being output to
‘sys.stderr’ in the absence of logging configuration).  If all logging
by a library _foo_ is done using loggers with names matching ’foo.x’,
’foo.x.y’, etc.  then the code:

     import logging
     logging.getLogger('foo').addHandler(logging.NullHandler())

  should have the desired effect.  If an organisation produces a number
of libraries, then the logger name specified can be ’orgname.foo’ rather
than just ’foo’.

     Note: It is strongly advised that you _do not add any handlers
     other than_ *note NullHandler: 1334. _to your library’s loggers_.
     This is because the configuration of handlers is the prerogative of
     the application developer who uses your library.  The application
     developer knows their target audience and what handlers are most
     appropriate for their application: if you add handlers ’under the
     hood’, you might well interfere with their ability to carry out
     unit tests and deliver logs which suit their requirements.


File: python.info,  Node: Logging Levels<2>,  Next: Useful Handlers,  Prev: Advanced Logging Tutorial,  Up: Logging HOWTO

10.7.3 Logging Levels
---------------------

The numeric values of logging levels are given in the following table.
These are primarily of interest if you want to define your own levels,
and need them to have specific values relative to the predefined levels.
If you define a level with the same numeric value, it overwrites the
predefined value; the predefined name is lost.

Level              Numeric value
                   
---------------------------------------
                   
‘CRITICAL’         50
                   
                   
‘ERROR’            40
                   
                   
‘WARNING’          30
                   
                   
‘INFO’             20
                   
                   
‘DEBUG’            10
                   
                   
‘NOTSET’           0
                   

  Levels can also be associated with loggers, being set either by the
developer or through loading a saved logging configuration.  When a
logging method is called on a logger, the logger compares its own level
with the level associated with the method call.  If the logger’s level
is higher than the method call’s, no logging message is actually
generated.  This is the basic mechanism controlling the verbosity of
logging output.

  Logging messages are encoded as instances of the *note LogRecord:
12d7. class.  When a logger decides to actually log an event, a *note
LogRecord: 12d7. instance is created from the logging message.

  Logging messages are subjected to a dispatch mechanism through the use
of _handlers_, which are instances of subclasses of the ‘Handler’ class.
Handlers are responsible for ensuring that a logged message (in the form
of a *note LogRecord: 12d7.) ends up in a particular location (or set of
locations) which is useful for the target audience for that message
(such as end users, support desk staff, system administrators,
developers).  Handlers are passed *note LogRecord: 12d7. instances
intended for particular destinations.  Each logger can have zero, one or
more handlers associated with it (via the *note addHandler(): 12d2.
method of *note Logger: 1dd.).  In addition to any handlers directly
associated with a logger, _all handlers associated with all ancestors of
the logger_ are called to dispatch the message (unless the _propagate_
flag for a logger is set to a false value, at which point the passing to
ancestor handlers stops).

  Just as for loggers, handlers can have levels associated with them.  A
handler’s level acts as a filter in the same way as a logger’s level
does.  If a handler decides to actually dispatch an event, the *note
emit(): 12e9. method is used to send the message to its destination.
Most user-defined subclasses of ‘Handler’ will need to override this
*note emit(): 12e9.

* Menu:

* Custom Levels:: 


File: python.info,  Node: Custom Levels,  Up: Logging Levels<2>

10.7.3.1 Custom Levels
......................

Defining your own levels is possible, but should not be necessary, as
the existing levels have been chosen on the basis of practical
experience.  However, if you are convinced that you need custom levels,
great care should be exercised when doing this, and it is possibly _a
very bad idea to define custom levels if you are developing a library_.
That’s because if multiple library authors all define their own custom
levels, there is a chance that the logging output from such multiple
libraries used together will be difficult for the using developer to
control and/or interpret, because a given numeric value might mean
different things for different libraries.


File: python.info,  Node: Useful Handlers,  Next: Exceptions raised during logging,  Prev: Logging Levels<2>,  Up: Logging HOWTO

10.7.4 Useful Handlers
----------------------

In addition to the base ‘Handler’ class, many useful subclasses are
provided:

  1. *note StreamHandler: 1310. instances send messages to streams
     (file-like objects).

  2. *note FileHandler: 1333. instances send messages to disk files.

  3. ‘BaseRotatingHandler’ is the base class for handlers that rotate
     log files at a certain point.  It is not meant to be instantiated
     directly.  Instead, use *note RotatingFileHandler: 1324. or *note
     TimedRotatingFileHandler: 134d.

  4. *note RotatingFileHandler: 1324. instances send messages to disk
     files, with support for maximum log file sizes and log file
     rotation.

  5. *note TimedRotatingFileHandler: 134d. instances send messages to
     disk files, rotating the log file at certain timed intervals.

  6. *note SocketHandler: 1352. instances send messages to TCP/IP
     sockets.

  7. *note DatagramHandler: 135c. instances send messages to UDP
     sockets.

  8. *note SMTPHandler: 1370. instances send messages to a designated
     email address.

  9. *note SysLogHandler: 1da. instances send messages to a Unix syslog
     daemon, possibly on a remote machine.

  10. *note NTEventLogHandler: 1368. instances send messages to a
     Windows NT/2000/XP event log.

  11. *note MemoryHandler: 132d. instances send messages to a buffer in
     memory, which is flushed whenever specific criteria are met.

  12. *note HTTPHandler: 1380. instances send messages to an HTTP server
     using either ‘GET’ or ‘POST’ semantics.

  13. *note WatchedFileHandler: 1345. instances watch the file they are
     logging to.  If the file changes, it is closed and reopened using
     the file name.  This handler is only useful on Unix-like systems;
     Windows does not support the underlying mechanism used.

  14. *note NullHandler: 1334. instances do nothing with error messages.
     They are used by library developers who want to use logging, but
     want to avoid the ’No handlers could be found for logger XXX’
     message which can be displayed if the library user has not
     configured logging.  See *note Configuring Logging for a Library:
     1342. for more information.

  New in version 2.7: The *note NullHandler: 1334. class.

  The *note NullHandler: 1334, *note StreamHandler: 1310. and *note
FileHandler: 1333. classes are defined in the core logging package.  The
other handlers are defined in a sub- module, *note logging.handlers:
103.  (There is also another sub-module, *note logging.config: 102, for
configuration functionality.)

  Logged messages are formatted for presentation through instances of
the *note Formatter: 12c7. class.  They are initialized with a format
string suitable for use with the % operator and a dictionary.

  For formatting multiple messages in a batch, instances of
‘BufferingFormatter’ can be used.  In addition to the format string
(which is applied to each message in the batch), there is provision for
header and trailer format strings.

  When filtering based on logger level and/or handler level is not
enough, instances of *note Filter: 12f4. can be added to both *note
Logger: 1dd. and ‘Handler’ instances (through their *note addFilter():
12e1. method).  Before deciding to process a message further, both
loggers and handlers consult all their filters for permission.  If any
filter returns a false value, the message is not processed further.

  The basic *note Filter: 12f4. functionality allows filtering by
specific logger name.  If this feature is used, messages sent to the
named logger and its children are allowed through the filter, and all
others dropped.


File: python.info,  Node: Exceptions raised during logging,  Next: Using arbitrary objects as messages,  Prev: Useful Handlers,  Up: Logging HOWTO

10.7.5 Exceptions raised during logging
---------------------------------------

The logging package is designed to swallow exceptions which occur while
logging in production.  This is so that errors which occur while
handling logging events - such as logging misconfiguration, network or
other similar errors - do not cause the application using logging to
terminate prematurely.

  ‘SystemExit’ and ‘KeyboardInterrupt’ exceptions are never swallowed.
Other exceptions which occur during the *note emit(): 12e9. method of a
‘Handler’ subclass are passed to its *note handleError(): 12e8. method.

  The default implementation of *note handleError(): 12e8. in ‘Handler’
checks to see if a module-level variable, ‘raiseExceptions’, is set.  If
set, a traceback is printed to *note sys.stderr: 647.  If not set, the
exception is swallowed.

     Note: The default value of ‘raiseExceptions’ is ‘True’.  This is
     because during development, you typically want to be notified of
     any exceptions that occur.  It’s advised that you set
     ‘raiseExceptions’ to ‘False’ for production usage.


File: python.info,  Node: Using arbitrary objects as messages,  Next: Optimization,  Prev: Exceptions raised during logging,  Up: Logging HOWTO

10.7.6 Using arbitrary objects as messages
------------------------------------------

In the preceding sections and examples, it has been assumed that the
message passed when logging the event is a string.  However, this is not
the only possibility.  You can pass an arbitrary object as a message,
and its *note __str__(): 496. method will be called when the logging
system needs to convert it to a string representation.  In fact, if you
want to, you can avoid computing a string representation altogether -
for example, the *note SocketHandler: 1352. emits an event by pickling
it and sending it over the wire.


File: python.info,  Node: Optimization,  Prev: Using arbitrary objects as messages,  Up: Logging HOWTO

10.7.7 Optimization
-------------------

Formatting of message arguments is deferred until it cannot be avoided.
However, computing the arguments passed to the logging method can also
be expensive, and you may want to avoid doing it if the logger will just
throw away your event.  To decide what to do, you can call the *note
isEnabledFor(): 12c4. method which takes a level argument and returns
true if the event would be created by the Logger for that level of call.
You can write code like this:

     if logger.isEnabledFor(logging.DEBUG):
         logger.debug('Message with %s, %s', expensive_func1(),
                                             expensive_func2())

  so that if the logger’s threshold is set above ‘DEBUG’, the calls to
‘expensive_func1()’ and ‘expensive_func2()’ are never made.

     Note: In some cases, *note isEnabledFor(): 12c4. can itself be more
     expensive than you’d like (e.g.  for deeply nested loggers where an
     explicit level is only set high up in the logger hierarchy).  In
     such cases (or if you want to avoid calling a method in tight
     loops), you can cache the result of a call to *note isEnabledFor():
     12c4. in a local or instance variable, and use that instead of
     calling the method each time.  Such a cached value would only need
     to be recomputed when the logging configuration changes dynamically
     while the application is running (which is not all that common).

  There are other optimizations which can be made for specific
applications which need more precise control over what logging
information is collected.  Here’s a list of things you can do to avoid
processing during logging which you don’t need:

What you don’t want to collect                      How to avoid collecting it
                                                    
-------------------------------------------------------------------------------------------------
                                                    
Information about where calls were made from.       Set ‘logging._srcfile’ to ‘None’.  This
                                                    avoids calling *note sys._getframe(): 4cd,
                                                    which may help to speed up your code in
                                                    environments like PyPy (which can’t speed
                                                    up code that uses *note sys._getframe():
                                                    4cd.).
                                                    
                                                    
Threading information.                              Set ‘logging.logThreads’ to ‘0’.
                                                    
                                                    
Process information.                                Set ‘logging.logProcesses’ to ‘0’.
                                                    

  Also note that the core logging module only includes the basic
handlers.  If you don’t import *note logging.handlers: 103. and *note
logging.config: 102, they won’t take up any memory.

See also
........

Module *note logging: 101.

     API reference for the logging module.

Module *note logging.config: 102.

     Configuration API for the logging module.

Module *note logging.handlers: 103.

     Useful handlers included with the logging module.

  *note A logging cookbook: 12bd.


File: python.info,  Node: Logging Cookbook,  Next: Regular Expression HOWTO,  Prev: Logging HOWTO,  Up: Python HOWTOs

10.8 Logging Cookbook
=====================

     Author: Vinay Sajip <vinay_sajip at red-dove dot com>

  This page contains a number of recipes related to logging, which have
been found useful in the past.

* Menu:

* Using logging in multiple modules:: 
* Multiple handlers and formatters:: 
* Logging to multiple destinations:: 
* Configuration server example:: 
* Sending and receiving logging events across a network:: 
* Adding contextual information to your logging output:: 
* Logging to a single file from multiple processes:: 
* Using file rotation:: 
* An example dictionary-based configuration:: 
* Inserting a BOM into messages sent to a SysLogHandler:: 
* Implementing structured logging:: 
* Customizing handlers with dictConfig(): Customizing handlers with dictConfig. 
* Configuring filters with dictConfig(): Configuring filters with dictConfig. 


File: python.info,  Node: Using logging in multiple modules,  Next: Multiple handlers and formatters,  Up: Logging Cookbook

10.8.1 Using logging in multiple modules
----------------------------------------

Multiple calls to ‘logging.getLogger('someLogger')’ return a reference
to the same logger object.  This is true not only within the same
module, but also across modules as long as it is in the same Python
interpreter process.  It is true for references to the same object;
additionally, application code can define and configure a parent logger
in one module and create (but not configure) a child logger in a
separate module, and all logger calls to the child will pass up to the
parent.  Here is a main module:

     import logging
     import auxiliary_module

     # create logger with 'spam_application'
     logger = logging.getLogger('spam_application')
     logger.setLevel(logging.DEBUG)
     # create file handler which logs even debug messages
     fh = logging.FileHandler('spam.log')
     fh.setLevel(logging.DEBUG)
     # create console handler with a higher log level
     ch = logging.StreamHandler()
     ch.setLevel(logging.ERROR)
     # create formatter and add it to the handlers
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
     fh.setFormatter(formatter)
     ch.setFormatter(formatter)
     # add the handlers to the logger
     logger.addHandler(fh)
     logger.addHandler(ch)

     logger.info('creating an instance of auxiliary_module.Auxiliary')
     a = auxiliary_module.Auxiliary()
     logger.info('created an instance of auxiliary_module.Auxiliary')
     logger.info('calling auxiliary_module.Auxiliary.do_something')
     a.do_something()
     logger.info('finished auxiliary_module.Auxiliary.do_something')
     logger.info('calling auxiliary_module.some_function()')
     auxiliary_module.some_function()
     logger.info('done with auxiliary_module.some_function()')

  Here is the auxiliary module:

     import logging

     # create logger
     module_logger = logging.getLogger('spam_application.auxiliary')

     class Auxiliary:
         def __init__(self):
             self.logger = logging.getLogger('spam_application.auxiliary.Auxiliary')
             self.logger.info('creating an instance of Auxiliary')
         def do_something(self):
             self.logger.info('doing something')
             a = 1 + 1
             self.logger.info('done doing something')

     def some_function():
         module_logger.info('received a call to "some_function"')

  The output looks like this:

     2005-03-23 23:47:11,663 - spam_application - INFO -
        creating an instance of auxiliary_module.Auxiliary
     2005-03-23 23:47:11,665 - spam_application.auxiliary.Auxiliary - INFO -
        creating an instance of Auxiliary
     2005-03-23 23:47:11,665 - spam_application - INFO -
        created an instance of auxiliary_module.Auxiliary
     2005-03-23 23:47:11,668 - spam_application - INFO -
        calling auxiliary_module.Auxiliary.do_something
     2005-03-23 23:47:11,668 - spam_application.auxiliary.Auxiliary - INFO -
        doing something
     2005-03-23 23:47:11,669 - spam_application.auxiliary.Auxiliary - INFO -
        done doing something
     2005-03-23 23:47:11,670 - spam_application - INFO -
        finished auxiliary_module.Auxiliary.do_something
     2005-03-23 23:47:11,671 - spam_application - INFO -
        calling auxiliary_module.some_function()
     2005-03-23 23:47:11,672 - spam_application.auxiliary - INFO -
        received a call to 'some_function'
     2005-03-23 23:47:11,673 - spam_application - INFO -
        done with auxiliary_module.some_function()


File: python.info,  Node: Multiple handlers and formatters,  Next: Logging to multiple destinations,  Prev: Using logging in multiple modules,  Up: Logging Cookbook

10.8.2 Multiple handlers and formatters
---------------------------------------

Loggers are plain Python objects.  The *note addHandler(): 12d2. method
has no minimum or maximum quota for the number of handlers you may add.
Sometimes it will be beneficial for an application to log all messages
of all severities to a text file while simultaneously logging errors or
above to the console.  To set this up, simply configure the appropriate
handlers.  The logging calls in the application code will remain
unchanged.  Here is a slight modification to the previous simple
module-based configuration example:

     import logging

     logger = logging.getLogger('simple_example')
     logger.setLevel(logging.DEBUG)
     # create file handler which logs even debug messages
     fh = logging.FileHandler('spam.log')
     fh.setLevel(logging.DEBUG)
     # create console handler with a higher log level
     ch = logging.StreamHandler()
     ch.setLevel(logging.ERROR)
     # create formatter and add it to the handlers
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
     ch.setFormatter(formatter)
     fh.setFormatter(formatter)
     # add the handlers to logger
     logger.addHandler(ch)
     logger.addHandler(fh)

     # 'application' code
     logger.debug('debug message')
     logger.info('info message')
     logger.warn('warn message')
     logger.error('error message')
     logger.critical('critical message')

  Notice that the ’application’ code does not care about multiple
handlers.  All that changed was the addition and configuration of a new
handler named _fh_.

  The ability to create new handlers with higher- or lower-severity
filters can be very helpful when writing and testing an application.
Instead of using many ‘print’ statements for debugging, use
‘logger.debug’: Unlike the print statements, which you will have to
delete or comment out later, the logger.debug statements can remain
intact in the source code and remain dormant until you need them again.
At that time, the only change that needs to happen is to modify the
severity level of the logger and/or handler to debug.


File: python.info,  Node: Logging to multiple destinations,  Next: Configuration server example,  Prev: Multiple handlers and formatters,  Up: Logging Cookbook

10.8.3 Logging to multiple destinations
---------------------------------------

Let’s say you want to log to console and file with different message
formats and in differing circumstances.  Say you want to log messages
with levels of DEBUG and higher to file, and those messages at level
INFO and higher to the console.  Let’s also assume that the file should
contain timestamps, but the console messages should not.  Here’s how you
can achieve this:

     import logging

     # set up logging to file - see previous section for more details
     logging.basicConfig(level=logging.DEBUG,
                         format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                         datefmt='%m-%d %H:%M',
                         filename='/temp/myapp.log',
                         filemode='w')
     # define a Handler which writes INFO messages or higher to the sys.stderr
     console = logging.StreamHandler()
     console.setLevel(logging.INFO)
     # set a format which is simpler for console use
     formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
     # tell the handler to use this format
     console.setFormatter(formatter)
     # add the handler to the root logger
     logging.getLogger('').addHandler(console)

     # Now, we can log to the root logger, or any other logger. First the root...
     logging.info('Jackdaws love my big sphinx of quartz.')

     # Now, define a couple of other loggers which might represent areas in your
     # application:

     logger1 = logging.getLogger('myapp.area1')
     logger2 = logging.getLogger('myapp.area2')

     logger1.debug('Quick zephyrs blow, vexing daft Jim.')
     logger1.info('How quickly daft jumping zebras vex.')
     logger2.warning('Jail zesty vixen who grabbed pay from quack.')
     logger2.error('The five boxing wizards jump quickly.')

  When you run this, on the console you will see

     root        : INFO     Jackdaws love my big sphinx of quartz.
     myapp.area1 : INFO     How quickly daft jumping zebras vex.
     myapp.area2 : WARNING  Jail zesty vixen who grabbed pay from quack.
     myapp.area2 : ERROR    The five boxing wizards jump quickly.

  and in the file you will see something like

     10-22 22:19 root         INFO     Jackdaws love my big sphinx of quartz.
     10-22 22:19 myapp.area1  DEBUG    Quick zephyrs blow, vexing daft Jim.
     10-22 22:19 myapp.area1  INFO     How quickly daft jumping zebras vex.
     10-22 22:19 myapp.area2  WARNING  Jail zesty vixen who grabbed pay from quack.
     10-22 22:19 myapp.area2  ERROR    The five boxing wizards jump quickly.

  As you can see, the DEBUG message only shows up in the file.  The
other messages are sent to both destinations.

  This example uses console and file handlers, but you can use any
number and combination of handlers you choose.


File: python.info,  Node: Configuration server example,  Next: Sending and receiving logging events across a network,  Prev: Logging to multiple destinations,  Up: Logging Cookbook

10.8.4 Configuration server example
-----------------------------------

Here is an example of a module using the logging configuration server:

     import logging
     import logging.config
     import time
     import os

     # read initial config file
     logging.config.fileConfig('logging.conf')

     # create and start listener on port 9999
     t = logging.config.listen(9999)
     t.start()

     logger = logging.getLogger('simpleExample')

     try:
         # loop through logging calls to see the difference
         # new configurations make, until Ctrl+C is pressed
         while True:
             logger.debug('debug message')
             logger.info('info message')
             logger.warn('warn message')
             logger.error('error message')
             logger.critical('critical message')
             time.sleep(5)
     except KeyboardInterrupt:
         # cleanup
         logging.config.stopListening()
         t.join()

  And here is a script that takes a filename and sends that file to the
server, properly preceded with the binary-encoded length, as the new
logging configuration:

     #!/usr/bin/env python
     import socket, sys, struct

     with open(sys.argv[1], 'rb') as f:
         data_to_send = f.read()

     HOST = 'localhost'
     PORT = 9999
     s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
     print('connecting...')
     s.connect((HOST, PORT))
     print('sending config...')
     s.send(struct.pack('>L', len(data_to_send)))
     s.send(data_to_send)
     s.close()
     print('complete')


File: python.info,  Node: Sending and receiving logging events across a network,  Next: Adding contextual information to your logging output,  Prev: Configuration server example,  Up: Logging Cookbook

10.8.5 Sending and receiving logging events across a network
------------------------------------------------------------

Let’s say you want to send logging events across a network, and handle
them at the receiving end.  A simple way of doing this is attaching a
‘SocketHandler’ instance to the root logger at the sending end:

     import logging, logging.handlers

     rootLogger = logging.getLogger('')
     rootLogger.setLevel(logging.DEBUG)
     socketHandler = logging.handlers.SocketHandler('localhost',
                         logging.handlers.DEFAULT_TCP_LOGGING_PORT)
     # don't bother with a formatter, since a socket handler sends the event as
     # an unformatted pickle
     rootLogger.addHandler(socketHandler)

     # Now, we can log to the root logger, or any other logger. First the root...
     logging.info('Jackdaws love my big sphinx of quartz.')

     # Now, define a couple of other loggers which might represent areas in your
     # application:

     logger1 = logging.getLogger('myapp.area1')
     logger2 = logging.getLogger('myapp.area2')

     logger1.debug('Quick zephyrs blow, vexing daft Jim.')
     logger1.info('How quickly daft jumping zebras vex.')
     logger2.warning('Jail zesty vixen who grabbed pay from quack.')
     logger2.error('The five boxing wizards jump quickly.')

  At the receiving end, you can set up a receiver using the *note
SocketServer: 15d. module.  Here is a basic working example:

     import pickle
     import logging
     import logging.handlers
     import SocketServer
     import struct


     class LogRecordStreamHandler(SocketServer.StreamRequestHandler):
         """Handler for a streaming logging request.

         This basically logs the record using whatever logging policy is
         configured locally.
         """

         def handle(self):
             """
             Handle multiple requests - each expected to be a 4-byte length,
             followed by the LogRecord in pickle format. Logs the record
             according to whatever policy is configured locally.
             """
             while True:
                 chunk = self.connection.recv(4)
                 if len(chunk) < 4:
                     break
                 slen = struct.unpack('>L', chunk)[0]
                 chunk = self.connection.recv(slen)
                 while len(chunk) < slen:
                     chunk = chunk + self.connection.recv(slen - len(chunk))
                 obj = self.unPickle(chunk)
                 record = logging.makeLogRecord(obj)
                 self.handleLogRecord(record)

         def unPickle(self, data):
             return pickle.loads(data)

         def handleLogRecord(self, record):
             # if a name is specified, we use the named logger rather than the one
             # implied by the record.
             if self.server.logname is not None:
                 name = self.server.logname
             else:
                 name = record.name
             logger = logging.getLogger(name)
             # N.B. EVERY record gets logged. This is because Logger.handle
             # is normally called AFTER logger-level filtering. If you want
             # to do filtering, do it at the client end to save wasting
             # cycles and network bandwidth!
             logger.handle(record)

     class LogRecordSocketReceiver(SocketServer.ThreadingTCPServer):
         """
         Simple TCP socket-based logging receiver suitable for testing.
         """

         allow_reuse_address = 1

         def __init__(self, host='localhost',
                      port=logging.handlers.DEFAULT_TCP_LOGGING_PORT,
                      handler=LogRecordStreamHandler):
             SocketServer.ThreadingTCPServer.__init__(self, (host, port), handler)
             self.abort = 0
             self.timeout = 1
             self.logname = None

         def serve_until_stopped(self):
             import select
             abort = 0
             while not abort:
                 rd, wr, ex = select.select([self.socket.fileno()],
                                            [], [],
                                            self.timeout)
                 if rd:
                     self.handle_request()
                 abort = self.abort

     def main():
         logging.basicConfig(
             format='%(relativeCreated)5d %(name)-15s %(levelname)-8s %(message)s')
         tcpserver = LogRecordSocketReceiver()
         print('About to start TCP server...')
         tcpserver.serve_until_stopped()

     if __name__ == '__main__':
         main()

  First run the server, and then the client.  On the client side,
nothing is printed on the console; on the server side, you should see
something like:

     About to start TCP server...
        59 root            INFO     Jackdaws love my big sphinx of quartz.
        59 myapp.area1     DEBUG    Quick zephyrs blow, vexing daft Jim.
        69 myapp.area1     INFO     How quickly daft jumping zebras vex.
        69 myapp.area2     WARNING  Jail zesty vixen who grabbed pay from quack.
        69 myapp.area2     ERROR    The five boxing wizards jump quickly.

  Note that there are some security issues with pickle in some
scenarios.  If these affect you, you can use an alternative
serialization scheme by overriding the *note makePickle(): 1357. method
and implementing your alternative there, as well as adapting the above
script to use your alternative serialization.


File: python.info,  Node: Adding contextual information to your logging output,  Next: Logging to a single file from multiple processes,  Prev: Sending and receiving logging events across a network,  Up: Logging Cookbook

10.8.6 Adding contextual information to your logging output
-----------------------------------------------------------

Sometimes you want logging output to contain contextual information in
addition to the parameters passed to the logging call.  For example, in
a networked application, it may be desirable to log client-specific
information in the log (e.g.  remote client’s username, or IP address).
Although you could use the _extra_ parameter to achieve this, it’s not
always convenient to pass the information in this way.  While it might
be tempting to create *note Logger: 1dd. instances on a per-connection
basis, this is not a good idea because these instances are not garbage
collected.  While this is not a problem in practice, when the number of
*note Logger: 1dd. instances is dependent on the level of granularity
you want to use in logging an application, it could be hard to manage if
the number of *note Logger: 1dd. instances becomes effectively
unbounded.

* Menu:

* Using LoggerAdapters to impart contextual information:: 
* Using Filters to impart contextual information:: 

Using LoggerAdapters to impart contextual information

* Using objects other than dicts to pass contextual information:: 


File: python.info,  Node: Using LoggerAdapters to impart contextual information,  Next: Using Filters to impart contextual information,  Up: Adding contextual information to your logging output

10.8.6.1 Using LoggerAdapters to impart contextual information
..............................................................

An easy way in which you can pass contextual information to be output
along with logging event information is to use the *note LoggerAdapter:
1df. class.  This class is designed to look like a *note Logger: 1dd, so
that you can call *note debug(): 12c9, *note info(): 12f6, *note
warning(): 1306, *note error(): 1307, *note exception(): 1309, *note
critical(): 1308. and *note log(): 130a.  These methods have the same
signatures as their counterparts in *note Logger: 1dd, so you can use
the two types of instances interchangeably.

  When you create an instance of *note LoggerAdapter: 1df, you pass it a
*note Logger: 1dd. instance and a dict-like object which contains your
contextual information.  When you call one of the logging methods on an
instance of *note LoggerAdapter: 1df, it delegates the call to the
underlying instance of *note Logger: 1dd. passed to its constructor, and
arranges to pass the contextual information in the delegated call.
Here’s a snippet from the code of *note LoggerAdapter: 1df.:

     def debug(self, msg, *args, **kwargs):
         """
         Delegate a debug call to the underlying logger, after adding
         contextual information from this adapter instance.
         """
         msg, kwargs = self.process(msg, kwargs)
         self.logger.debug(msg, *args, **kwargs)

  The *note process(): 1301. method of *note LoggerAdapter: 1df. is
where the contextual information is added to the logging output.  It’s
passed the message and keyword arguments of the logging call, and it
passes back (potentially) modified versions of these to use in the call
to the underlying logger.  The default implementation of this method
leaves the message alone, but inserts an ’extra’ key in the keyword
argument whose value is the dict-like object passed to the constructor.
Of course, if you had passed an ’extra’ keyword argument in the call to
the adapter, it will be silently overwritten.

  The advantage of using ’extra’ is that the values in the dict-like
object are merged into the *note LogRecord: 12d7. instance’s __dict__,
allowing you to use customized strings with your *note Formatter: 12c7.
instances which know about the keys of the dict-like object.  If you
need a different method, e.g.  if you want to prepend or append the
contextual information to the message string, you just need to subclass
*note LoggerAdapter: 1df. and override *note process(): 1301. to do what
you need.  Here is a simple example:

     class CustomAdapter(logging.LoggerAdapter):
         """
         This example adapter expects the passed in dict-like object to have a
         'connid' key, whose value in brackets is prepended to the log message.
         """
         def process(self, msg, kwargs):
             return '[%s] %s' % (self.extra['connid'], msg), kwargs

  which you can use like this:

     logger = logging.getLogger(__name__)
     adapter = CustomAdapter(logger, {'connid': some_conn_id})

  Then any events that you log to the adapter will have the value of
‘some_conn_id’ prepended to the log messages.

* Menu:

* Using objects other than dicts to pass contextual information:: 


File: python.info,  Node: Using objects other than dicts to pass contextual information,  Up: Using LoggerAdapters to impart contextual information

10.8.6.2 Using objects other than dicts to pass contextual information
......................................................................

You don’t need to pass an actual dict to a *note LoggerAdapter: 1df. -
you could pass an instance of a class which implements ‘__getitem__’ and
‘__iter__’ so that it looks like a dict to logging.  This would be
useful if you want to generate values dynamically (whereas the values in
a dict would be constant).


File: python.info,  Node: Using Filters to impart contextual information,  Prev: Using LoggerAdapters to impart contextual information,  Up: Adding contextual information to your logging output

10.8.6.3 Using Filters to impart contextual information
.......................................................

You can also add contextual information to log output using a
user-defined *note Filter: 12f4.  ‘Filter’ instances are allowed to
modify the ‘LogRecords’ passed to them, including adding additional
attributes which can then be output using a suitable format string, or
if needed a custom *note Formatter: 12c7.

  For example in a web application, the request being processed (or at
least, the interesting parts of it) can be stored in a threadlocal
(*note threading.local: 15c9.) variable, and then accessed from a
‘Filter’ to add, say, information from the request - say, the remote IP
address and remote user’s username - to the ‘LogRecord’, using the
attribute names ’ip’ and ’user’ as in the ‘LoggerAdapter’ example above.
In that case, the same format string can be used to get similar output
to that shown above.  Here’s an example script:

     import logging
     from random import choice

     class ContextFilter(logging.Filter):
         """
         This is a filter which injects contextual information into the log.

         Rather than use actual contextual information, we just use random
         data in this demo.
         """

         USERS = ['jim', 'fred', 'sheila']
         IPS = ['123.231.231.123', '127.0.0.1', '192.168.0.1']

         def filter(self, record):

             record.ip = choice(ContextFilter.IPS)
             record.user = choice(ContextFilter.USERS)
             return True

     if __name__ == '__main__':
        levels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL)
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)-15s %(name)-5s %(levelname)-8s IP: %(ip)-15s User: %(user)-8s %(message)s')
        a1 = logging.getLogger('a.b.c')
        a2 = logging.getLogger('d.e.f')

        f = ContextFilter()
        a1.addFilter(f)
        a2.addFilter(f)
        a1.debug('A debug message')
        a1.info('An info message with %s', 'some parameters')
        for x in range(10):
            lvl = choice(levels)
            lvlname = logging.getLevelName(lvl)
            a2.log(lvl, 'A message at %s level with %d %s', lvlname, 2, 'parameters')

  which, when run, produces something like:

     2010-09-06 22:38:15,292 a.b.c DEBUG    IP: 123.231.231.123 User: fred     A debug message
     2010-09-06 22:38:15,300 a.b.c INFO     IP: 192.168.0.1     User: sheila   An info message with some parameters
     2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f ERROR    IP: 127.0.0.1       User: jim      A message at ERROR level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 127.0.0.1       User: sheila   A message at DEBUG level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f ERROR    IP: 123.231.231.123 User: fred     A message at ERROR level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 192.168.0.1     User: jim      A message at CRITICAL level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
     2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 192.168.0.1     User: jim      A message at DEBUG level with 2 parameters
     2010-09-06 22:38:15,301 d.e.f ERROR    IP: 127.0.0.1       User: sheila   A message at ERROR level with 2 parameters
     2010-09-06 22:38:15,301 d.e.f DEBUG    IP: 123.231.231.123 User: fred     A message at DEBUG level with 2 parameters
     2010-09-06 22:38:15,301 d.e.f INFO     IP: 123.231.231.123 User: fred     A message at INFO level with 2 parameters


File: python.info,  Node: Logging to a single file from multiple processes,  Next: Using file rotation,  Prev: Adding contextual information to your logging output,  Up: Logging Cookbook

10.8.7 Logging to a single file from multiple processes
-------------------------------------------------------

Although logging is thread-safe, and logging to a single file from
multiple threads in a single process _is_ supported, logging to a single
file from _multiple processes_ is _not_ supported, because there is no
standard way to serialize access to a single file across multiple
processes in Python.  If you need to log to a single file from multiple
processes, one way of doing this is to have all the processes log to a
*note SocketHandler: 1352, and have a separate process which implements
a socket server which reads from the socket and logs to file.  (If you
prefer, you can dedicate one thread in one of the existing processes to
perform this function.)  *note This section: 3032. documents this
approach in more detail and includes a working socket receiver which can
be used as a starting point for you to adapt in your own applications.

  If you are using a recent version of Python which includes the *note
multiprocessing: 119. module, you could write your own handler which
uses the *note Lock: 1620. class from this module to serialize access to
the file from your processes.  The existing *note FileHandler: 1333. and
subclasses do not make use of *note multiprocessing: 119. at present,
though they may do so in the future.  Note that at present, the *note
multiprocessing: 119. module does not provide working lock functionality
on all platforms (see ‘http://bugs.python.org/issue3770’).


File: python.info,  Node: Using file rotation,  Next: An example dictionary-based configuration,  Prev: Logging to a single file from multiple processes,  Up: Logging Cookbook

10.8.8 Using file rotation
--------------------------

Sometimes you want to let a log file grow to a certain size, then open a
new file and log to that.  You may want to keep a certain number of
these files, and when that many files have been created, rotate the
files so that the number of files and the size of the files both remain
bounded.  For this usage pattern, the logging package provides a *note
RotatingFileHandler: 1324.:

     import glob
     import logging
     import logging.handlers

     LOG_FILENAME = 'logging_rotatingfile_example.out'

     # Set up a specific logger with our desired output level
     my_logger = logging.getLogger('MyLogger')
     my_logger.setLevel(logging.DEBUG)

     # Add the log message handler to the logger
     handler = logging.handlers.RotatingFileHandler(
                   LOG_FILENAME, maxBytes=20, backupCount=5)

     my_logger.addHandler(handler)

     # Log some messages
     for i in range(20):
         my_logger.debug('i = %d' % i)

     # See what files are created
     logfiles = glob.glob('%s*' % LOG_FILENAME)

     for filename in logfiles:
         print(filename)

  The result should be 6 separate files, each with part of the log
history for the application:

     logging_rotatingfile_example.out
     logging_rotatingfile_example.out.1
     logging_rotatingfile_example.out.2
     logging_rotatingfile_example.out.3
     logging_rotatingfile_example.out.4
     logging_rotatingfile_example.out.5

  The most current file is always ‘logging_rotatingfile_example.out’,
and each time it reaches the size limit it is renamed with the suffix
‘.1’.  Each of the existing backup files is renamed to increment the
suffix (‘.1’ becomes ‘.2’, etc.)  and the ‘.6’ file is erased.

  Obviously this example sets the log length much too small as an
extreme example.  You would want to set _maxBytes_ to an appropriate
value.


File: python.info,  Node: An example dictionary-based configuration,  Next: Inserting a BOM into messages sent to a SysLogHandler,  Prev: Using file rotation,  Up: Logging Cookbook

10.8.9 An example dictionary-based configuration
------------------------------------------------

Below is an example of a logging configuration dictionary - it’s taken
from the documentation on the Django project(1).  This dictionary is
passed to *note dictConfig(): 1317. to put the configuration into
effect:

     LOGGING = {
         'version': 1,
         'disable_existing_loggers': True,
         'formatters': {
             'verbose': {
                 'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'
             },
             'simple': {
                 'format': '%(levelname)s %(message)s'
             },
         },
         'filters': {
             'special': {
                 '()': 'project.logging.SpecialFilter',
                 'foo': 'bar',
             }
         },
         'handlers': {
             'null': {
                 'level':'DEBUG',
                 'class':'django.utils.log.NullHandler',
             },
             'console':{
                 'level':'DEBUG',
                 'class':'logging.StreamHandler',
                 'formatter': 'simple'
             },
             'mail_admins': {
                 'level': 'ERROR',
                 'class': 'django.utils.log.AdminEmailHandler',
                 'filters': ['special']
             }
         },
         'loggers': {
             'django': {
                 'handlers':['null'],
                 'propagate': True,
                 'level':'INFO',
             },
             'django.request': {
                 'handlers': ['mail_admins'],
                 'level': 'ERROR',
                 'propagate': False,
             },
             'myproject.custom': {
                 'handlers': ['console', 'mail_admins'],
                 'level': 'INFO',
                 'filters': ['special']
             }
         }
     }

  For more information about this configuration, you can see the
relevant section(2) of the Django documentation.

   ---------- Footnotes ----------

   (1) 
https://docs.djangoproject.com/en/1.3/topics/logging/#configuring-logging

   (2) 
https://docs.djangoproject.com/en/1.6/topics/logging/#configuring-logging


File: python.info,  Node: Inserting a BOM into messages sent to a SysLogHandler,  Next: Implementing structured logging,  Prev: An example dictionary-based configuration,  Up: Logging Cookbook

10.8.10 Inserting a BOM into messages sent to a SysLogHandler
-------------------------------------------------------------

RFC 5424(1) requires that a Unicode message be sent to a syslog daemon
as a set of bytes which have the following structure: an optional
pure-ASCII component, followed by a UTF-8 Byte Order Mark (BOM),
followed by Unicode encoded using UTF-8.  (See the relevant section of
the specification(2).)

  In Python 2.6 and 2.7, code was added to *note SysLogHandler: 1da. to
insert a BOM into the message, but unfortunately, it was implemented
incorrectly, with the BOM appearing at the beginning of the message and
hence not allowing any pure-ASCII component to appear before it.

  As this behaviour is broken, the incorrect BOM insertion code is being
removed from Python 2.7.4 and later.  However, it is not being replaced,
and if you want to produce RFC 5424-compliant messages which include a
BOM, an optional pure-ASCII sequence before it and arbitrary Unicode
after it, encoded using UTF-8, then you need to do the following:

  1. Attach a *note Formatter: 12c7. instance to your *note
     SysLogHandler: 1da. instance, with a format string such as:

          u'ASCII section\ufeffUnicode section'

     The Unicode code point ‘u'\ufeff'’, when encoded using UTF-8, will
     be encoded as a UTF-8 BOM – the byte-string ‘'\xef\xbb\xbf'’.

  2. Replace the ASCII section with whatever placeholders you like, but
     make sure that the data that appears in there after substitution is
     always ASCII (that way, it will remain unchanged after UTF-8
     encoding).

  3. Replace the Unicode section with whatever placeholders you like; if
     the data which appears there after substitution contains characters
     outside the ASCII range, that’s fine – it will be encoded using
     UTF-8.

  If the formatted message is Unicode, it _will_ be encoded using UTF-8
encoding by ‘SysLogHandler’.  If you follow the above rules, you should
be able to produce RFC 5424-compliant messages.  If you don’t, logging
may not complain, but your messages will not be RFC 5424-compliant, and
your syslog daemon may complain.

   ---------- Footnotes ----------

   (1) http://tools.ietf.org/html/rfc5424

   (2) http://tools.ietf.org/html/rfc5424#section-6


File: python.info,  Node: Implementing structured logging,  Next: Customizing handlers with dictConfig,  Prev: Inserting a BOM into messages sent to a SysLogHandler,  Up: Logging Cookbook

10.8.11 Implementing structured logging
---------------------------------------

Although most logging messages are intended for reading by humans, and
thus not readily machine-parseable, there might be cirumstances where
you want to output messages in a structured format which _is_ capable of
being parsed by a program (without needing complex regular expressions
to parse the log message).  This is straightforward to achieve using the
logging package.  There are a number of ways in which this could be
achieved, but the following is a simple approach which uses JSON to
serialise the event in a machine-parseable manner:

     import json
     import logging

     class StructuredMessage(object):
         def __init__(self, message, **kwargs):
             self.message = message
             self.kwargs = kwargs

         def __str__(self):
             return '%s >>> %s' % (self.message, json.dumps(self.kwargs))

     _ = StructuredMessage   # optional, to improve readability

     logging.basicConfig(level=logging.INFO, format='%(message)s')
     logging.info(_('message 1', foo='bar', bar='baz', num=123, fnum=123.456))

  If the above script is run, it prints:

     message 1 >>> {"fnum": 123.456, "num": 123, "bar": "baz", "foo": "bar"}

  Note that the order of items might be different according to the
version of Python used.

  If you need more specialised processing, you can use a custom JSON
encoder, as in the following complete example:

     from __future__ import unicode_literals

     import json
     import logging

     # This next bit is to ensure the script runs unchanged on 2.x and 3.x
     try:
         unicode
     except NameError:
         unicode = str

     class Encoder(json.JSONEncoder):
         def default(self, o):
             if isinstance(o, set):
                 return tuple(o)
             elif isinstance(o, unicode):
                 return o.encode('unicode_escape').decode('ascii')
             return super(Encoder, self).default(o)

     class StructuredMessage(object):
         def __init__(self, message, **kwargs):
             self.message = message
             self.kwargs = kwargs

         def __str__(self):
             s = Encoder().encode(self.kwargs)
             return '%s >>> %s' % (self.message, s)

     _ = StructuredMessage   # optional, to improve readability

     def main():
         logging.basicConfig(level=logging.INFO, format='%(message)s')
         logging.info(_('message 1', set_value=set([1, 2, 3]), snowman='\u2603'))

     if __name__ == '__main__':
         main()

  When the above script is run, it prints:

     message 1 >>> {"snowman": "\u2603", "set_value": [1, 2, 3]}

  Note that the order of items might be different according to the
version of Python used.


File: python.info,  Node: Customizing handlers with dictConfig,  Next: Configuring filters with dictConfig,  Prev: Implementing structured logging,  Up: Logging Cookbook

10.8.12 Customizing handlers with ‘dictConfig()’
------------------------------------------------

There are times when you want to customize logging handlers in
particular ways, and if you use *note dictConfig(): 1317. you may be
able to do this without subclassing.  As an example, consider that you
may want to set the ownership of a log file.  On POSIX, this is easily
done using ‘shutil.chown()’, but the file handlers in the stdlib don’t
offer built-in support.  You can customize handler creation using a
plain function such as:

     def owned_file_handler(filename, mode='a', encoding=None, owner=None):
         if owner:
             if not os.path.exists(filename):
                 open(filename, 'a').close()
             shutil.chown(filename, *owner)
         return logging.FileHandler(filename, mode, encoding)

  You can then specify, in a logging configuration passed to *note
dictConfig(): 1317, that a logging handler be created by calling this
function:

     LOGGING = {
         'version': 1,
         'disable_existing_loggers': False,
         'formatters': {
             'default': {
                 'format': '%(asctime)s %(levelname)s %(name)s %(message)s'
             },
         },
         'handlers': {
             'file':{
                 # The values below are popped from this dictionary and
                 # used to create the handler, set the handler's level and
                 # its formatter.
                 '()': owned_file_handler,
                 'level':'DEBUG',
                 'formatter': 'default',
                 # The values below are passed to the handler creator callable
                 # as keyword arguments.
                 'owner': ['pulse', 'pulse'],
                 'filename': 'chowntest.log',
                 'mode': 'w',
                 'encoding': 'utf-8',
             },
         },
         'root': {
             'handlers': ['file'],
             'level': 'DEBUG',
         },
     }

  In this example I am setting the ownership using the ‘pulse’ user and
group, just for the purposes of illustration.  Putting it together into
a working script, ‘chowntest.py’:

     import logging, logging.config, os, shutil

     def owned_file_handler(filename, mode='a', encoding=None, owner=None):
         if owner:
             if not os.path.exists(filename):
                 open(filename, 'a').close()
             shutil.chown(filename, *owner)
         return logging.FileHandler(filename, mode, encoding)

     LOGGING = {
         'version': 1,
         'disable_existing_loggers': False,
         'formatters': {
             'default': {
                 'format': '%(asctime)s %(levelname)s %(name)s %(message)s'
             },
         },
         'handlers': {
             'file':{
                 # The values below are popped from this dictionary and
                 # used to create the handler, set the handler's level and
                 # its formatter.
                 '()': owned_file_handler,
                 'level':'DEBUG',
                 'formatter': 'default',
                 # The values below are passed to the handler creator callable
                 # as keyword arguments.
                 'owner': ['pulse', 'pulse'],
                 'filename': 'chowntest.log',
                 'mode': 'w',
                 'encoding': 'utf-8',
             },
         },
         'root': {
             'handlers': ['file'],
             'level': 'DEBUG',
         },
     }

     logging.config.dictConfig(LOGGING)
     logger = logging.getLogger('mylogger')
     logger.debug('A debug message')

  To run this, you will probably need to run as ‘root’:

     $ sudo python3.3 chowntest.py
     $ cat chowntest.log
     2013-11-05 09:34:51,128 DEBUG mylogger A debug message
     $ ls -l chowntest.log
     -rw-r--r-- 1 pulse pulse 55 2013-11-05 09:34 chowntest.log

  Note that this example uses Python 3.3 because that’s where
‘shutil.chown()’ makes an appearance.  This approach should work with
any Python version that supports *note dictConfig(): 1317. - namely,
Python 2.7, 3.2 or later.  With pre-3.3 versions, you would need to
implement the actual ownership change using e.g.  *note os.chown():
1144.

  In practice, the handler-creating function may be in a utility module
somewhere in your project.  Instead of the line in the configuration:

     '()': owned_file_handler,

  you could use e.g.:

     '()': 'ext://project.util.owned_file_handler',

  where ‘project.util’ can be replaced with the actual name of the
package where the function resides.  In the above working script, using
‘'ext://__main__.owned_file_handler'’ should work.  Here, the actual
callable is resolved by *note dictConfig(): 1317. from the ‘ext://’
specification.

  This example hopefully also points the way to how you could implement
other types of file change - e.g.  setting specific POSIX permission
bits - in the same way, using *note os.chmod(): e2e.

  Of course, the approach could also be extended to types of handler
other than a *note FileHandler: 1333. - for example, one of the rotating
file handlers, or a different type of handler altogether.


File: python.info,  Node: Configuring filters with dictConfig,  Prev: Customizing handlers with dictConfig,  Up: Logging Cookbook

10.8.13 Configuring filters with ‘dictConfig()’
-----------------------------------------------

You _can_ configure filters using *note dictConfig(): 1317, though it
might not be obvious at first glance how to do it (hence this recipe).
Since *note Filter: 12f4. is the only filter class included in the
standard library, and it is unlikely to cater to many requirements (it’s
only there as a base class), you will typically need to define your own
*note Filter: 12f4. subclass with an overridden *note filter(): 12f5.
method.  To do this, specify the ‘()’ key in the configuration
dictionary for the filter, specifying a callable which will be used to
create the filter (a class is the most obvious, but you can provide any
callable which returns a *note Filter: 12f4. instance).  Here is a
complete example:

     import logging
     import logging.config
     import sys

     class MyFilter(logging.Filter):
         def __init__(self, param=None):
             self.param = param

         def filter(self, record):
             if self.param is None:
                 allow = True
             else:
                 allow = self.param not in record.msg
             if allow:
                 record.msg = 'changed: ' + record.msg
             return allow

     LOGGING = {
         'version': 1,
         'filters': {
             'myfilter': {
                 '()': MyFilter,
                 'param': 'noshow',
             }
         },
         'handlers': {
             'console': {
                 'class': 'logging.StreamHandler',
                 'filters': ['myfilter']
             }
         },
         'root': {
             'level': 'DEBUG',
             'handlers': ['console']
         },
     }

     if __name__ == '__main__':
         logging.config.dictConfig(LOGGING)
         logging.debug('hello')
         logging.debug('hello - noshow')

  This example shows how you can pass configuration data to the callable
which constructs the instance, in the form of keyword parameters.  When
run, the above script will print:

     changed: hello

  which shows that the filter is working as configured.

  A couple of extra points to note:

   * If you can’t refer to the callable directly in the configuration
     (e.g.  if it lives in a different module, and you can’t import it
     directly where the configuration dictionary is), you can use the
     form ‘ext://...’ as described in *note Access to external objects:
     132a.  For example, you could have used the text
     ‘'ext://__main__.MyFilter'’ instead of ‘MyFilter’ in the above
     example.

   * As well as for filters, this technique can also be used to
     configure custom handlers and formatters.  See *note User-defined
     objects: 1323. for more information on how logging supports using
     user-defined objects in its configuration, and see the other
     cookbook recipe *note Customizing handlers with dictConfig(): 303f.
     above.


File: python.info,  Node: Regular Expression HOWTO,  Next: Socket Programming HOWTO,  Prev: Logging Cookbook,  Up: Python HOWTOs

10.9 Regular Expression HOWTO
=============================

     Author: A.M. Kuchling <<amk@amk.ca>>

Abstract
........

This document is an introductory tutorial to using regular expressions
in Python with the *note re: 143. module.  It provides a gentler
introduction than the corresponding section in the Library Reference.

* Menu:

* Introduction: Introduction<13>. 
* Simple Patterns:: 
* Using Regular Expressions:: 
* More Pattern Power:: 
* Modifying Strings:: 
* Common Problems:: 
* Feedback:: 


File: python.info,  Node: Introduction<13>,  Next: Simple Patterns,  Up: Regular Expression HOWTO

10.9.1 Introduction
-------------------

The *note re: 143. module was added in Python 1.5, and provides
Perl-style regular expression patterns.  Earlier versions of Python came
with the ‘regex’ module, which provided Emacs-style patterns.  The
‘regex’ module was removed completely in Python 2.5.

  Regular expressions (called REs, or regexes, or regex patterns) are
essentially a tiny, highly specialized programming language embedded
inside Python and made available through the *note re: 143. module.
Using this little language, you specify the rules for the set of
possible strings that you want to match; this set might contain English
sentences, or e-mail addresses, or TeX commands, or anything you like.
You can then ask questions such as "Does this string match the
pattern?", or "Is there a match for the pattern anywhere in this
string?".  You can also use REs to modify a string or to split it apart
in various ways.

  Regular expression patterns are compiled into a series of bytecodes
which are then executed by a matching engine written in C. For advanced
use, it may be necessary to pay careful attention to how the engine will
execute a given RE, and write the RE in a certain way in order to
produce bytecode that runs faster.  Optimization isn’t covered in this
document, because it requires that you have a good understanding of the
matching engine’s internals.

  The regular expression language is relatively small and restricted, so
not all possible string processing tasks can be done using regular
expressions.  There are also tasks that _can_ be done with regular
expressions, but the expressions turn out to be very complicated.  In
these cases, you may be better off writing Python code to do the
processing; while Python code will be slower than an elaborate regular
expression, it will also probably be more understandable.


File: python.info,  Node: Simple Patterns,  Next: Using Regular Expressions,  Prev: Introduction<13>,  Up: Regular Expression HOWTO

10.9.2 Simple Patterns
----------------------

We’ll start by learning about the simplest possible regular expressions.
Since regular expressions are used to operate on strings, we’ll begin
with the most common task: matching characters.

  For a detailed explanation of the computer science underlying regular
expressions (deterministic and non-deterministic finite automata), you
can refer to almost any textbook on writing compilers.

* Menu:

* Matching Characters:: 
* Repeating Things:: 


File: python.info,  Node: Matching Characters,  Next: Repeating Things,  Up: Simple Patterns

10.9.2.1 Matching Characters
............................

Most letters and characters will simply match themselves.  For example,
the regular expression ‘test’ will match the string ‘test’ exactly.
(You can enable a case-insensitive mode that would let this RE match
‘Test’ or ‘TEST’ as well; more about this later.)

  There are exceptions to this rule; some characters are special
_metacharacters_, and don’t match themselves.  Instead, they signal that
some out-of-the-ordinary thing should be matched, or they affect other
portions of the RE by repeating them or changing their meaning.  Much of
this document is devoted to discussing various metacharacters and what
they do.

  Here’s a complete list of the metacharacters; their meanings will be
discussed in the rest of this HOWTO.

     . ^ $ * + ? { } [ ] \ | ( )

  The first metacharacters we’ll look at are ‘[’ and ‘]’.  They’re used
for specifying a character class, which is a set of characters that you
wish to match.  Characters can be listed individually, or a range of
characters can be indicated by giving two characters and separating them
by a ‘'-'’.  For example, ‘[abc]’ will match any of the characters ‘a’,
‘b’, or ‘c’; this is the same as ‘[a-c]’, which uses a range to express
the same set of characters.  If you wanted to match only lowercase
letters, your RE would be ‘[a-z]’.

  Metacharacters are not active inside classes.  For example, ‘[akm$]’
will match any of the characters ‘'a'’, ‘'k'’, ‘'m'’, or ‘'$'’; ‘'$'’ is
usually a metacharacter, but inside a character class it’s stripped of
its special nature.

  You can match the characters not listed within the class by
_complementing_ the set.  This is indicated by including a ‘'^'’ as the
first character of the class; ‘'^'’ outside a character class will
simply match the ‘'^'’ character.  For example, ‘[^5]’ will match any
character except ‘'5'’.

  Perhaps the most important metacharacter is the backslash, ‘\’.  As in
Python string literals, the backslash can be followed by various
characters to signal various special sequences.  It’s also used to
escape all the metacharacters so you can still match them in patterns;
for example, if you need to match a ‘[’ or ‘\’, you can precede them
with a backslash to remove their special meaning: ‘\[’ or ‘\\’.

  Some of the special sequences beginning with ‘'\'’ represent
predefined sets of characters that are often useful, such as the set of
digits, the set of letters, or the set of anything that isn’t
whitespace.  The following predefined special sequences are a subset of
those available.  The equivalent classes are for byte string patterns.
For a complete list of sequences and expanded class definitions for
Unicode string patterns, see the last part of *note Regular Expression
Syntax: 9b2.

‘\d’

     Matches any decimal digit; this is equivalent to the class ‘[0-9]’.

‘\D’

     Matches any non-digit character; this is equivalent to the class
     ‘[^0-9]’.

‘\s’

     Matches any whitespace character; this is equivalent to the class
     ‘[ \t\n\r\f\v]’.

‘\S’

     Matches any non-whitespace character; this is equivalent to the
     class ‘[^ \t\n\r\f\v]’.

‘\w’

     Matches any alphanumeric character; this is equivalent to the class
     ‘[a-zA-Z0-9_]’.

‘\W’

     Matches any non-alphanumeric character; this is equivalent to the
     class ‘[^a-zA-Z0-9_]’.

  These sequences can be included inside a character class.  For
example, ‘[\s,.]’ is a character class that will match any whitespace
character, or ‘','’ or ‘'.'’.

  The final metacharacter in this section is ‘.’.  It matches anything
except a newline character, and there’s an alternate mode (‘re.DOTALL’)
where it will match even a newline.  ‘'.'’ is often used where you want
to match "any character".


File: python.info,  Node: Repeating Things,  Prev: Matching Characters,  Up: Simple Patterns

10.9.2.2 Repeating Things
.........................

Being able to match varying sets of characters is the first thing
regular expressions can do that isn’t already possible with the methods
available on strings.  However, if that was the only additional
capability of regexes, they wouldn’t be much of an advance.  Another
capability is that you can specify that portions of the RE must be
repeated a certain number of times.

  The first metacharacter for repeating things that we’ll look at is
‘*’.  ‘*’ doesn’t match the literal character ‘*’; instead, it specifies
that the previous character can be matched zero or more times, instead
of exactly once.

  For example, ‘ca*t’ will match ‘ct’ (0 ‘a’ characters), ‘cat’ (1 ‘a’),
‘caaat’ (3 ‘a’ characters), and so forth.  The RE engine has various
internal limitations stemming from the size of C’s ‘int’ type that will
prevent it from matching over 2 billion ‘a’ characters; you probably
don’t have enough memory to construct a string that large, so you
shouldn’t run into that limit.

  Repetitions such as ‘*’ are _greedy_; when repeating a RE, the
matching engine will try to repeat it as many times as possible.  If
later portions of the pattern don’t match, the matching engine will then
back up and try again with few repetitions.

  A step-by-step example will make this more obvious.  Let’s consider
the expression ‘a[bcd]*b’.  This matches the letter ‘'a'’, zero or more
letters from the class ‘[bcd]’, and finally ends with a ‘'b'’.  Now
imagine matching this RE against the string ‘abcbd’.

Step       Matched         Explanation
                           
-----------------------------------------------------------------
                           
1          ‘a’             The ‘a’ in the RE matches.
                           
                           
2          ‘abcbd’         The engine matches ‘[bcd]*’, going
                           as far as it can, which is to the
                           end of the string.
                           
                           
3          _Failure_       The engine tries to match ‘b’, but
                           the current position is at the end
                           of the string, so it fails.
                           
                           
4          ‘abcb’          Back up, so that ‘[bcd]*’ matches
                           one less character.
                           
                           
5          _Failure_       Try ‘b’ again, but the current
                           position is at the last character,
                           which is a ‘'d'’.
                           
                           
6          ‘abc’           Back up again, so that ‘[bcd]*’ is
                           only matching ‘bc’.
                           
                           
6          ‘abcb’          Try ‘b’ again.  This time the
                           character at the current position
                           is ‘'b'’, so it succeeds.
                           

  The end of the RE has now been reached, and it has matched ‘abcb’.
This demonstrates how the matching engine goes as far as it can at
first, and if no match is found it will then progressively back up and
retry the rest of the RE again and again.  It will back up until it has
tried zero matches for ‘[bcd]*’, and if that subsequently fails, the
engine will conclude that the string doesn’t match the RE at all.

  Another repeating metacharacter is ‘+’, which matches one or more
times.  Pay careful attention to the difference between ‘*’ and ‘+’; ‘*’
matches _zero_ or more times, so whatever’s being repeated may not be
present at all, while ‘+’ requires at least _one_ occurrence.  To use a
similar example, ‘ca+t’ will match ‘cat’ (1 ‘a’), ‘caaat’ (3 ‘a’’s), but
won’t match ‘ct’.

  There are two more repeating qualifiers.  The question mark character,
‘?’, matches either once or zero times; you can think of it as marking
something as being optional.  For example, ‘home-?brew’ matches either
‘homebrew’ or ‘home-brew’.

  The most complicated repeated qualifier is ‘{m,n}’, where _m_ and _n_
are decimal integers.  This qualifier means there must be at least _m_
repetitions, and at most _n_.  For example, ‘a/{1,3}b’ will match ‘a/b’,
‘a//b’, and ‘a///b’.  It won’t match ‘ab’, which has no slashes, or
‘a////b’, which has four.

  You can omit either _m_ or _n_; in that case, a reasonable value is
assumed for the missing value.  Omitting _m_ is interpreted as a lower
limit of 0, while omitting _n_ results in an upper bound of infinity —
actually, the upper bound is the 2-billion limit mentioned earlier, but
that might as well be infinity.

  Readers of a reductionist bent may notice that the three other
qualifiers can all be expressed using this notation.  ‘{0,}’ is the same
as ‘*’, ‘{1,}’ is equivalent to ‘+’, and ‘{0,1}’ is the same as ‘?’.
It’s better to use ‘*’, ‘+’, or ‘?’ when you can, simply because they’re
shorter and easier to read.


File: python.info,  Node: Using Regular Expressions,  Next: More Pattern Power,  Prev: Simple Patterns,  Up: Regular Expression HOWTO

10.9.3 Using Regular Expressions
--------------------------------

Now that we’ve looked at some simple regular expressions, how do we
actually use them in Python?  The *note re: 143. module provides an
interface to the regular expression engine, allowing you to compile REs
into objects and then perform matches with them.

* Menu:

* Compiling Regular Expressions:: 
* The Backslash Plague:: 
* Performing Matches:: 
* Module-Level Functions: Module-Level Functions<2>. 
* Compilation Flags:: 


File: python.info,  Node: Compiling Regular Expressions,  Next: The Backslash Plague,  Up: Using Regular Expressions

10.9.3.1 Compiling Regular Expressions
......................................

Regular expressions are compiled into pattern objects, which have
methods for various operations such as searching for pattern matches or
performing string substitutions.

     >>> import re
     >>> p = re.compile('ab*')
     >>> p  #doctest: +ELLIPSIS
     <_sre.SRE_Pattern object at 0x...>

  *note re.compile(): 9bf. also accepts an optional _flags_ argument,
used to enable various special features and syntax variations.  We’ll go
over the available settings later, but for now a single example will do:

     >>> p = re.compile('ab*', re.IGNORECASE)

  The RE is passed to *note re.compile(): 9bf. as a string.  REs are
handled as strings because regular expressions aren’t part of the core
Python language, and no special syntax was created for expressing them.
(There are applications that don’t need REs at all, so there’s no need
to bloat the language specification by including them.)  Instead, the
*note re: 143. module is simply a C extension module included with
Python, just like the *note socket: 15c. or *note zlib: 1ad. modules.

  Putting REs in strings keeps the Python language simpler, but has one
disadvantage which is the topic of the next section.


File: python.info,  Node: The Backslash Plague,  Next: Performing Matches,  Prev: Compiling Regular Expressions,  Up: Using Regular Expressions

10.9.3.2 The Backslash Plague
.............................

As stated earlier, regular expressions use the backslash character
(‘'\'’) to indicate special forms or to allow special characters to be
used without invoking their special meaning.  This conflicts with
Python’s usage of the same character for the same purpose in string
literals.

  Let’s say you want to write a RE that matches the string ‘\section’,
which might be found in a LaTeX file.  To figure out what to write in
the program code, start with the desired string to be matched.  Next,
you must escape any backslashes and other metacharacters by preceding
them with a backslash, resulting in the string ‘\\section’.  The
resulting string that must be passed to *note re.compile(): 9bf. must be
‘\\section’.  However, to express this as a Python string literal, both
backslashes must be escaped _again_.

Characters              Stage
                        
-----------------------------------------------------------------------
                        
‘\section’              Text string to be matched
                        
                        
‘\\section’             Escaped backslash for *note re.compile():
                        9bf.
                        
                        
‘"\\\\section"’         Escaped backslashes for a string literal
                        

  In short, to match a literal backslash, one has to write ‘'\\\\'’ as
the RE string, because the regular expression must be ‘\\’, and each
backslash must be expressed as ‘\\’ inside a regular Python string
literal.  In REs that feature backslashes repeatedly, this leads to lots
of repeated backslashes and makes the resulting strings difficult to
understand.

  The solution is to use Python’s raw string notation for regular
expressions; backslashes are not handled in any special way in a string
literal prefixed with ‘'r'’, so ‘r"\n"’ is a two-character string
containing ‘'\'’ and ‘'n'’, while ‘"\n"’ is a one-character string
containing a newline.  Regular expressions will often be written in
Python code using this raw string notation.

Regular String          Raw string
                        
-----------------------------------------------
                        
‘"ab*"’                 ‘r"ab*"’
                        
                        
‘"\\\\section"’         ‘r"\\section"’
                        
                        
‘"\\w+\\s+\\1"’         ‘r"\w+\s+\1"’
                        


File: python.info,  Node: Performing Matches,  Next: Module-Level Functions<2>,  Prev: The Backslash Plague,  Up: Using Regular Expressions

10.9.3.3 Performing Matches
...........................

Once you have an object representing a compiled regular expression, what
do you do with it?  Pattern objects have several methods and attributes.
Only the most significant ones will be covered here; consult the *note
re: 143. docs for a complete listing.

Method/Attribute       Purpose
                       
---------------------------------------------------------------------------
                       
‘match()’              Determine if the RE matches at the beginning of
                       the string.
                       
                       
‘search()’             Scan through a string, looking for any location
                       where this RE matches.
                       
                       
‘findall()’            Find all substrings where the RE matches, and
                       returns them as a list.
                       
                       
‘finditer()’           Find all substrings where the RE matches, and
                       returns them as an *note iterator: 87f.
                       

  ‘match()’ and ‘search()’ return ‘None’ if no match can be found.  If
they’re successful, a *note match object: 9d9. instance is returned,
containing information about the match: where it starts and ends, the
substring it matched, and more.

  You can learn about this by interactively experimenting with the *note
re: 143. module.  If you have Tkinter available, you may also want to
look at Tools/scripts/redemo.py(1), a demonstration program included
with the Python distribution.  It allows you to enter REs and strings,
and displays whether the RE matches or fails.  ‘redemo.py’ can be quite
useful when trying to debug a complicated RE. Phil Schwartz’s Kodos(2)
is also an interactive tool for developing and testing RE patterns.

  This HOWTO uses the standard Python interpreter for its examples.
First, run the Python interpreter, import the *note re: 143. module, and
compile a RE:

     Python 2.2.2 (#1, Feb 10 2003, 12:57:01)
     >>> import re
     >>> p = re.compile('[a-z]+')
     >>> p  #doctest: +ELLIPSIS
     <_sre.SRE_Pattern object at 0x...>

  Now, you can try matching various strings against the RE ‘[a-z]+’.  An
empty string shouldn’t match at all, since ‘+’ means ’one or more
repetitions’.  ‘match()’ should return ‘None’ in this case, which will
cause the interpreter to print no output.  You can explicitly print the
result of ‘match()’ to make this clear.

     >>> p.match("")
     >>> print p.match("")
     None

  Now, let’s try it on a string that it should match, such as ‘tempo’.
In this case, ‘match()’ will return a *note match object: 9d9, so you
should store the result in a variable for later use.

     >>> m = p.match('tempo')
     >>> m  #doctest: +ELLIPSIS
     <_sre.SRE_Match object at 0x...>

  Now you can query the *note match object: 9d9. for information about
the matching string.  *note match object: 9d9. instances also have
several methods and attributes; the most important ones are:

Method/Attribute       Purpose
                       
------------------------------------------------------------------------
                       
‘group()’              Return the string matched by the RE
                       
                       
‘start()’              Return the starting position of the match
                       
                       
‘end()’                Return the ending position of the match
                       
                       
‘span()’               Return a tuple containing the (start, end)
                       positions of the match
                       

  Trying these methods will soon clarify their meaning:

     >>> m.group()
     'tempo'
     >>> m.start(), m.end()
     (0, 5)
     >>> m.span()
     (0, 5)

  ‘group()’ returns the substring that was matched by the RE. ‘start()’
and ‘end()’ return the starting and ending index of the match.  ‘span()’
returns both start and end indexes in a single tuple.  Since the
‘match()’ method only checks if the RE matches at the start of a string,
‘start()’ will always be zero.  However, the ‘search()’ method of
patterns scans through the string, so the match may not start at zero in
that case.

     >>> print p.match('::: message')
     None
     >>> m = p.search('::: message'); print m  #doctest: +ELLIPSIS
     <_sre.SRE_Match object at 0x...>
     >>> m.group()
     'message'
     >>> m.span()
     (4, 11)

  In actual programs, the most common style is to store the *note match
object: 9d9. in a variable, and then check if it was ‘None’.  This
usually looks like:

     p = re.compile( ... )
     m = p.match( 'string goes here' )
     if m:
         print 'Match found: ', m.group()
     else:
         print 'No match'

  Two pattern methods return all of the matches for a pattern.
‘findall()’ returns a list of matching strings:

     >>> p = re.compile('\d+')
     >>> p.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')
     ['12', '11', '10']

  ‘findall()’ has to create the entire list before it can be returned as
the result.  The ‘finditer()’ method returns a sequence of *note match
object: 9d9. instances as an *note iterator: 87f.  (3)

     >>> iterator = p.finditer('12 drummers drumming, 11 ... 10 ...')
     >>> iterator  #doctest: +ELLIPSIS
     <callable-iterator object at 0x...>
     >>> for match in iterator:
     ...     print match.span()
     ...
     (0, 2)
     (22, 24)
     (29, 31)

   ---------- Footnotes ----------

   (1) http://hg.python.org/cpython/file/2.7/Tools/scripts/redemo.py

   (2) http://kodos.sourceforge.net/

   (3) Introduced in Python 2.2.2.


File: python.info,  Node: Module-Level Functions<2>,  Next: Compilation Flags,  Prev: Performing Matches,  Up: Using Regular Expressions

10.9.3.4 Module-Level Functions
...............................

You don’t have to create a pattern object and call its methods; the
*note re: 143. module also provides top-level functions called
‘match()’, ‘search()’, ‘findall()’, ‘sub()’, and so forth.  These
functions take the same arguments as the corresponding pattern method,
with the RE string added as the first argument, and still return either
‘None’ or a *note match object: 9d9. instance.

     >>> print re.match(r'From\s+', 'Fromage amk')
     None
     >>> re.match(r'From\s+', 'From amk Thu May 14 19:12:10 1998')  #doctest: +ELLIPSIS
     <_sre.SRE_Match object at 0x...>

  Under the hood, these functions simply create a pattern object for you
and call the appropriate method on it.  They also store the compiled
object in a cache, so future calls using the same RE are faster.

  Should you use these module-level functions, or should you get the
pattern and call its methods yourself?  That choice depends on how
frequently the RE will be used, and on your personal coding style.  If
the RE is being used at only one point in the code, then the module
functions are probably more convenient.  If a program contains a lot of
regular expressions, or re-uses the same ones in several locations, then
it might be worthwhile to collect all the definitions in one place, in a
section of code that compiles all the REs ahead of time.  To take an
example from the standard library, here’s an extract from the deprecated
‘xmllib’ module:

     ref = re.compile( ... )
     entityref = re.compile( ... )
     charref = re.compile( ... )
     starttagopen = re.compile( ... )

  I generally prefer to work with the compiled object, even for one-time
uses, but few people will be as much of a purist about this as I am.


File: python.info,  Node: Compilation Flags,  Prev: Module-Level Functions<2>,  Up: Using Regular Expressions

10.9.3.5 Compilation Flags
..........................

Compilation flags let you modify some aspects of how regular expressions
work.  Flags are available in the *note re: 143. module under two names,
a long name such as ‘IGNORECASE’ and a short, one-letter form such as
‘I’.  (If you’re familiar with Perl’s pattern modifiers, the one-letter
forms use the same letters; the short form of *note re.VERBOSE: 9c7. is
*note re.X: 9bd, for example.)  Multiple flags can be specified by
bitwise OR-ing them; ‘re.I | re.M’ sets both the ‘I’ and ‘M’ flags, for
example.

  Here’s a table of the available flags, followed by a more detailed
explanation of each one.

Flag                                  Meaning
                                      
---------------------------------------------------------------------------------------
                                      
‘DOTALL’, ‘S’                         Make ‘.’ match any character, including
                                      newlines
                                      
                                      
‘IGNORECASE’, ‘I’                     Do case-insensitive matches
                                      
                                      
‘LOCALE’, ‘L’                         Do a locale-aware match
                                      
                                      
‘MULTILINE’, ‘M’                      Multi-line matching, affecting ‘^’ and ‘$’
                                      
                                      
‘VERBOSE’, ‘X’                        Enable verbose REs, which can be organized
                                      more cleanly and understandably.
                                      
                                      
‘UNICODE’, ‘U’                        Makes several escapes like ‘\w’, ‘\b’, ‘\s’
                                      and ‘\d’ dependent on the Unicode character
                                      database.
                                      

 -- Data: I

 -- Data: IGNORECASE

     Perform case-insensitive matching; character class and literal
     strings will match letters by ignoring case.  For example, ‘[A-Z]’
     will match lowercase letters, too, and ‘Spam’ will match ‘Spam’,
     ‘spam’, or ‘spAM’.  This lowercasing doesn’t take the current
     locale into account; it will if you also set the ‘LOCALE’ flag.

 -- Data: L

 -- Data: LOCALE

     Make ‘\w’, ‘\W’, ‘\b’, and ‘\B’, dependent on the current locale.

     Locales are a feature of the C library intended to help in writing
     programs that take account of language differences.  For example,
     if you’re processing French text, you’d want to be able to write
     ‘\w+’ to match words, but ‘\w’ only matches the character class
     ‘[A-Za-z]’; it won’t match ‘'é'’ or ‘'ç'’.  If your system is
     configured properly and a French locale is selected, certain C
     functions will tell the program that ‘'é'’ should also be
     considered a letter.  Setting the ‘LOCALE’ flag when compiling a
     regular expression will cause the resulting compiled object to use
     these C functions for ‘\w’; this is slower, but also enables ‘\w+’
     to match French words as you’d expect.

 -- Data: M

 -- Data: MULTILINE

     (‘^’ and ‘$’ haven’t been explained yet; they’ll be introduced in
     section *note More Metacharacters: 304e.)

     Usually ‘^’ matches only at the beginning of the string, and ‘$’
     matches only at the end of the string and immediately before the
     newline (if any) at the end of the string.  When this flag is
     specified, ‘^’ matches at the beginning of the string and at the
     beginning of each line within the string, immediately following
     each newline.  Similarly, the ‘$’ metacharacter matches either at
     the end of the string and at the end of each line (immediately
     preceding each newline).

 -- Data: S

 -- Data: DOTALL

     Makes the ‘'.'’ special character match any character at all,
     including a newline; without this flag, ‘'.'’ will match anything
     _except_ a newline.

 -- Data: U

 -- Data: UNICODE

     Make ‘\w’, ‘\W’, ‘\b’, ‘\B’, ‘\d’, ‘\D’, ‘\s’ and ‘\S’ dependent on
     the Unicode character properties database.

 -- Data: X

 -- Data: VERBOSE

     This flag allows you to write regular expressions that are more
     readable by granting you more flexibility in how you can format
     them.  When this flag has been specified, whitespace within the RE
     string is ignored, except when the whitespace is in a character
     class or preceded by an unescaped backslash; this lets you organize
     and indent the RE more clearly.  This flag also lets you put
     comments within a RE that will be ignored by the engine; comments
     are marked by a ‘'#'’ that’s neither in a character class or
     preceded by an unescaped backslash.

     For example, here’s a RE that uses *note re.VERBOSE: 9c7.; see how
     much easier it is to read?

          charref = re.compile(r"""
           &[#]                # Start of a numeric entity reference
           (
               0[0-7]+         # Octal form
             | [0-9]+          # Decimal form
             | x[0-9a-fA-F]+   # Hexadecimal form
           )
           ;                   # Trailing semicolon
          """, re.VERBOSE)

     Without the verbose setting, the RE would look like this:

          charref = re.compile("&#(0[0-7]+"
                               "|[0-9]+"
                               "|x[0-9a-fA-F]+);")

     In the above example, Python’s automatic concatenation of string
     literals has been used to break up the RE into smaller pieces, but
     it’s still more difficult to understand than the version using
     *note re.VERBOSE: 9c7.


File: python.info,  Node: More Pattern Power,  Next: Modifying Strings,  Prev: Using Regular Expressions,  Up: Regular Expression HOWTO

10.9.4 More Pattern Power
-------------------------

So far we’ve only covered a part of the features of regular expressions.
In this section, we’ll cover some new metacharacters, and how to use
groups to retrieve portions of the text that was matched.

* Menu:

* More Metacharacters:: 
* Grouping:: 
* Non-capturing and Named Groups:: 
* Lookahead Assertions:: 


File: python.info,  Node: More Metacharacters,  Next: Grouping,  Up: More Pattern Power

10.9.4.1 More Metacharacters
............................

There are some metacharacters that we haven’t covered yet.  Most of them
will be covered in this section.

  Some of the remaining metacharacters to be discussed are _zero-width
assertions_.  They don’t cause the engine to advance through the string;
instead, they consume no characters at all, and simply succeed or fail.
For example, ‘\b’ is an assertion that the current position is located
at a word boundary; the position isn’t changed by the ‘\b’ at all.  This
means that zero-width assertions should never be repeated, because if
they match once at a given location, they can obviously be matched an
infinite number of times.

‘|’

     Alternation, or the "or" operator.  If A and B are regular
     expressions, ‘A|B’ will match any string that matches either ‘A’ or
     ‘B’.  ‘|’ has very low precedence in order to make it work
     reasonably when you’re alternating multi-character strings.
     ‘Crow|Servo’ will match either ‘Crow’ or ‘Servo’, not ‘Cro’, a
     ‘'w'’ or an ‘'S'’, and ‘ervo’.

     To match a literal ‘'|'’, use ‘\|’, or enclose it inside a
     character class, as in ‘[|]’.

‘^’

     Matches at the beginning of lines.  Unless the ‘MULTILINE’ flag has
     been set, this will only match at the beginning of the string.  In
     ‘MULTILINE’ mode, this also matches immediately after each newline
     within the string.

     For example, if you wish to match the word ‘From’ only at the
     beginning of a line, the RE to use is ‘^From’.

          >>> print re.search('^From', 'From Here to Eternity')  #doctest: +ELLIPSIS
          <_sre.SRE_Match object at 0x...>
          >>> print re.search('^From', 'Reciting From Memory')
          None

‘$’

     Matches at the end of a line, which is defined as either the end of
     the string, or any location followed by a newline character.

          >>> print re.search('}$', '{block}')  #doctest: +ELLIPSIS
          <_sre.SRE_Match object at 0x...>
          >>> print re.search('}$', '{block} ')
          None
          >>> print re.search('}$', '{block}\n')  #doctest: +ELLIPSIS
          <_sre.SRE_Match object at 0x...>

     To match a literal ‘'$'’, use ‘\$’ or enclose it inside a character
     class, as in ‘[$]’.

‘\A’

     Matches only at the start of the string.  When not in ‘MULTILINE’
     mode, ‘\A’ and ‘^’ are effectively the same.  In ‘MULTILINE’ mode,
     they’re different: ‘\A’ still matches only at the beginning of the
     string, but ‘^’ may match at any location inside the string that
     follows a newline character.

‘\Z’

     Matches only at the end of the string.

‘\b’

     Word boundary.  This is a zero-width assertion that matches only at
     the beginning or end of a word.  A word is defined as a sequence of
     alphanumeric characters, so the end of a word is indicated by
     whitespace or a non-alphanumeric character.

     The following example matches ‘class’ only when it’s a complete
     word; it won’t match when it’s contained inside another word.

          >>> p = re.compile(r'\bclass\b')
          >>> print p.search('no class at all')  #doctest: +ELLIPSIS
          <_sre.SRE_Match object at 0x...>
          >>> print p.search('the declassified algorithm')
          None
          >>> print p.search('one subclass is')
          None

     There are two subtleties you should remember when using this
     special sequence.  First, this is the worst collision between
     Python’s string literals and regular expression sequences.  In
     Python’s string literals, ‘\b’ is the backspace character, ASCII
     value 8.  If you’re not using raw strings, then Python will convert
     the ‘\b’ to a backspace, and your RE won’t match as you expect it
     to.  The following example looks the same as our previous RE, but
     omits the ‘'r'’ in front of the RE string.

          >>> p = re.compile('\bclass\b')
          >>> print p.search('no class at all')
          None
          >>> print p.search('\b' + 'class' + '\b')  #doctest: +ELLIPSIS
          <_sre.SRE_Match object at 0x...>

     Second, inside a character class, where there’s no use for this
     assertion, ‘\b’ represents the backspace character, for
     compatibility with Python’s string literals.

‘\B’

     Another zero-width assertion, this is the opposite of ‘\b’, only
     matching when the current position is not at a word boundary.


File: python.info,  Node: Grouping,  Next: Non-capturing and Named Groups,  Prev: More Metacharacters,  Up: More Pattern Power

10.9.4.2 Grouping
.................

Frequently you need to obtain more information than just whether the RE
matched or not.  Regular expressions are often used to dissect strings
by writing a RE divided into several subgroups which match different
components of interest.  For example, an RFC-822 header line is divided
into a header name and a value, separated by a ‘':'’, like this:

     From: author@example.com
     User-Agent: Thunderbird 1.5.0.9 (X11/20061227)
     MIME-Version: 1.0
     To: editor@example.com

  This can be handled by writing a regular expression which matches an
entire header line, and has one group which matches the header name, and
another group which matches the header’s value.

  Groups are marked by the ‘'('’, ‘')'’ metacharacters.  ‘'('’ and ‘')'’
have much the same meaning as they do in mathematical expressions; they
group together the expressions contained inside them, and you can repeat
the contents of a group with a repeating qualifier, such as ‘*’, ‘+’,
‘?’, or ‘{m,n}’.  For example, ‘(ab)*’ will match zero or more
repetitions of ‘ab’.

     >>> p = re.compile('(ab)*')
     >>> print p.match('ababababab').span()
     (0, 10)

  Groups indicated with ‘'('’, ‘')'’ also capture the starting and
ending index of the text that they match; this can be retrieved by
passing an argument to ‘group()’, ‘start()’, ‘end()’, and ‘span()’.
Groups are numbered starting with 0.  Group 0 is always present; it’s
the whole RE, so *note match object: 9d9. methods all have group 0 as
their default argument.  Later we’ll see how to express groups that
don’t capture the span of text that they match.

     >>> p = re.compile('(a)b')
     >>> m = p.match('ab')
     >>> m.group()
     'ab'
     >>> m.group(0)
     'ab'

  Subgroups are numbered from left to right, from 1 upward.  Groups can
be nested; to determine the number, just count the opening parenthesis
characters, going from left to right.

     >>> p = re.compile('(a(b)c)d')
     >>> m = p.match('abcd')
     >>> m.group(0)
     'abcd'
     >>> m.group(1)
     'abc'
     >>> m.group(2)
     'b'

  ‘group()’ can be passed multiple group numbers at a time, in which
case it will return a tuple containing the corresponding values for
those groups.

     >>> m.group(2,1,2)
     ('b', 'abc', 'b')

  The ‘groups()’ method returns a tuple containing the strings for all
the subgroups, from 1 up to however many there are.

     >>> m.groups()
     ('abc', 'b')

  Backreferences in a pattern allow you to specify that the contents of
an earlier capturing group must also be found at the current location in
the string.  For example, ‘\1’ will succeed if the exact contents of
group 1 can be found at the current position, and fails otherwise.
Remember that Python’s string literals also use a backslash followed by
numbers to allow including arbitrary characters in a string, so be sure
to use a raw string when incorporating backreferences in a RE.

  For example, the following RE detects doubled words in a string.

     >>> p = re.compile(r'(\b\w+)\s+\1')
     >>> p.search('Paris in the the spring').group()
     'the the'

  Backreferences like this aren’t often useful for just searching
through a string — there are few text formats which repeat data in this
way — but you’ll soon find out that they’re _very_ useful when
performing string substitutions.


File: python.info,  Node: Non-capturing and Named Groups,  Next: Lookahead Assertions,  Prev: Grouping,  Up: More Pattern Power

10.9.4.3 Non-capturing and Named Groups
.......................................

Elaborate REs may use many groups, both to capture substrings of
interest, and to group and structure the RE itself.  In complex REs, it
becomes difficult to keep track of the group numbers.  There are two
features which help with this problem.  Both of them use a common syntax
for regular expression extensions, so we’ll look at that first.

  Perl 5 added several additional features to standard regular
expressions, and the Python *note re: 143. module supports most of them.
It would have been difficult to choose new single-keystroke
metacharacters or new special sequences beginning with ‘\’ to represent
the new features without making Perl’s regular expressions confusingly
different from standard REs.  If you chose ‘&’ as a new metacharacter,
for example, old expressions would be assuming that ‘&’ was a regular
character and wouldn’t have escaped it by writing ‘\&’ or ‘[&]’.

  The solution chosen by the Perl developers was to use ‘(?...)’ as the
extension syntax.  ‘?’ immediately after a parenthesis was a syntax
error because the ‘?’ would have nothing to repeat, so this didn’t
introduce any compatibility problems.  The characters immediately after
the ‘?’ indicate what extension is being used, so ‘(?=foo)’ is one thing
(a positive lookahead assertion) and ‘(?:foo)’ is something else (a
non-capturing group containing the subexpression ‘foo’).

  Python adds an extension syntax to Perl’s extension syntax.  If the
first character after the question mark is a ‘P’, you know that it’s an
extension that’s specific to Python.  Currently there are two such
extensions: ‘(?P<name>...)’ defines a named group, and ‘(?P=name)’ is a
backreference to a named group.  If future versions of Perl 5 add
similar features using a different syntax, the *note re: 143. module
will be changed to support the new syntax, while preserving the
Python-specific syntax for compatibility’s sake.

  Now that we’ve looked at the general extension syntax, we can return
to the features that simplify working with groups in complex REs.  Since
groups are numbered from left to right and a complex expression may use
many groups, it can become difficult to keep track of the correct
numbering.  Modifying such a complex RE is annoying, too: insert a new
group near the beginning and you change the numbers of everything that
follows it.

  Sometimes you’ll want to use a group to collect a part of a regular
expression, but aren’t interested in retrieving the group’s contents.
You can make this fact explicit by using a non-capturing group:
‘(?:...)’, where you can replace the ‘...’ with any other regular
expression.

     >>> m = re.match("([abc])+", "abc")
     >>> m.groups()
     ('c',)
     >>> m = re.match("(?:[abc])+", "abc")
     >>> m.groups()
     ()

  Except for the fact that you can’t retrieve the contents of what the
group matched, a non-capturing group behaves exactly the same as a
capturing group; you can put anything inside it, repeat it with a
repetition metacharacter such as ‘*’, and nest it within other groups
(capturing or non-capturing).  ‘(?:...)’ is particularly useful when
modifying an existing pattern, since you can add new groups without
changing how all the other groups are numbered.  It should be mentioned
that there’s no performance difference in searching between capturing
and non-capturing groups; neither form is any faster than the other.

  A more significant feature is named groups: instead of referring to
them by numbers, groups can be referenced by a name.

  The syntax for a named group is one of the Python-specific extensions:
‘(?P<name>...)’.  _name_ is, obviously, the name of the group.  Named
groups also behave exactly like capturing groups, and additionally
associate a name with a group.  The *note match object: 9d9. methods
that deal with capturing groups all accept either integers that refer to
the group by number or strings that contain the desired group’s name.
Named groups are still given numbers, so you can retrieve information
about a group in two ways:

     >>> p = re.compile(r'(?P<word>\b\w+\b)')
     >>> m = p.search( '(((( Lots of punctuation )))' )
     >>> m.group('word')
     'Lots'
     >>> m.group(1)
     'Lots'

  Named groups are handy because they let you use easily-remembered
names, instead of having to remember numbers.  Here’s an example RE from
the *note imaplib: f2. module:

     InternalDate = re.compile(r'INTERNALDATE "'
             r'(?P<day>[ 123][0-9])-(?P<mon>[A-Z][a-z][a-z])-'
             r'(?P<year>[0-9][0-9][0-9][0-9])'
             r' (?P<hour>[0-9][0-9]):(?P<min>[0-9][0-9]):(?P<sec>[0-9][0-9])'
             r' (?P<zonen>[-+])(?P<zoneh>[0-9][0-9])(?P<zonem>[0-9][0-9])'
             r'"')

  It’s obviously much easier to retrieve ‘m.group('zonem')’, instead of
having to remember to retrieve group 9.

  The syntax for backreferences in an expression such as ‘(...)\1’
refers to the number of the group.  There’s naturally a variant that
uses the group name instead of the number.  This is another Python
extension: ‘(?P=name)’ indicates that the contents of the group called
_name_ should again be matched at the current point.  The regular
expression for finding doubled words, ‘(\b\w+)\s+\1’ can also be written
as ‘(?P<word>\b\w+)\s+(?P=word)’:

     >>> p = re.compile(r'(?P<word>\b\w+)\s+(?P=word)')
     >>> p.search('Paris in the the spring').group()
     'the the'


File: python.info,  Node: Lookahead Assertions,  Prev: Non-capturing and Named Groups,  Up: More Pattern Power

10.9.4.4 Lookahead Assertions
.............................

Another zero-width assertion is the lookahead assertion.  Lookahead
assertions are available in both positive and negative form, and look
like this:

‘(?=...)’

     Positive lookahead assertion.  This succeeds if the contained
     regular expression, represented here by ‘...’, successfully matches
     at the current location, and fails otherwise.  But, once the
     contained expression has been tried, the matching engine doesn’t
     advance at all; the rest of the pattern is tried right where the
     assertion started.

‘(?!...)’

     Negative lookahead assertion.  This is the opposite of the positive
     assertion; it succeeds if the contained expression _doesn’t_ match
     at the current position in the string.

  To make this concrete, let’s look at a case where a lookahead is
useful.  Consider a simple pattern to match a filename and split it
apart into a base name and an extension, separated by a ‘.’.  For
example, in ‘news.rc’, ‘news’ is the base name, and ‘rc’ is the
filename’s extension.

  The pattern to match this is quite simple:

  ‘.*[.].*$’

  Notice that the ‘.’ needs to be treated specially because it’s a
metacharacter; I’ve put it inside a character class.  Also notice the
trailing ‘$’; this is added to ensure that all the rest of the string
must be included in the extension.  This regular expression matches
‘foo.bar’ and ‘autoexec.bat’ and ‘sendmail.cf’ and ‘printers.conf’.

  Now, consider complicating the problem a bit; what if you want to
match filenames where the extension is not ‘bat’?  Some incorrect
attempts:

  ‘.*[.][^b].*$’ The first attempt above tries to exclude ‘bat’ by
requiring that the first character of the extension is not a ‘b’.  This
is wrong, because the pattern also doesn’t match ‘foo.bar’.

  ‘.*[.]([^b]..|.[^a].|..[^t])$’

  The expression gets messier when you try to patch up the first
solution by requiring one of the following cases to match: the first
character of the extension isn’t ‘b’; the second character isn’t ‘a’; or
the third character isn’t ‘t’.  This accepts ‘foo.bar’ and rejects
‘autoexec.bat’, but it requires a three-letter extension and won’t
accept a filename with a two-letter extension such as ‘sendmail.cf’.
We’ll complicate the pattern again in an effort to fix it.

  ‘.*[.]([^b].?.?|.[^a]?.?|..?[^t]?)$’

  In the third attempt, the second and third letters are all made
optional in order to allow matching extensions shorter than three
characters, such as ‘sendmail.cf’.

  The pattern’s getting really complicated now, which makes it hard to
read and understand.  Worse, if the problem changes and you want to
exclude both ‘bat’ and ‘exe’ as extensions, the pattern would get even
more complicated and confusing.

  A negative lookahead cuts through all this confusion:

  ‘.*[.](?!bat$).*$’ The negative lookahead means: if the expression
‘bat’ doesn’t match at this point, try the rest of the pattern; if
‘bat$’ does match, the whole pattern will fail.  The trailing ‘$’ is
required to ensure that something like ‘sample.batch’, where the
extension only starts with ‘bat’, will be allowed.

  Excluding another filename extension is now easy; simply add it as an
alternative inside the assertion.  The following pattern excludes
filenames that end in either ‘bat’ or ‘exe’:

  ‘.*[.](?!bat$|exe$).*$’


File: python.info,  Node: Modifying Strings,  Next: Common Problems,  Prev: More Pattern Power,  Up: Regular Expression HOWTO

10.9.5 Modifying Strings
------------------------

Up to this point, we’ve simply performed searches against a static
string.  Regular expressions are also commonly used to modify strings in
various ways, using the following pattern methods:

Method/Attribute       Purpose
                       
---------------------------------------------------------------------------
                       
‘split()’              Split the string into a list, splitting it
                       wherever the RE matches
                       
                       
‘sub()’                Find all substrings where the RE matches, and
                       replace them with a different string
                       
                       
‘subn()’               Does the same thing as ‘sub()’, but returns the
                       new string and the number of replacements
                       

* Menu:

* Splitting Strings:: 
* Search and Replace:: 


File: python.info,  Node: Splitting Strings,  Next: Search and Replace,  Up: Modifying Strings

10.9.5.1 Splitting Strings
..........................

The ‘split()’ method of a pattern splits a string apart wherever the RE
matches, returning a list of the pieces.  It’s similar to the ‘split()’
method of strings but provides much more generality in the delimiters
that you can split by; ‘split()’ only supports splitting by whitespace
or by a fixed string.  As you’d expect, there’s a module-level *note
re.split(): 247. function, too.

 -- Method: .split (string[, maxsplit=0])

     Split _string_ by the matches of the regular expression.  If
     capturing parentheses are used in the RE, then their contents will
     also be returned as part of the resulting list.  If _maxsplit_ is
     nonzero, at most _maxsplit_ splits are performed.

  You can limit the number of splits made, by passing a value for
_maxsplit_.  When _maxsplit_ is nonzero, at most _maxsplit_ splits will
be made, and the remainder of the string is returned as the final
element of the list.  In the following example, the delimiter is any
sequence of non-alphanumeric characters.

     >>> p = re.compile(r'\W+')
     >>> p.split('This is a test, short and sweet, of split().')
     ['This', 'is', 'a', 'test', 'short', 'and', 'sweet', 'of', 'split', '']
     >>> p.split('This is a test, short and sweet, of split().', 3)
     ['This', 'is', 'a', 'test, short and sweet, of split().']

  Sometimes you’re not only interested in what the text between
delimiters is, but also need to know what the delimiter was.  If
capturing parentheses are used in the RE, then their values are also
returned as part of the list.  Compare the following calls:

     >>> p = re.compile(r'\W+')
     >>> p2 = re.compile(r'(\W+)')
     >>> p.split('This... is a test.')
     ['This', 'is', 'a', 'test', '']
     >>> p2.split('This... is a test.')
     ['This', '... ', 'is', ' ', 'a', ' ', 'test', '.', '']

  The module-level function *note re.split(): 247. adds the RE to be
used as the first argument, but is otherwise the same.

     >>> re.split('[\W]+', 'Words, words, words.')
     ['Words', 'words', 'words', '']
     >>> re.split('([\W]+)', 'Words, words, words.')
     ['Words', ', ', 'words', ', ', 'words', '.', '']
     >>> re.split('[\W]+', 'Words, words, words.', 1)
     ['Words', 'words, words.']


File: python.info,  Node: Search and Replace,  Prev: Splitting Strings,  Up: Modifying Strings

10.9.5.2 Search and Replace
...........................

Another common task is to find all the matches for a pattern, and
replace them with a different string.  The ‘sub()’ method takes a
replacement value, which can be either a string or a function, and the
string to be processed.

 -- Method: .sub (replacement, string[, count=0])

     Returns the string obtained by replacing the leftmost
     non-overlapping occurrences of the RE in _string_ by the
     replacement _replacement_.  If the pattern isn’t found, _string_ is
     returned unchanged.

     The optional argument _count_ is the maximum number of pattern
     occurrences to be replaced; _count_ must be a non-negative integer.
     The default value of 0 means to replace all occurrences.

  Here’s a simple example of using the ‘sub()’ method.  It replaces
colour names with the word ‘colour’:

     >>> p = re.compile( '(blue|white|red)')
     >>> p.sub( 'colour', 'blue socks and red shoes')
     'colour socks and colour shoes'
     >>> p.sub( 'colour', 'blue socks and red shoes', count=1)
     'colour socks and red shoes'

  The ‘subn()’ method does the same work, but returns a 2-tuple
containing the new string value and the number of replacements that were
performed:

     >>> p = re.compile( '(blue|white|red)')
     >>> p.subn( 'colour', 'blue socks and red shoes')
     ('colour socks and colour shoes', 2)
     >>> p.subn( 'colour', 'no colours at all')
     ('no colours at all', 0)

  Empty matches are replaced only when they’re not adjacent to a
previous match.

     >>> p = re.compile('x*')
     >>> p.sub('-', 'abxd')
     '-a-b-d-'

  If _replacement_ is a string, any backslash escapes in it are
processed.  That is, ‘\n’ is converted to a single newline character,
‘\r’ is converted to a carriage return, and so forth.  Unknown escapes
such as ‘\j’ are left alone.  Backreferences, such as ‘\6’, are replaced
with the substring matched by the corresponding group in the RE. This
lets you incorporate portions of the original text in the resulting
replacement string.

  This example matches the word ‘section’ followed by a string enclosed
in ‘{’, ‘}’, and changes ‘section’ to ‘subsection’:

     >>> p = re.compile('section{ ( [^}]* ) }', re.VERBOSE)
     >>> p.sub(r'subsection{\1}','section{First} section{second}')
     'subsection{First} subsection{second}'

  There’s also a syntax for referring to named groups as defined by the
‘(?P<name>...)’ syntax.  ‘\g<name>’ will use the substring matched by
the group named ‘name’, and ‘\g<number>’ uses the corresponding group
number.  ‘\g<2>’ is therefore equivalent to ‘\2’, but isn’t ambiguous in
a replacement string such as ‘\g<2>0’.  (‘\20’ would be interpreted as a
reference to group 20, not a reference to group 2 followed by the
literal character ‘'0'’.)  The following substitutions are all
equivalent, but use all three variations of the replacement string.

     >>> p = re.compile('section{ (?P<name> [^}]* ) }', re.VERBOSE)
     >>> p.sub(r'subsection{\1}','section{First}')
     'subsection{First}'
     >>> p.sub(r'subsection{\g<1>}','section{First}')
     'subsection{First}'
     >>> p.sub(r'subsection{\g<name>}','section{First}')
     'subsection{First}'

  _replacement_ can also be a function, which gives you even more
control.  If _replacement_ is a function, the function is called for
every non-overlapping occurrence of _pattern_.  On each call, the
function is passed a *note match object: 9d9. argument for the match and
can use this information to compute the desired replacement string and
return it.

  In the following example, the replacement function translates decimals
into hexadecimal:

     >>> def hexrepl(match):
     ...     "Return the hex string for a decimal number"
     ...     value = int(match.group())
     ...     return hex(value)
     ...
     >>> p = re.compile(r'\d+')
     >>> p.sub(hexrepl, 'Call 65490 for printing, 49152 for user code.')
     'Call 0xffd2 for printing, 0xc000 for user code.'

  When using the module-level *note re.sub(): 248. function, the pattern
is passed as the first argument.  The pattern may be provided as an
object or as a string; if you need to specify regular expression flags,
you must either use a pattern object as the first parameter, or use
embedded modifiers in the pattern string, e.g.  ‘sub("(?i)b+", "x",
"bbbb BBBB")’ returns ‘'x x'’.

